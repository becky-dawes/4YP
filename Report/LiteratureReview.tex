
\chapter{Literature Review}

Currently just a load of notes from reading the papers...

\section{A Hierarchical Bayesian Language Model Based on Pitman Yor Processes - \textit{Teh} 2006} \cite{teh2006hierarchical}

\textit{n}-gram models approximate the distribution over sentences using the conditional distribution of each work given a context consisting of only the previous \textit{n} - 1 words:

\begin{equation}
P(sentence)\approx\prod_{i=1}^{T}P(word_{i}|word_{i-n+1}^{i-1})
\label{eq:nGramModel}
\end{equation}

Even for small \textit{n}, the number of parameters is huge due to the large vocabulary size, therefore direct maximum likelihood parameter fitting severely over fits to the training data.

This paper proposes a novel language model based on a hierarchical Bayesian model where each hidden variable is distributed according to a Pitman Yor process (a nonparametric generalisation of the Dirichlet distribution).

\textbf{Pitman Yor Process:} Let $W$ be a fixed and finite vocabulary of $V$ words. For each word $w \in W$ let $G(w)$ be the (to be estimated) probability of $w$, and let $G = [G(w)]_{w\in W}$ be the vector of word probabilities. We place a Pitman Yor process prior on $G$: $G \sim PY(d, \theta, G_{0})$ where discount parameter $0\leq d <1$, strength parameter $\theta >-d$ and a mean vector $G_{0} = [G_{0}(w)]_{w\in W}$. $G_{0}(w)$ is the a priori probability of word $w$ (usually $G_{0}(w) = 1/V$). $\theta$ and $d$ control the amount of variability around $G_{0}$. When $d=0$, the Pitman Yor process reduces to a Dirichlet distribution with parameters $\theta G_{0}$.

Let $x_{1}, x_{2}, ...$ be a sequence of words drawn independently and identically (i.i.d.) from $G$. The first word $x_{1}$ is assigned a value of the first draw $y_{1}$ from $G_{0}$. Let $t$ be the current number of draws from $G_{0}$ (currently $t=1$), $c_{k}$ be the number of words assigned the value of draw $y_{k}$ (currently $c_{1}=1$), and $c.=\sum_{k=1}^{t}c_{k}$ be the current number of draws from $G$. For each subsequent word $x_{c.+1}$, we either assign it the value of a previous draw $y_{k}$ with probability $\frac{c_{k}-d}{\theta +c.}$ (increment $c_{k}$; set $x_{c.+1}\leftarrow y_{k}$), or we assign it the value of a new draw from $G_{0}$ with probability $\frac{\theta +dt}{\theta +c.}$ (increment $t$; set $c_{t}=1$; draw $y_{t}\sim G_{0}$; set $x_{c.+1}<y_{t}$).

The more words that have been assigned to a draw from $G_{0}$, the more likely subsequent words will be assigned to the draw. Also, the more we draw from $G_{0}$, the more likely a new word will be assigned to a new draw from $G_{0}$. These effects together produce a power-law distribution where many unique words are observed, most of them rarely. $\theta$ controls the overall numbers of unique words, while $d$ controls the asymptotic growth of the number of unique words. This procedure for generating words drawn from $G$ is the Chinese Restaurant Process. (Consider a sequence of customers (corresponding to the words drawn from $G$) visiting a Chinese restaurant with an unbounded number of tables (corresponding to the draws from $G_{0}$), each of which can accommodate an unbounded number of customers. The first customer sits at the first table, and each subsequent customer either joins an already occupied table (assign the word to the corresponding draw from $G_{0}$), or sits at a new table (assign the word to a new draw from $G_{0}$).)

\textbf{Hierarchical Pitman Yor Language Models:} Given a context $\boldsymbol{u}$, let $G_{\boldsymbol{u}}(w)$ be the probability of the current word taking on value $w$. We use a Pitman Yor process as the prior for $G_{\boldsymbol{u}}[G_{\boldsymbol{u}}(w)]_{w \in W}$, in particular,

\begin{equation}
G_{\boldsymbol{u}}\sim PY(d_{|\boldsymbol{u}|}, \theta_{|\boldsymbol{u}|}, G_{\pi(\boldsymbol{u})})
\label{eq:pitmanYorPrior}
\end{equation}

\noindent where $\pi(\boldsymbol{u})$ is the suffix of $\boldsymbol{u}$ consisting of all but the earliest word. As we don't know $G_{\pi(\boldsymbol{u})}$, we recursively place a prior over $G_{\pi(\boldsymbol{u})}$ using Equation \ref{eq:pitmanYorPrior}, but now with parameters $\theta_{|\pi(\boldsymbol{u})|}, d_{|\pi(\boldsymbol{u})|}$ and mean vector $G_{\pi(\pi(\boldsymbol{u}))}$ instead. This is repeated until we get to $G_{\emptyset}$, the vector of probabilities over the current word given the empty context $\emptyset$. Finally we place a prior on $G_{\emptyset}$:

\begin{equation}
G_{\emptyset} \sim PY(d_{0}, \theta_{0}, G_{0})
\label{eq:emptyContextPrior}
\end{equation}

\noindent where $G_{0}$ is the global mean vector, given a uniform value of $G_{0}(w)=1/V$ for all $w \in W$. Finally, we place a uniform prior on the discount parameters and a Gamma(1,1) prior on the strength parameters. The total number of parameters in the model is $2n$.

The prior is structured as a suffix tree of depth $n$, where each node corresponds to a context consisting of up to $n-1$ words, and each child corresponds to adding a different word to the beginning of the context (words appearing earlier in a context have (a priori) the least importance in modelling the probability of the current word).

\textbf{Hierarchical Chinese Restaurant Process:} We may treat each $G_{\boldsymbol{u}}$ as a distribution over the current word. Since $G_{\boldsymbol{u}}$ is Pitman Yor process distributed, we can draw words from it using the Chinese Restaurant process. The same applies for $G_{\pi(\boldsymbol{u})}$. This is recursively applied until we need draws from the global mean distribution $G_{0}$, which is easy since it is just uniformally distributed.

For each context $\boldsymbol{u}$, there is a sequence of words $x_{\boldsymbol{u}1}, x_{\boldsymbol{u}2}, ...$ drawn i.i.d. from $G_{\boldsymbol{u}}$ and another sequence of words $y_{\boldsymbol{u}1}, y_{\boldsymbol{u}2}, ...$ drawn i.i.d. from the parent distribution $G_{\pi(\boldsymbol{u})}$. We use $l$ to index draws from $G_{\boldsymbol{u}}$ and $k$ to index draws from $G_{\pi(\boldsymbol{u})}$. $t_{\boldsymbol{u}wk}=1$ if $y_{\boldsymbol{u}k}$ takes on value $w$, and $t_{\boldsymbol{u}wk}=0$ otherwise. Each word $x_{\boldsymbol{u}l}$ is assigned to one of the draws $y_{\boldsymbol{u}k}$ from $G_{\pi(\boldsymbol{u})}$. If $y_{\boldsymbol{u}k}$ takes on value $w$ define $c_{\boldsymbol{u}wk}$ as the number of words $x_{\boldsymbol{u}l}$ drawn from $G_{\boldsymbol{u}}$ assigned to $y_{\boldsymbol{u}k}$, otherwise let $c_{\boldsymbol{u}wk}=0$. Finally, we denote marginal counts by dots, e.g. $c_{\boldsymbol{u}.k}$ is the number of $x_{\boldsymbol{u}l}$s assigned with the value of $y_{\boldsymbol{u}k}$, $c_{\boldsymbol{u}w.}$ is the number of $x_{\boldsymbol{u}l}$s with value $w$, and $t_{\boldsymbol{u}..}$ is the current number of draws $y_{\boldsymbol{u}k}$ from $G_{\pi(\boldsymbol{u})}$.

\begin{equation}
\left \{ \begin{array} {c c} t_{\boldsymbol{u}w.}=0 & if\ c_{\boldsymbol{u}w.}=0 \\
1\leq t_{\boldsymbol{u}w.} \leq c_{\boldsymbol{u}w.} & if\ c_{\boldsymbol{u}w.}>0
\end{array}
\right.
\label{eq:hierarchicalChineseRestaurantT}
\end{equation}

\begin{equation}
c_{\boldsymbol{u}w.}=\sum _{\boldsymbol{u}':\pi (\boldsymbol{u}')=\boldsymbol{u}}t_{\boldsymbol{u}'w.}
\label{eq:hierarchicalChineseRestaurantC}
\end{equation}

The more a word $w$ has been drawn in context $\boldsymbol{u}$, the more likely it will be drawn again in context $\boldsymbol{u}$. Word $w$ will be reinforced for other contexts that share a common suffix with $\boldsymbol{u}$, with the probability of drawing $w$ increasing as the length of the common suffix increases ($w$ will be more likely under the context of the common suffix as well).

\textbf{Markov Chain Monte Carlo sampling based inference scheme for the hierarchical Pitman Yor language model:} Training data $D$ consists of the number of occurrences $c_{\boldsymbol{u}w.}$ of each word $w$ after each context $\boldsymbol{u}$ of length exactly $n-1$. This corresponds to observing word $w$ drawn $c_{\boldsymbol{u}w.}$ times from $G_{\boldsymbol{u}}$. Given $D$, we are interested in the posterior distribution over the latent vectors $\boldsymbol{G}=\{ G_{\boldsymbol{v}}\colon all\ contexts\ \boldsymbol{v} \} $ and parameters $\boldsymbol\Theta =\{ \theta_{m}, d_{m} \colon 0 \leq m \leq n-1 \}$:

\begin{equation}
p(\boldsymbol{G}, \boldsymbol\Theta | D)=p(\boldsymbol{G}, \boldsymbol\Theta, D)/p(D)
\label{eq:mcmcPosteriorG}
\end{equation}

The hierarchical Chinese Restaurant process marginalises out each $G_{\boldsymbol{u}}$, replacing it with the seating arrangement in the corresponding restaurant, $S_{\boldsymbol{u}}$. Let $\boldsymbol{S}=\{S_{\boldsymbol{v}}:all\ context\ \boldsymbol{v}\}$. Therefore we are instead interested in the posterior over seating arrangements:

\begin{equation}
p(\boldsymbol{S}, \boldsymbol\Theta |D)=p(\boldsymbol{S}, \boldsymbol\Theta, D)/p(D)
\label{eq:mcmcPosteriorS}
\end{equation}

The probability of a test word $w$ after a context $\boldsymbol{u}$ is given by:

\begin{equation}
p(w | \boldsymbol{u}, D)=\int p(w | \boldsymbol{u}, \boldsymbol{S}, \boldsymbol\Theta)p(\boldsymbol{S}, \boldsymbol\Theta | D)d(\boldsymbol{S}, \boldsymbol\Theta)
\label{eq:mcmcWordProb}
\end{equation}

\noindent where the first probability on the right is the predictive probability under a particular setting of seating arrangements $\boldsymbol{S}$ and parameters $\boldsymbol\Theta$, and the overall predictive probability is obtained by averaging this with respect to the posterior over $\boldsymbol{S}$ and $\boldsymbol\Theta$ (second probability on right). Approximate the integral with samples $\{\boldsymbol S^{(i)}, \boldsymbol\Theta^{(i)}\}_{i=1}^{I}$ drawn from $p(\boldsymbol S, \boldsymbol\Theta |D)$:

\begin{equation}
p(w|\boldsymbol u, D)\approx \sum_{i=1}^{I}p(w|\boldsymbol u, \boldsymbol S^{(i)}, \boldsymbol \Theta ^{(i)})
\label{eq:mcmcIntegralApprox}
\end{equation}

\noindent while $p(w|\boldsymbol u, \boldsymbol S, \boldsymbol \Theta)$ is given by the function $WordProb(\boldsymbol u, w)$:

\begin{equation}
p(w|0, \boldsymbol S, \boldsymbol \Theta)=1/V
\label{eq:mcmcWordProb1}
\end{equation}

\begin{equation}
p(w|\boldsymbol u, \boldsymbol S, \boldsymbol \Theta)=\frac{c_{\boldsymbol u w.}d_{|\boldsymbol u |}t_{\boldsymbol u w.}}{\theta_{|\boldsymbol u |}+c_{\boldsymbol u ..}} + \frac{\theta_{|\boldsymbol u |}+d_{|\boldsymbol u |}t_{\boldsymbol u ..}}{\theta_{|\boldsymbol u |}+c_{\boldsymbol u ..}}p(w|\pi(\boldsymbol u), \boldsymbol S, \boldsymbol \Theta)
\label{eq:mcmcWordProb2}
\end{equation}

\noindent where the counts are obtained from the seating arrangement $S_{\boldsymbol u}$ in the Chinese Restaurant process corresponding to $G_{\boldsymbol u}$.

\section{A Hierarchical, Hierarchical Pitman Yor Process Language Model - \textit{Wood, Teh}} \cite{wood2008hierarchical}

\textbf{Normal Pitman Yor process language modelling:} Distribution over words following a particular context:

\begin{equation}
w_{t}|w_{t-1}, w_{t-2} \sim G_{\{w_{t-2},w_{t-1}\}}^{0}
\label{eq:PYWordDistribution}
\end{equation}

\noindent (context length 2) is a random distribution:

\begin{equation}
G_{\{w_{t-2}, w_{t-1}\}}^{0} \sim PY(d_{2}, \alpha_{2}, H)
\label{eq:PYDistributionPY}
\end{equation}

\noindent where $PY(d, \alpha, H)$ is a Pitman Yor process with a distributed count $d$, concentration $\alpha$ and base distribution $H$. When the base distribution is the distribution over words following the same context with one fewer antecedents, $H=G_{\{w_{t-1}\}}^{0}$, and $G_{\{w_{t-1}\}}^{0}$ is a random distribution which is distributed according to a Pitman Yor process with another more general base distribution, this is a \textit{Hierarchical Pitman Yor Process (HPYP)}. This "recursion" continues until the set of antecedent words is empty - "root" PY process is typically given a base distribution which is uniform over the corpus vocabulary.

\textbf{Graphical Pitman Yor Process:} Assume we have corpora from domains $D_{1}, D_{2}$ and we take the approach common to Bayesain domain adaptation, specifying a hierarchical model that allows statistical sharing between the models of each sorpus. The model has the same form as the HPYP, except that the base distribution of every PY process in the hierarchy is different:

\begin{equation}
G_{\{w_{t-2}, w_{t-1}\}}^{D_{i}} \sim PY(d_{j}, \theta_{j}, \pi G_{\{w_{t-1}\}}^{D_{i}} + (1-\pi)G_{\{w_{t-2}, w_{t-1}\}}^{0})
\label{eq:graphicalPY}
\end{equation}

\noindent The distribution over words in a particular context in a particular domain could reasonably back off to a distribution over words given a shorted context in the same domain or a distribution over words given the whole context in a general domain. Here $\pi$ is the parameter that controls how closely the base distribution is tied to the domain specific model or the general model.

\section{Dirichlet Process - \textit{Teh} 2010} \cite{teh2010dirichlet}

The Dirichlet Process is a stochastic process used in Bayesian nonparametric models of data. It is a distribution over distributions and has Dirichlet distributed finite dimensional marginal distributions. Distributions drawn from a Dirichlet process are discrete, but cannot be described using a finite number of parameters (nonparametric).

DP is a stochastic process whose sample paths are probability measures with probability 1. Stochastic processes are distributions over function spaces, with sample paths being random functions drawn from the distribution. DP is a distribution over probability measures, which are functions with certain special properties which allow them to be interpreted as distributions over some probability space $\Theta$. Therefore draws from a DP can be interpreted as random distributions. DP is an infinite dimensional generalisation of Dirichlet distributions.

\section{The Sequence Memoizer - \textit{Wood, Gasthaus, Archambeau, James, Teh} 2011} \cite{wood2011sequence}

"The Sequence Memoizer is a new hierarchical Bayesian model for discrete sequence data that captures long range dependencies and power-law characteristics, while remaining computationally attractive."

Let $\Sigma$ be the set of symbols that can occur in some sequence. Suppose that we are given a sequence $\boldsymbol{x}=x_{1}, x_{2}, ..., x_{T}$ of symbols from $\Sigma$ and want to estimate the probability that the next symbol takes a particular value $s \in \Sigma$.

\textbf{Using Relative Frequency:} i.e. if $s$ occurs frequently in $\boldsymbol{x}$ we expect its probability of appearing next to be high as well. $N(s)$ is number os occurrences of $s$ in $\boldsymbol{x}$. Probability of $s$ being next symbol is $G(s)=N(s)/T=N(s)/\sum_{s' \in \Sigma}N(s')$. $G$ is a discrete distribution over the elements of $\Sigma$: it assigns a non-negative number $G(s)$ to each symbol $s$ signifying the probability of observing $s$ with the numbers summing to 1 over $\Sigma$. This approach is reasonable only if the process generating $\boldsymbol{x}$ has no history dependence. 
\textbf{Taking into account context:} If the last symbol in $\boldsymbol{x}$ is $\boldsymbol{u}$, then we can estimate the probability of the next symbol being $s$ by counting the number of times $s$ occurs after $\boldsymbol{u}$ in $\boldsymbol{x}$:

\begin{equation}
G_{\boldsymbol{u}}(s)=\frac{N(\boldsymbol{u}s)}{\sum_{s' \in \Sigma}N(\boldsymbol{u}s')}
\label{eq:seqMemRelativeFreqContext}
\end{equation}

\noindent is the estimated probability go $s$ occurring after $\boldsymbol{u}$, where $N(\boldsymbol{u}s)$ is the number os occurrences of the subsequence $\boldsymbol{u}s$ in $\boldsymbol{x}$. $G_{\boldsymbol{u}}$ is a discrete distribution over the symbols in $\Sigma$, but it is a conditional distribution as the probability assigned to each symbol $s$ depends on the context $\boldsymbol{u}$.

\textbf{Maximum Likelihood:} An optimistic procedure - assumes $\boldsymbol{x}$ is an accurate reflection of the true underlying process that generated it, to the ML parameters will be an accurate estimate of the true parameters - leads to overfitting. if $\boldsymbol{u}$ is long, the chance that it nevers occurs in $\boldsymbol{x}$ is high, therefore the denominator in Equation \ref{eq:seqMemRelativeFreqContext} is 0. If $\boldsymbol{u}$ did occur in $\boldsymbol{x}$, a high probability will be assigned to the symbols that followed $\boldsymbol{u}$ and zero probability to all other symbols. The amount of data in $\boldsymbol{x}$ is insufficient to characterise the conditional distribution $G_{\boldsymbol{u}}$. Avoid this by making a fixed-order Markov assumption and restricting to estimating collections of distributions conditioned on short contexts.

\textbf{Bayesian Modelling:} conservative compared to ML approach. Uncertainty in estimation is taken into account by treating the parameters $\Theta$ as random, with a prior distribution $p(\Theta)$ reflecting the prior knowledge we have about the true data generating process. $p(\Theta | \boldsymbol{x})=p(\Theta)p(\boldsymbol{x}|\Theta)/p(\boldsymbol{x})$. Computations such as prediction are then done taking into account the a posteriori uncertainty about the underlying parameters.

Natural sequence data often exhibits power-law properties. Conditional distributions of similar contexts tend to be similar themselves, particularly in the sense that recency matters.

\textbf{Power-law scaling:} There are a small number of words that occur disproportionately frequently and a very large number of rare words that, although each occurs rarely, when taken together make up a large proportion of the language. ML estimates the probabilities of the frequently occurring symbols well, since they are based on many observations of the symbols. The estimates of the rare symbols will be bad.If a symbol did not occur in our sequence, our estimate of its probability is 0, while the estimate of a rare symbol that occurred by chance will be too high.

Using Pitman Yor:

\begin{equation}
p(\boldsymbol{x}_{T+1}=s|\boldsymbol{x})=\int(\boldsymbol{x}_{T+1}=s | G)p(G|\boldsymbol{x})dG={\mathbb E} [G(s)]
\label{eq:seqMemPowerLaw}
\end{equation}

\noindent where $\mathbb E$ stands for expectation wrt posterior distribution $p(G|\boldsymbol{x})$. This equation doesn't always have an analytic solution therefore use numerical integration approaches, including sampling and Monte Carlo integration. 

In addition to the counts $\{N(s')\}_{s' \in \Sigma}$ assume that there is another set of random "counts" $\{M(s')\}_{s' \in \Sigma}$ satisfying $1/leq M(s') /leq N(s')$ if $N(s') >)$ and $M(s')>0$ otherwise. Probability of a symbol $s$ occurring next:

\begin{equation}
\mathbb E [G(s)]=\mathbb E \left [\frac {N(s)-\alpha M(s)+\sum_{s' \in \Sigma}\alpha M(s')G_{0}(s)}{\sum_{s' \in \Sigma}N(s')}\right ]
\label{eq:seqMemPYP}
\end{equation}

\noindent Each $M(s)$ reduces the count $N)s)$ by $\alpha M(s)$. The total amount subtracted is redistributed across all symbols in $\Sigma$ proportionally according to the symbols' probability under the base distribution $G_{0}$. Therefore non-zero counts are usually reduced, with larger counts typically reduced by a larger amount. This mitigates the overestimation of probabilities of rare symbols that happen to appear by chance. For symbols that did not appear at all, the estimates of their probabilities are pulled upward from zero, mitigating underestimation.

\textbf{Context trees:} If two contexts are similar, then the corresponding conditional distributions over the symbols that follow those context will tend to be similar as well. Similarity is defined by overlapping contextual suffixes.

Using a hierarchical Bayesian model and considering only fixed, finite length contexts, we are making an \textit{n}\textsuperscript{th} order Markov assumption - each symbol depends only on the last \textit{n} observed symbols. This assumption dictates that distributions are not only similar, but equal among context whose suffixes overlap in their last \textit{n} symbols.

To construct a context tree: Arrange the context $\boldsymbol{u}$ (and the associated distributions $G_{\boldsymbol{u}}$) in a tree where the parent of a node $\boldsymbol{u}$, denoted by $\sigma(\boldsymbol{u})$, is given by its longest proper suffix (i.e. $\boldsymbol{u}$ with its first symbol from the left removed). Since we are making an \textit{n}\textsuperscript{th} order Markov assumption, it is sufficient to consider only the contexts $\boldsymbol{u} \in \Sigma_{\boldsymbol{u}}^{*} = \{u' \in \Sigma^{*} :|\boldsymbol{U}'|\leq n\}$ of length at most \textit{n}. The resulting context tree has height \textit{n} and the total number of nodes in the tree grows exponentially in \textit{n}. The memory complexity of models built on such context trees usually grows too large and too quickly for reasonable values of \textit{n} and $|\Sigma|$.

\textbf{HPYP:}

\begin{equation}
G_{\varepsilon}\sim PY(\alpha_{0}, G_{0})
\label{eq:seqMemG}
\end{equation}

\begin{equation}
G_{\boldsymbol{u}}|G_{\sigma(\boldsymbol{u})}\sim PY(\alpha_{|\boldsymbol{u}|}, G_{\sigma(\boldsymbol{u})}) \ \ \ \  for\ all\ \boldsymbol{u} \in \Sigma_{n}^{*}/\varepsilon
\label{eq:seqMemGU}
\end{equation}

\begin{equation}
x_{i}|\boldsymbol{x}_{i-n:i-1}=\boldsymbol{u}, G_{\boldsymbol{u}}\sim G_{\boldsymbol{u}} \ \ \ \  for\ i=1, ..., T
\label{eq:seqMemXi}
\end{equation}

\noindent Equation \ref{eq:seqMemGU} says that a priori the conditional distribution $G_{\boldsymbol{u}}$ should be similar to $G_{\sigma(\boldsymbol{u})}$, its parent in the context tree. The variation of $G_{\boldsymbol{u}}$ around its mean $G_{\sigma(\boldsymbol{u})}$ is described by a PYP with a context length-dependent discount parameter $\alpha_{|\boldsymbol{u}|}$. At the top of the tree, the distribution $G_{\varepsilon}$ for the empty context $\varepsilon$ is similar to an overall base distribution $G_{0}$ which specifies our prior belief that each symbol $s$ will appear with the probability $G_{0}(s)$. Equation \ref{eq:seqMemXi} describes the \textit{n}\textsuperscript{th} order Markov model for $\boldsymbol{x}$ - the distribution over each symbol $x_{i}$ in $\boldsymbol{x}$, given that its context consisting of the previous \textit{n} symbols $x_{i-n:i-1}$ is $\boldsymbol{u}$, is simply $G_{\boldsymbol{u}}$. This is the hierarchical PY process.

\textbf{The Sequence Memoizer Model:} Instead of limiting context lengths to \textit{n}, the model is extended to include the set of distributions in all contexts of any (finite) length - the distribution over each symbol is now conditioned on all previous symbols, not just the previous \textit{n}. The model is the hierarchical PY model defined in Equations \ref{eq:seqMemG}, \ref{eq:seqMemGU} and \ref{eq:seqMemXi}, but with 2 changes: the contexts range over all finite nonempty strings, $\boldsymbol{u}\in \Sigma^{*}/\varepsilon$; Equation \ref{eq:seqMemXi} becomes $x_{i}|\boldsymbol{x}_{1:i-1}=\boldsymbol{u}, G_{\boldsymbol{u}}\sim G_{\boldsymbol{u}}$ (conditioning on all previous symbols). The Model can be interpreted as the limit of an HPYP model as the Markov order \textit{n} tends to infinity.

\textbf{Compacting the context tree:} Given a finite length sequence of symbols $\boldsymbol{x}$ we need access to only a finite number of conditional distributions. We need only $G_{x_{1:i}}$ where $i=0, ..., T$ and all the ancestors of each $G_{x_{1:i}}$ in the context tree. The ancestors are needed because each $G_{\boldsymbol{u}}$ has a prior that depends on its parents $G_{\sigma(\boldsymbol{u})}$. The resulting set of conditional distributions that the sequence $\boldsymbol{x}$ actually depends on consists of $G_{\boldsymbol{u}}$ where $\boldsymbol{u}$ ranges over all continuous substrings of $\boldsymbol{x}$, a finite set of $O(T^2)$ contexts. This subtree is denoted $T(\boldsymbol{x})$.

Many of the contexts that appear in $T(\boldsymbol{x})$ appear only in non-branching chains, i.e. each node on the chain has only one child in $T(\boldsymbol{x})$. If we can directly express the prior of the longer context in terms of the shortest non-branching suffix, then we can effectively ignore those in between and marginalise them out from the model (coagulation: $G_{11}|G_{1}\sim PY(\alpha_{2}, G_{1})$ and $G_{011}|G_{11}\sim PY(\alpha_{3}, G_{11})$ gives $G_{011}|G_{1}\sim PY(\alpha_{2}\alpha_{3}, G_{1})$ where $G_{11}$ has been marginalised out - i.e. the prior for $G_{011}$ is another PYP whose discount parameter is simply the product of the discount parameters along the chaing leading into it on the tree $T(\boldsymbol{x})$, while the base distribution is simply the head of the $G_{1}$.

We can apply this marginalization procedure to all non-branching chains of $T(\boldsymbol{x})$ to give a compact context tree $\hat{T}(\boldsymbol{x})$ where all internal nodes have at least two children. The number of nodes in $\hat{T}(\boldsymbol{x})$ is at most twice the length of the sequence $\boldsymbol{x}$ (independent of $|\Sigma|$). The structure of the compact context tree is given by the suffix structure for the reverse sequence $x_{T}, x_{T-1}, ..., x_{1}$.

Inference in the full SM model with an infinite number of parameters is equivalent to inference in the compact context tree with a linear number of parameters. The prior over the conditional distributions on $\hat{T}(\boldsymbol{x})$ still retains the form of an HPYP - each node has a PYP prior with its parent as the base distribution.

\begin{equation}
\mathbb E [G_{\boldsymbol{u}}(s)]=\mathbb E \left [ \frac{N(\boldsymbol{u}s)-\alpha_{\boldsymbol{u}}M(\boldsymbol{u}s)+\sum_{s'\in \Sigma}\alpha_{\boldsymbol{u}}M(\boldsymbol{u}s')G_{\sigma(\boldsymbol{u})}(s)}{\sum_{s'\in \Sigma}N(\boldsymbol{u}s')}\right ]
\label{eq:seqMemCompact}
\end{equation}

The first term in the numerator is a count of the number of times $s$ occurs in the context $\boldsymbol u$. The second term is the reduction applied to the count. The third term spreads the total reduction across $\Sigma$ according to the base distribution $G_{\sigma(\boldsymbol u)}(s)$. Each context $\boldsymbol u$ now has its own discount parameter $\alpha_{\boldsymbol u}$, which is the product of discounts on the non-branching chain leading to $\boldsymbol u$ on $T(\boldsymbol s)$, while the parent $\sigma(\boldsymbol u)$ is the head of the chain. Equation \ref{eq:seqMemCompact} is defined recursively, with the predictive distribution $G_{\boldsymbol u}$ in context $\boldsymbol u$ being a function of the same in the parent $\sigma(\boldsymbol u)$ and so on up the tree.

If the context $\boldsymbol u$ does not appear in the compact context tree $\hat{T}(\boldsymbol x)$, the predictive distribution is simply the one given by the longest suffix of $\boldsymbol u$ that is in $T(\boldsymbol x)$. If that is still not in $\hat{T}(\boldsymbol u)$, then a converse of the coagulation property (\textit{fragmentation}) allows us to reintroduce the node back into the tree.

Use stochastic (Monte Carlo) approximations to evaluate Equation \ref{eq:seqMemCompact} where the expectation is approximated using samples from the posterior distribution. Samples are obtained using Gibbs sample, which repeatedly makes local changes to the counts, and using sequential Monte Carlo which iterates through the sequence $x_{1}, x_{2}, ..., x_{T}$, keeping track of a set of samples at each step and updating the samples as each symbol $x_{i}$ is incorporated into the model.

\textbf{Language modelling:} Language model performance is report in terms of a standard measure called \textit{perplexity}, defined as $2^{\ell(\boldsymbol x)}$ where $\ell(\boldsymbol x)=-\frac{1}{|\boldsymbol x|}\sum_{i=1}^{|\boldsymbol x|}\log_{2}p(x_{i}|\boldsymbol x_{1:i-1})$ is the average \textit{log-loss} on a sequence $\boldsymbol x$ and the average number of bits per word required to encode the sequence using an optimal code. Another interpretation of perplexity is that it is the average number of guesses the model would have to make before it guessed each word correctly (if it makes these guesses by drawing samples from its estimate of the conditional distribution). For SM, $p(x_{i}|\boldsymbol x_{1:i-1})$ is computed using Equation \ref{eq:seqMemCompact}.

As more data is used to estimate a language model, typically its performance improves.Prediction in the SM has real-world time complexity that is essentially the same as that of a smoothed finite-order Markov model, while its memory complexity is linear in the amount of data. The computational complexity of Markov models theoretically does not depend on the amount of data, but is exponential in the Markov order, rendering straightforward extensions to higher orders impractical. The SM model fixes this problem while remaining computationally tractable.

\textbf{Compression}



\section{A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation - \textit{Wood, Teh} 2009} \cite{wood2009hierarchical}

Corpus $\mathcal{C} =[w_{1}...w_{T}]$ takes simplified form $P(\mathcal{C})=\prod_{t=1}^{T}P(w_{t}|w_{t-1}...w_{t-n+1}])$ where probability of word $w_{t}$ is conditionally dependent on, at most, the $n-1$ preceding words. MLE of \textit{n}-gram model parameters is likely to overfit, particularly when there are zero counts therefore need to regularise through smoothing.

HPYLM is a hierarchical nonparametric Bayesian language model based on the HPYP. Assume a fixed and finite sized dictionary of $\mathcal{L}$ unique words and has following structure:

\begin{equation}
\begin{split}
\mathcal{G}_{[]}\sim PY(d_{0}, \alpha_{0}, \mathcal{U})
\\ 
\mathcal{G}_{[x_{1}]}\sim PY(d_{1}, \alpha_{1}, \mathcal{G}_{[]})
\\
...
 \\
\mathcal{G}_{[x_{j}...x_{1}]}\sim PY(d_{j}, \alpha_{j}, \mathcal{G}_{[x_{j-1}...x_{1}]})
 \\
w_{t}|w_{t-n+1}...w_{t-1}\sim \mathcal{G}_{[w_{t-n+1}...w_{t-1}]}
\end{split}
\end{equation}

where the $w$s are the observed instances of words (\textit{tokens}) and the $x$s range over the unique words (\textit{types}) in the dictionary. $\mathcal{G}\sim PY(d, \alpha, \mathcal{F})$ means that $\mathcal{G}$ is a random distribution drawn from a PY process with concentration parameter $\alpha$, discount parameter $d$ and base measure $\mathcal{F}$. Can think of $\mathcal{F}$ as being the mean distribution on which $\mathcal{G}$ is "centred" in the sense that $E[\mathcal{G}(x)]=\mathcal{F}(x)$. $\mathcal{U}$ is a uniform distribution over word types.

\section{A Stochastic Memoizer for Sequence Data - \textit{Wood, Archambeau, Gasthaus, James, Teh} 2009} \cite{wood2009stochastic}

\section{A Nonparametric Bayesian Alternative to Spike Sorting - \textit{Wood, Black} 2006} \cite{wood2008nonparametric}

A "spike train" is a temporal sequence containing the times of all action potentials for a given cell. Spike sorting involves detecting all action potentials that occur in a neurophysiological recording and labelling them to indicated from which neurone each came. A distribution over spike trains is esteemed which allows us to express uncertainties that arise during the spike sorting process and in the results of neural data analysis.

If we have $N$ action potential waveforms from a single-channel neurophysiological recording $R=[\overrightarrow{t_{1}}, ..., \overrightarrow{t_{N}}]$, where each waveform is represented by a vector of $n=40$ voltage samples, $\overrightarrow{t_{i}}=[t_{i}^{1}, ..., t_{i}^{n}]^{T}\in \mathbb{R}^{n}$. Our goal is to build a posterior distribution over sortings of this recording; a distribution which simultaneously represents how many neurone are present and which waveforms came from which neurone. We cluster a reduced dimensionality representation of the waveforms where each $\overrightarrow{t_{i}}$ is represented in a lower dimensional basis via principal component analysis (PCA). Given a set of recordings we perform PCA and keep only the first $D=2$ eigenvectors characterising the largest variance in the data and approximate a waveform as $\overrightarrow{t_{i}}\approx \mu +\sum_{d=1}^{D}y_{i}^{d}\overrightarrow{u_{d}}$. Here $\mu$ is the mean wave shape in $R$, $\overrightarrow{u_{d}}$ is the $d$\textsuperscript{th} PCA basis vector, and the $y_{i}^{d}$ are linear coefficients. The spike sorting algorithm clusters the low dimensional representation of the waveforms $\mathcal{Y}=[\overrightarrow{y_{i}}, ..., \overrightarrow{y_{N}}]$ rather than the full waveforms.

\section{Deplump for Streaming Data - \textit{Bartlett, Wood} 2011} \cite{bartlett2011deplump}

Deplump is a general purpose, lossless, batch compressor based on a probabilistic model of discrete sequences called the sequence memoizer.

The Sequence Memoizer is an incremental method for estimating the conditional distributions in an \textit{n}-gram model in the limit of $n\to\infty$. The space complexity is a function of the number of instantiated nodes in the corresponding suffix-tree-shaped graphical model and the storage required at each node.

\textbf{Review:} A distribution over $P$ can be factorized as $P(S=[s_{0}, s_{1}, ..., s_{m}])=P(s_{0})P_{[s_{0}]}(s_{1})P_{[s_{0}, s_{1}]}(s_{2})...P_{[s_{0}, s_{1}, ..., s_{m-1}]}(s_{m})$, where $P_{U}(s)=P(s|U)$. The Sequence Memoizer jointly models these conditional distributions using a hierarchical Bayesian framework in which non-negative, integer parameters $\{c_{\sigma}^{U}, t_{\sigma}^{U}\}_{\sigma \in \Sigma, U \in \Sigma^{+}}$ are used to characterise each $P_{I}$. If we define $c^{U}=\sum_{\sigma\in\Sigma}c_{\sigma}^{U}$ and $t^{U}=\sum_{\sigma\in\Sigma}t_{\sigma}^{U}$ then the model is:

\begin{equation}
P_{U}(s)=\frac{c_{s}^{U}-t_{s}^{U}\delta^{U}}{c^{U}}+\frac{t^{U}\delta^{U}P_{\sigma(U)}(s)}{c^{U}}
\label{eq:deplumpReview}
\end{equation}

\noindent where $\sigma([s_{1}, s_{2}, s_{3}, ...])=[s_{2}, s_{3}, ...]$ and $P_{\sigma([])}$ is the uniform distribution over $\Sigma$.

The latent parameters of the SM are the counts in the set of sets $G=\{\{c_{\sigma}^{U}, t_{\sigma}^{U}\}_{\sigma\in\Sigma}|U\in\Sigma^{+}\}$ and $\delta_{n}$ for $n\geq 0$. While $|G|=\infty$ for sequences of unbounded length, inference requires only a computation on $H\subset G$ for any finite training sequence $\boldsymbol S$ such that $|H|\leq 2|\boldsymbol S |$. The linear growth of $|H|$ makes using the model intractable for streaming sequences. Deplump yields an inference algorithm with asymptotically constant storage complexity for $H$.

\textbf{Approximation:} Batch deplump uses a suffix tree which has storage complexity linear in input sequence length. Streaming deplump cannot have suffix tree representation of entire input sequence, so instead a fixed-length "reference sequence" is maintained, along with a dynamically updated suffix tree referencing only the suffixes found therein. Deleting nodes to constrain the size of this context tree forces node-removal inference approximations - operations are justifiable from a statistical perspective.

During incremental estimation, for each symbol $s$ in the input sequence, $c_{s}^{U}$ is incremented in at least one node on the tree. $max_{P\in H, \sigma\in\Sigma}\{c_{\sigma}^{U}\}$ then grows monotonically as a function of the length of the input sequence. Incremental construction of the model includes an operation on node elements known as fragmentation that requires computation proportional to $max_{\sigma\in\Sigma}\{c_{\sigma}^{U}\}$ for node $U$. To ensure computational complexity that is independent of the sequence length, $c_{\sigma}^{U}$ must be bounded for all $U\in H$.

\textbf{Algorithm:} Given a sequence of symbols $\boldsymbol S=[s_{0}, s_{1}, s_{2}, ...]$ where each symbol $s_{n}$ comes from an ordered set of symbols $\{\sigma_{1}, \sigma_{2}, ...\}=\Sigma$, streaming deplump worlds by repeatedly producing a predictive distribution for the continuation of the sequence given the full preceding context and encoding the next symbol by passing the predictive distribution to an entropy encoder. We assume that if the predictive distribution function is $F$ and the next symbol in the stream is $s$ then an entropy encoder exists that takes $F(s-1)$ and $F(s)$ as arguments and returns a bit stream. An incrementally constructed suffix-tree-like data structure is used. This efficiently encodes a subset of the unique suffixes of all contexts in a sequence. Each edge in the tree has a label which is a sub-sequence of the input sequence. Each edge label is represented by two indices into a fixed-length suffix of the input sequence (reference sequence) i.e. each edge label is $[r_{i+1}, r_{i+2}, ..., r_{j}]$ for indices $i$ and $j$ into the reference sequence where $[r_{1}=s_{n-T}, ..., r_{T}=s_{n}]$ after then $n$th input sequence element is processed. Therefore each node in the tree corresponds to a subsequence $[s_{m}, ..., s_{m+k}]$ of the input sequence and a (potentially non-sequential) set of subsequences of the reference sequence. $\mathcal{N}$ refers interchangeably to a node and the suffix in the input sequence to which the node corresponds. For a given suffix, the corresponding node is accessed by traversing edges of the tree that match the suffix read from right to left.

Deplump processes each element of input sequence $\mathcal{I}$ incrementally. For each element $s_{n}$ of $\mathcal{I}$, $PMFNext$ computes the predictive probability mass function $\pi$ (conditioned on the observed context $\mathcal{R}$) needed for compression. The element $s_{n}$ is then encoded by an entropy encoder with parameter $\pi$. $PMFNext$ also handles the incremental maintenance of the underlain constant-space data structures by both restricting the length of the reference sequence and constructing and deleting nodes in the tree as needed. Finally it incrementally integrates the observation into the approximated sequence memoizer and adjusts the back-off/smoothing parameters $\mathcal{D}$. This involves updating counts in the tree and calculating a stochastic gradient for $\mathcal{D}$.

$PMFNext$ starts by enforcing the bounds on $|\mathcal{R}|$ and $|\mathcal{H}|$ which requires nodes to be removed from tree. Removing leaf nodes from the sequence memoizer results in a coherent approximate inference algorithm for the sequence memoizer. Enforcing the bound on $|\mathcal{H}|<L$ is as simple as incrementally removing leaf nodes uniformly at random. To facilitate random deletion we maintain a count in each node of the number of leaf nodes in the subtree below it. A random leaf node can then be obtained by traversing a weighted random path down the tree.

To use streaming deplump, a choice of approximation parameters must be made. The full set of these parameters consist of $\mathcal{D}, D, T, k, L, \eta$. $\mathcal{D} = [\delta_{0}, \delta_{1}, ..., \delta_{10}, \alpha]$ is a list of discount parameters, each taking a real value in (0, 1). If we define $\delta_{n}=\delta_{10}^{\alpha^{n-10}}$ for $n\geq 10$ and $\delta_{n}$ for $n\leq 10$ then $d^{\mathcal{N}}=\prod_{i=|PA(\mathcal{N}|+1}^{|\mathcal{N}|}\delta_{i}$. $D$ is the maximum depth of the suffix tree which corresponds to both the maximum context length used in modelling and the maximum recursion depth of all of the algorithms. $T$ is the bound on the length of the reference sequence $\mathcal{R}$ and is typically set to a multiple of the upper bound on the number of node instances in the suffix tree $L$. The parameter $k$ is the upper bound on the total count $c^{\mathcal{N}}$ in each node. The parameter $\eta$ is a learning rate for the updating of the discount parameters and is typically set to a very small value.

\section{A Hierarchical Dirichlet Language Model - \textit{MacKay, Peto} 1995}   \cite{mackay1995hierarchical}

\textbf{Bigram model with smoothing: } String of $T$ words $D=w_{1}, w_{2}, ... w_{T}$ is observed and the marginal and conditional frequencies observed. Marginal count $F_{i}$ is number of times that word $i$ occurs. Conditional count $F_{i|j}$ is number of times word $j$ is immediately followed by word $i$. Ignore option of grouping words by a common root. 'Estimators' of marginal probability of word $i$ and conditional probability of word $i$ following word $j$: $f_{i}=\frac{F_{i}+\alpha/W}{T+\alpha}$ and $f_{i|j}=\frac{F_{i|j}+\beta/W}{F_{j}+\beta}$ where the 'initial counts' $\alpha/W$ and $\beta/W$ are commonly set to 0, 1/2 or 1. $0\leq i,j\leq W$ where $W$ is number of distinct words in language. 

If initial counts are set to 0 we obtain ML estimators $f_{i}=F_{i}/T$ and $f_{i|j}=F_{i|j}/F_{j}$, which assign zero frequency to all words and word pairs which didn't occur in data. Could write $\hat{P}(w_{t}|w_{t-1})=f_{w_{t}|w_{t-1}}$ as an estimator, but this conditional probability typically has large variance because there are so many possible couplets $ij$ that only a small fraction of them have been observed in the data. Instead: $\hat{P}(w_{t}|w_{t-1})=\lambda f_{w_{t}}+(1-\lambda)f_{w_{t}|w_{t-1}}$. Deleted interpolation (a cross-validation procedure) is used to set $\lambda$. This involves dividing the data into a number of blocks, computing predictions for each block using the other blocks as training data and adjusting $\lambda$ to optimise predictive performance. It has been found that better predictions can be obtained if contexts $w_{1}$ with similar values of $f_{w_{1}}$ are grouped together, with a separate $\lambda$ for each group determined by deleted interpolation.

\textbf{An explicit model using Dirichlet priors: } Bigram model has conditional distribution $P(w_{t}=i|w_{t-1}=j)$ described by $W(W-1)$ independent parameters, where $W$ is number of words in language ($W$ possible conditioning terms on RHS for each of which a probability distribution with $(W-1)$ independent parameters is specified). These parameters will be denoted $Q$ with $P(w_{t}=i|w_{t-1}=j)\equiv q_{i|j}$. $Q$ is a $WxW$ transition probability matrix. A single row f $Q$, the probability vector for transitions from state $j$, is denoted $\boldsymbol{q}_{|j}$. $Q$ is the natural representation of a Markov process. The marginal distribution $P(w_{t})$ is not independent, but is a deterministic function of the conditional probability matrix $Q$ i.e. $P(w_{t})$ is the principal eigenvector of $Q$. 

A model $\mathscr{H}$ is a specification of the model parameters, the way that the probability of the data depends on those parameters, and a riot probability distribution on those parameters. Given a model $\mathscr{H}$:

\textbf{Infer parameters given data: } Probabilty of parameters $Q$ given data $D$ in terms of likelihood $P(D|Q\mathscr{H})$ and prior distribution $P(Q|\mathscr{H})$:

\begin{equation}
P(Q|D\mathscr{H})=\frac{P(D|Q, \mathscr{H})P(Q|\mathscr{H})}{P(D|\mathscr{H})}
\end{equation}

\noindent The normalising constant is given by integrating the numerator over $Q$:

\begin{equation}
P(D|\mathscr{H})=\int P(D|Q, \mathscr{H})P(Q|\mathscr{H})d^{k}Q
\end{equation}

\noindent where $k$ is the dimensionality of $Q$.

\textbf{Predict the next word in a given context: } To obtain the probability of $w_{t}$ given $w_{t-1}$ and the data $D$, we use the sum rule of probability $P(A|C)=\int P(A|B, C)P(B|C)dB$ to marginalise over the unknown parameters $Q$:

%\begin{equation}
\begin{align}
P(w_{t}|w_{t-1}, D, \mathscr{H}) &= \int P(w_{t}|w_{t-1}, Q, D, \mathscr{H})P(Q|D, \mathscr{H})d^{k}Q
\\
&= \int q_{w_{t}|w_{t-1}}P(Q|D, \mathscr{H})d^{k}Q
\end{align}
%\end{equation}

\noindent The distribution inside the integral, $P(Q|D, \mathscr{H})$, depends on the likelihood function and the prior.

Assume that first word of data set is given \textit{a priori} and is not to be predicted by the model. The probability of the string of words is then then probability of the second word given the first, times the probability of the third word given the second etc:

\begin{equation}
P(D|Q, \mathscr{H})=]prod_{t}q_{w_{t}|w_{t-1}}
\end{equation}

\noindent We can rewrite this by counting how often each variable $w_{i|j}$ appears in the product. This is given by the conditional count $F_{i|j}$:

\begin{equation}
P(D|Q, \mathscr{H})=\prod_{j}\prod_{i}q_{i|j}^{F_{i|j}}
\end{equation}

\noindent So given the assumed bigram model, the conditional counts $F_{i|j}$ contain all the relevant information that data convey about $Q$.

\textbf{Dirichlet distribution priors: }The Dirichlet distribution for a probability vector $\boldsymbol{p}$ with $I$ components is parameterised by a measure $\boldsymbol{u}$ (a vector with all coefficients $u_{i}>0$) , which we write as $\boldsymbol{u}=\alpha\boldsymbol{m}$, where $\boldsymbol{m}$ is a normalised measure over the $I$ components ($\sum m_{i}=1$) and $\alpha$ is a positive scalar:

\begin{equation}
P(\boldsymbol{p}|\alpha\boldsymbol{m})=\frac{1}{Z(\alpha\boldsymbol{m}}\prod_{i=1}^{I}p^{\alpha m_{i}-1}\delta\left (\sum_{i}-1\right )\equiv Dirichlet^{(I)}(\boldsymbol{p}|\alpha\boldsymbol{m})
\end{equation}

\noindent The function $\delta(x)$ is the Dirac delta function, which simply restricts the distribution to the simplex such that $\boldsymbol{p}$ is normalised, i.e. $\sum_{i}p_{i}=1$. The normalising constant of the Dirichlet distribution:

\begin{equation}
Z(\alpha\boldsymbol{m})=\prod_{i}\Gamma(\alpha m_{i})/\Gamma(\alpha)
\end{equation}

\noindent The vector $\boldsymbol{m}$ is the mean probability of the distribution:

\begin{equation}
\int Dirichlet^{(I)}(\boldsymbol{p}|\alpha\boldsymbol{m})\boldsymbol{p}d^{I}\boldsymbol{p}=\boldsymbol{m}
\end{equation}

$\alpha$ firstly measure the sharpness of the distribution - it measures how different we expect typical samples $\boldsymbol{p}$ from the distribution to be from the mean $\boldsymbol{m}$. A large value of $\alpha$ produces a distribution over $\boldsymbol{p}$ which is sharply peaked around $\boldsymbol{m}$. For small $\alpha$, typically one component $p_{i}$ receives an overwhelming share of the probability, and of the probability that remains to be shared among the other components, another component $p_{i'}$ receives a similarly large share. In the limit as $\alpha$ goes to zero, the plot tends to an increasingly steep power law.

We can also characterise $\alpha$ in terms of the predictive distribution that results when we observe samples from $\boldsymbol{p}$ and obtain counts $\boldsymbol{F}=(F_{1}, F_{2}, ..., F_{I})$ of the possible outcomes. The posterior probability of $\boldsymbol{p}$ is another Dirichlet distribution:

\begin{align}
P(\boldsymbol{p}|\boldsymbol{F}, \alpha\boldsymbol{m}) &= \frac{P(\boldsymbol{F}|\boldsymbol{p})P(\boldsymbol{p}|\alpha\boldsymbol{m})}{P(\boldsymbol{F}|\alpha\boldsymbol{m})}
\\
&= \frac{\prod_{i}p_{i}^{F_{i}}\prod_{i}p_{i}^{\alpha m_{i}-1} \delta \left (\sum_{i}p_{i}-1 \right)/Z(\alpha \boldsymbol{m})}{P(\boldsymbol{F}|\alpha\boldsymbol{m})}
\\
&= \frac{\prod_{i}p_{i}^{F_{i}+\alpha m_{i}-1} \delta \left(\sum_{i}p_{i}-1\right)}{P(\boldsymbol{F}|\alpha\boldsymbol{m})Z(\alpha\boldsymbol{m})}
\\
&= Dirichlet^{(I)}(\boldsymbol{p}|boldsymbol{F}+\alpha\boldsymbol{m})
\end{align}

\noindent The predictive distribution given the data $\boldsymbol{F}$ is then:

\begin{align}
P(i|\boldsymbol{F},\alpha\boldsymbol{m}) &=\int Dirichlet^{(I)}(\boldsymbol{p}|\boldsymbol{F}+\alpha\boldsymbol{m})\boldsymbol{p}d^{I}\boldsymbol{p}
\\
&= \frac{F_{i}+\alpha m_{i}}{\sum_{i'}F_{i'}+\alpha m_{i'}}
\end{align}

\noindent The term $\alpha m_{i}$ appears as an effective initial count in bin $i$. The value of $\alpha$ defines the number of sample from $\boldsymbol{p}$ that are required in order that that data dominate over the prior in subsequent predictions. If $\alpha \gg \sum_{i}F_{i}$ then $P(i|\boldsymbol{F},\alpha\boldsymbol{m})\simeq m_{i}$. If $\alpha\ll\sum_{i}F_{i}$ then $P(i|\boldsymbol{F},\alpha\boldsymbol{m})\simeq F_{i}/\left(\sum_{i'}F_{i'}\right)$.

The 'evidence' for $\alpha\boldsymbol{m}$ is:

\begin{equation}
P(\boldsymbol{F}|\alpha\boldsymbol{m})=\frac{Z(\boldsymbol{F}+\alpha\boldsymbol{m})}{Z(\alpha\boldsymbol{m})}=\frac{\prod_{i}\Gamma(F_{i}+\alpha m_{i})}{\Gamma\left(\sum_{i}F_{i}+\alpha\right)}\frac{\Gamma(\alpha)}{\prod_{i}\Gamma(\alpha m_{i})}
\end{equation}

\textbf{Definition of hierarchical model $\mathscr{H}_{D}$: } $D$ for Dirichlet. Hierarchical because as well as containing unknown parameters $Q$, which place a probability distribution on data, it contains unknown 'hyper parameters' which define a probability distribution over the parameters $Q$.

We assign a \textit{coupled} prior to parameters $Q$ - a prior under which learning the probability vector for one context $\boldsymbol{q}_{j}$ gives us information about what the probability vectors $\boldsymbol{q}_{j'}$ in other contexts might be. We introduce an unknown measure on the words, $\boldsymbol{u}=\alpha\boldsymbol{m}$, and define a separable prior, given $\alpha\boldsymbol{m}$, on the vectors $\boldsymbol{q}_{|j}$ that make up $Q$:

\begin{equation}
P(Q|\alpha\boldsymbol{m}, \mathscr{H}_{D})=\prod_{j}Dirichlet^{(I)}(\boldsymbol{q}_{|j}|\alpha\boldsymbol{m})
\end{equation}

\noindent We produce a dependence between the vectors $\boldsymbol{q}_{|j}$ by putting an uninformative prior $P(\alpha\boldsymbol{m})$ on the measure $\alpha\boldsymbol{m}$ (a flat prior on $\boldsymbol{m}$ and a broad gamma prior over $\alpha$). The prior on $Q$ defined by this hierarchical model is then:

\begin{equation}
P(Q|\mathscr{H}_{D})=\int\prod_{j}[Dirichlet^{(I)}(\boldsymbol{q}_{|j}|\alpha\boldsymbol{m})]P(\alpha\boldsymbol{m})d^{I}\alpha\boldsymbol{m}
\end{equation}

Method is different from many empirical Bayes prescriptions in that we use Bayesian inference to control the hyper parameters and we motivate this procedure as an approximation to the ideal predictive Bayesian approach.

\textbf{Level 1 inference: } Interested in plausible values of parameters $Q=\{q_{i|j}\}$. Assume we know $\boldsymbol{m}$ and $\alpha$. Then easy to infer a posterior distribution for $Q$ and get a predictive distribution. Bayes' Theorem gives posterior:

\begin{equation}
P(Q|D, \alpha\boldsymbol{m}, \mathscr{H}_{D})=\frac{P(D|Q,\mathscr{H}_{D})P(Q|\alpha\boldsymbol{m}, \mathscr{H}_{D})}{P(D|\alpha\boldsymbol{m}, \mathscr{H}_{D})}
\end{equation}

\noindent Equation is separable into a product over contexts $j$ because both the prior $P(Q|\alpha\boldsymbol{m}, \mathscr{H}_{D})$ and the likelihood $P(Q|D, \alpha\boldsymbol{m}, \mathscr{H}_{D})$ are separable:

\begin{equation}
P(Q|D, \alpha\boldsymbol{m}, \mathscr{H}_{D})=\prod_{j}P(\boldsymbol{q}_{|j}|D, \alpha\boldsymbol{m}, \mathscr{H}_{D})
\end{equation}

\noindent The posterior distribution of each conditional probability vector is another Dirichlet distribution:

\begin{equation}
P(\boldsymbol{q}_{|j}|D, \alpha\boldsymbol{m}, \mathscr{H}_{D})\propto \prod_{i}q_{i|j}^{F_{i|j}+\alpha m_{i}+1}\delta\left(\sum_{i}q_{i|j}-1\right)=Dirichlet^{(I)}(\boldsymbol{q}_{|j}|\boldsymbol{F}+\alpha\boldsymbol{m})
\end{equation}

\noindent The posterior can be used for prediction:

\begin{align}
P(i|j, D, \alpha\boldsymbol{m}, \mathscr{H}_{D})&=\frac{F_{i|j}+\alpha m_{i}}{\sum_{i'}F_{i'|j}+\alpha m_{i'}}
\\
&= \lambda_{j}m_{i}+(1-\lambda_{j})f_{i|j}
\end{align}

\noindent where $f_{i|j}=F_{i|j}/F_{j}$ and 

\begin{equation}
\lambda_{j}=\frac{\alpha}{F_{j}+\alpha}
\end{equation}

\textbf{Level 2 inference: } Interested in plausible values of 'hyper parameters' $\alpha\boldsymbol{m}$. Posterior distribution of $\alpha\boldsymbol{m}$ by Bayes' Theorem:

\begin{equation}
P(\alpha\boldsymbol{m}|D, \mathscr{H}_{D})=\frac{P(D|\alpha\boldsymbol{m},\mathscr{H}_{D})P(\alpha\boldsymbol{m}|\mathscr{H}_{D})}{P(D|\mathscr{H}_{D})}
\end{equation}

\noindent The data-dependent term $P(D|\alpha\boldsymbol{m},\mathscr{H}_{D})$ is the normalising constant from the first level of inference - the \textit{evidence} for $\alpha\boldsymbol{m}$. If (as we expect) the posterior distribution $P(\alpha\boldsymbol{m}|D\mathscr{H}_{D})$ is sharply peaked in $\alpha\boldsymbol{m}$ so that it is effectively a delta function, relative to $P(i|j, D, \alpha\boldsymbol{m},\mathscr{H}_{D})$ then we may approximate:

\begin{equation}
P(i|j,D\mathscr{H}_{D})\simeq P(i|j,D[\alpha\boldsymbol{m}]^{MP},\mathscr{H}_{D})
\end{equation}

\noindent where $[\alpha\boldsymbol{m}]^{MP}$ is the maximum of the posterior distribution. Therefore, instead of marginalising over the hyper parameters, we optimise them (computationally more convenient an often gives predictive distributions that are indistinguishable from the true predictive distribution). Assume a non informative prior $P(\alpha\boldsymbol{m}|\mathscr{H}_{D})$ so the posterior probability maximum $[\alpha\boldsymbol{m}]^{MP}$ is found by maximising the evidence $P(D|\alpha\boldsymbol{m},\mathscr{H}_{D})$.

\textbf{'You see' text example: } 
\\ \\
\noindent \textit{"Imagine, you see, that the language, you see, has, you see, a frequently occurring couplet, 'you see', you see, in which the second word of the couplet, see, follows the first word, you, with very high probability, you see.. Then the marginal statistics, you see, are going to become hugely dominated, you see, by the words you and see, with equal frequency, you see."}
\\ \\
\indent Where the Dirichlet model would assign probabilities $m_{i}^{MP}$, the smoothing formula would assign probabilities proportional to $f_{i}$. So, using the smoothing formal, the predictions $\hat{P}(you|novel)$ and $\hat{P}(see|novel)$ would come out equal, since 'you' and 'see' both have occurred equally often so far. 'You' evidently has a relatively high probability in any context, whereas 'see' only has a high frequency because it has a high probability f following 'you'. Therefore intuitively $P(you|novel)$ should be greater than $P(see|novel)$.

\textbf{The dice factory: } A factory produces biased $I$-sided dice. Model the probability vector $\boldsymbol{q}$ of a single dies as coming from a Dirichlet prior with unknown hyper parameters $\boldsymbol{u}=\alpha\boldsymbol{m}$ that characterise the factory. The data are outcomes of rolls of $J$ dice labelled by $j$. Each die $j$ is rolled a number of times $F_{j}$ and we are told the counts of the outcomes $F_{i|j}$ which give us imperfect information about the parameters $Q=\{q_{i|}\}$. Given these measurements, our task is to infer the hyper parameters $\boldsymbol{u}=\alpha\boldsymbol{m}$ of the factory in order to make better predictions about future rolls of individual dice.

In a language modelling problem, the number of dice $J$ and the number of classes $I$ are the number of words $W$. Can imagine the language being generated by a dice rolling procedure in which the outcome of roll $t$ determines which die is rolled at time $t+1$

\textbf{The evidence for $\alpha\boldsymbol{m}$: } Posterior probability of $\alpha\boldsymbol{m}$ is proportional to $P(D|\alpha\boldsymbol{m})=\prod_{j}P(\boldsymbol{F}_{|j}|\alpha\boldsymbol{m})$ from:

\begin{equation}
P(D|\alpha\boldsymbol{m})=]prod_{j}\left(\frac{\prod_{i}\Gamma(F_{i|j}+\alpha m_{i})}{\Gamma(F_{j}+\alpha)}\frac{\Gamma(\alpha)}{\prod_{i}\Gamma(\alpha m_{i})}\right)
\end{equation}

Let $u_{i}=\alpha m_{i}$. To find the most probable $\boldsymbol{u}=\alpha\boldsymbol{m}$, we differentiate, using digamma functions defined by $\Psi(x)\equiv \frac{\partial \log \Gamma(x)}{\partial x}$. 

\begin{equation}
\frac{\partial}{\partial u_{i}}\log P(D|\boldsymbol{u})=\sum_{j}[\Psi(F_{i|j}+u_{i})-\Psi(F_{j}+\sum_{i'}u_{i'})+\Psi(\sum_{i'}u_{i'})-\Psi(w_{i})]
\end{equation}

\noindent This gradient may be fed into any optimisation program to find $\boldsymbol{u}^{MP}$.

\textbf{Inferring $\boldsymbol{u}=\alpha\boldsymbol{m}$: } Assume $\alpha>1$ (corresponds to rough number of data points needed to overwhelm the Dirichlet prior) and $u_{i}<1$ ($m_{i}$s sum to 1 and a typical $m_{i}$ will be $\frac{1}{size of vocabulary}$ therefore) to derive an algorithm specialised for the parameter regimes expected in language modelling.

Use relationship $\Psi(x+1)=\Psi(x)+\frac{1}{x}$ to combine terms:

\begin{equation}
\Psi(F_{i|j}+u_{i})-\Psi(u_{i})=\frac{1}{F_{i|j}-1+u_{i}}+\frac{1}{F_{i|j}-2+u_{i}}+...+\frac{1}{2+u_{i}+\frac{1}{1+u_{i}}+\frac{1}[u_{i}}
\end{equation}

\noindent The number of terms is $F_{i|j}$. Assuming $u_{i}<1$, we can approximate for $F_{i|j}>1$:

\begin{equation}
\frac{1}{u_{i}}+\sum_{f=2}^{F_{i|j}}\frac{1}{f-1}-u_{i}\sum_{f=2}^{F_{i|j}}\frac{1}{(f-1)^{2}}+O(u_{i}^{2})
\end{equation}

Approximating the other terms $\Psi(\alpha)-\Psi(F_{j}+\alpha)$, we obtain the following prescription for the maximum evidence hyper parameters $\boldsymbol{u}^{MP}$. For each $F$ and $i$, let $N_{Fi}$ be the number of context $j$ such that $F_{i|j}\geq F$, and let $F_{i}^{max}$ be the largest $F$ such that $N_{Fi}>0$. Denote the number of entries in row $i$ of $F_{i|j}$ that are non-zero, $N_{1i}$ by $V_{i}$.

\begin{align}
G_{i} &= \sum_{f=2}^{F_{i}^{max}}\frac{N_{fi}}{f-1}
\\
H_{i} &= \sum_{f=2}^{F_{i}^{max}}\frac{N_{fi}}{(f-1)^{2}}
\end{align}

\noindent and define:

\begin{equation}
K(\alpha)=\sum_{j}\log\left[\frac{F_{j}+\alpha}{\alpha}\right]+\frac{1}{2}\sum_{j}\left[\frac{F_{j}}{\alpha(F_{j}+\alpha)}\right]
\end{equation}

\noindent then the optimal hyper parameters $\boldsymbol{u}$ satisfy the implicit equation:

\begin{equation}
u_{i}^{MP}=\frac{2V_{i}}{K(\alpha^{MP})-G_{i}+\sqrt{(K(\alpha^{MP})-G_{i})^{2}+4H_{i}V_{i}}}
\end{equation}

\noindent This defines a 1D problem: to find the $\alpha$ such that the $u_{i}$ satisfy $\sum_{i}u_{i}=\alpha$. The optimal $\alpha$ can be found by a bracketing procedure or by a reestimation procedure in which we alternately use the above equation to set $u_{i}$ given $\alpha$ and then set $\alpha:=\sum_{i}u_{i}$.

\textbf{Application to a small corpus: } Perplexity is defined as $2^{H(Q;\hat{P})}$ where $H(Q;\hat{P})$ is the cross-entropy between the unknown 'true' model $Q$ and the assumed model $\hat{P}$. For the bigram model and large enough test corpus, the perplexity of the test corpus can be approximated as:

\begin{equation}
Perplexity\simeq\left[\prod_{t=2}^{T}\hat{P}(w_{t}|w_{t-1})\right]^{-]frac{1}{T}}
\end{equation}

\noindent where $T$ is the number of words in the corpus. For the deleted interpolation method, the number of $\lambda$s is chosen subjectively. In the new hierarchical model, there is one hyper parameter $u_{i}$ for each type in the training data vocabulary.

Experiment:
\begin{enumerate}
\item Raw frequencies and relative frequencies of tokens and bigrams obtained from training data as a whole.
\item Most probable values for parameters of each model solved for iteratively: 
\indent\begin{itemize}
 \item Dirichlet model: solve simultaneous equations to obtain $\boldsymbol{u}^{MP}$.
\item Smoothing method: separate frequencies first calculated for each block, then $\lambda$s obtained using deleted interpolation.
\end{itemize}
\item Optimisation halted when on average each parameter of model converged to 8dp.
\item Optimised parameter values for each model used to compute predictive probabilities $\hat{P}(i|j)$ for each bigram in test data.
\item Perplexity of test data samples evaluated using each model and results compared.
\end{enumerate}

\noindent Perplexities under the deleted interpolation model and under the Dirichlet model are nearly the same. Effect of altering $\lambda$s for deleted interpolation very small. Values of $\lambda$ decreased with frequency $F_{j}$.

The number of iterations required for each algorithm to converge was comparable. However, a single iteration of the Dirichlet model requires time linear in the size of the vocabulary, while an iteration of deleted interpolation requires time linear in the size of the training corus. The large the training corpus, the more significant would be this advantage of the Dirichlet model. Deleted interpolation also requires more memory because it keeps separate count and frequency data for each block of the training corpus. 

\textbf{Discussion: }Dirichlet model does away with cross-validation and therefore makes full use of the data while requiring fewer computational resources.

An alternative density over probabilities to the Dirichlet distribution is the \textit{entropic prior}:

\begin{equation}
P(\boldsymbol{p}|\alpha\boldsymbol{m})=\frac{1}{Z}\frac{1}{\prod_{i}p_{i}^{1/2}}exp\left(\alpha\sum_{i}p_{i}\log\frac{m_{i}}{p_{i}}\right)\delta\left(\sum p_{i}-1\right)
\end{equation}

\noindent The entropic prior, like the Dirichlet prior, characterises a language by a single mean and spread of a distribution of conditional probabilities $\boldsymbol{q}_{|j}$ for all contexts $j$.

Could assert that there are different types of contexts, such that for all contexts of the same type, the conditional probabilities $\boldsymbol{q}$ are similar. If we do not know \textit{a priori} what the type of each context is, then this model is a \textbf{mixture model}. A mixture model $\mathscr{H}_{M}$ defines a density over $\boldsymbol{q}$ as a weighted combination of $C$ independently parameterised simple distributions, where each mixture component $c=1...C$ might be a Dirichlet or entropic distribution. Various algorithms can be used t implement mixture model: both Monte Carlo methods and Gaussian approximations. 

Alternatively, a model might define the context to be the last two words, with the type of the context being defined by the most recent word. With a coupled prior for the context hyper parameters, this model would give predictions similar to those of the smoothed trigram language model. 

\textbf{Gamma function and Digamma function: }

\begin{equation}
\Gamma(x)\equiv\int_{0}^{\inf}du\ u^{x-1}e^{-u}
\end{equation}

\noindent for $x>0$. In general $\Gamma(x+1)=x\Gamma(x)$ and for integer arguments, $\Gamma(x+1)=x!$. The digamma function is denoted by $\Psi(x)\equiv\frac{d}{dx}\log\Gamma(x)$. For large $x$ ($0.1\leq x \leq \inf$), the following approximations are useful:

\begin{equation}
\begin{split}
\log\Gamma(x) \simeq (x-\frac{1}{2})\log(x)-x+\frac{1}{2}\log2\pi+O(\frac{1}{x})
\\
\Psi(x) = \frac{d}{dx}\log\Gamma(x)\simeq \log(x)-\frac{1}{2x}+O(\frac{1}{x^{2}})
\end{split}
\end{equation}

\noindent And for small $x$ ($0\leq x \leq 0.5$):

\begin{equation}
\begin{split}
\log \Gamma(x) \simeq \log\frac{1}{x}-\gamma_{e}x+O(x^{2})
\\
\Psi(x)\simeq-\frac{1}{x}-\gamma_{e}+O(x)
\end{split}
\end{equation}

\noindent where $\gamma_{e}$ is Euler's constant. The digamma function satisfies the following recurrence relation exactly:

\begin{equation}
\Psi(x+1)=\Psi(x)+\frac{1}{x}
\end{equation}

\textbf{Formula for a more general algorithm: }Gives an approximation to the difference $\Psi(F+u)-\Psi(u)$ that is accurate to within 2\% for all $u$ and all positive integers $F$:

\begin{equation}
\Psi(F+u)-\Psi(u)\simeq\frac{1}{u}+\log\frac{F+u-1/2}{u+1/2}
\end{equation}

\noindent This approximation is useful for gradient-based optimisation of Dirichlet distributions.

\section{Other Notes}

\textit{n}-gram model is a contiguous sequence of \textit{n} items from a given sequence of text.

\null
\noindent Entropy encoding is a lossless data compression scheme that is independent of the specific characteristics of the medium.

