\chapter{Testing}\label{chap:results}

\section{Perplexity}\label{sec:perplexity}

Language models are compared by evaluating the perplexity of the data for each. A better model has a smaller perplexity. Perplexity is defined in Equation \ref{eq:HD-perplexity}, where $H(Q;\hat{P})=\sum_{i}Q_{i}\log_{2}\hat{P_{i}}$ is the cross-entropy between the unknown true model $Q$ and the assumed model $\hat{P}$ \cite{mackay1995hierarchical}. For a bi-gram model, we can approximate this to Equation \ref{eq:HD-perplexity-approx}, where $T$ is the total number of words in the corpus.

\begin{equation}
\text{Perplexity}=2^{H(Q;\hat{P})}
\label{eq:HD-perplexity}
\end{equation}

\begin{equation}
\text{Perplexity}\simeq\left[\prod_{t=2}^{T}\hat{P}(w_{t}|w_{t-1})\right]^{-\frac{1}{T}}
\label{eq:HD-perplexity-approx}
\end{equation}

Another definition of perplexity is $2^{\ell(\boldsymbol{x})}$ where $\ell(\boldsymbol{x})$ is defined as in Equation \ref{eq:ellX} \cite{wood2011sequence}. This is relevant for the Sequence Memoizer implementation as it considers context of infinite length.

\begin{equation}
\ell(\boldsymbol{x})=-\frac{1}{|\boldsymbol{x}|}\sum_{i=1}^{|\boldsymbol{x}|}\log_{2}P(x_{i}|\boldsymbol{x}_{1:i-1})
\label{eq:ellX}
\end{equation}

Perplexity was found for the hierarchical Dirichlet model as in the following code. We find $P(i|j,D,\alpha\boldsymbol{m},\mathscr{H}_{D})$ for all bigrams as a vector, using the optimal values for $\alpha$ and $\boldsymbol{m}$. All elements in the vector are then multiplied together and the result is raised to the power $-\frac{1}{T}$, where $T$ is the total number of words in the corpus.



\section{Comparison of $n$-gram Smoothing Techniques}

\todo[inline]{Comparison of n-gram smoothing techniques}

\section{Evaluation of Hierarchical Models}

\todo[inline]{Evaluation of hierarchical models}

\section{Comparison of Model Performance with Wikipedia}

\todo[inline]{Comparison of model performance with wikipedia}
