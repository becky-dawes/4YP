\chapter{Results}

\section{Comparing Models}

Language models are compared by evaluating the perplexity of the data for each. A better model has a smaller perplexity. Perplexity is defined in Equation \ref{eq:HD-perplexity}, where $H(Q;\hat{P})$ is the cross-entropy between the unknown true model $Q$ and the assumed model $\hat{P}$ \cite{mackay1995hierarchical}. We can approximate this to Equation \ref{eq:HD-perplexity-approx}.

\begin{equation}
\text{Perplexity}=2^{H(Q;\hat{P})}
\label{eq:HD-perplexity}
\end{equation}

\begin{equation}
\text{Perplexity}\simeq\left[\prod_{t=2}^{T}\hat{P}(w_{t}|w_{t-1})\right]^{-\frac{1}{T}}
\label{eq:HD-perplexity-approx}
\end{equation}

Perplexity was found for the hierarchical Dirichlet model as in the following code. We find $P(i|j,D,\alpha\boldsymbol{m},\mathscr{H}_{D})$ for all bigrams as a vector, using the optimal values for $\alpha$ and $\boldsymbol{m}$. All elements in the vector are then multiplied together and the result is raised to the power $-\frac{1}{T}$, where $T$ is the total number of words in the corpus.

\begin{lstlisting}
(defn perplexity [] (Math/pow (reduce * (map #(P (second %) (first %) alpha-optimum m-optimum) pairs)) (- (/ 1 N))))
\end{lstlisting}

\todo[inline]{Comparison of models and smoothing techniques}
