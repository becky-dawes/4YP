\chapter{Introduction}

%\section{Background}

Sequence prediction has possible applications in many different areas. We come across it every day whilst using the search engine Google\footnote{\url{www.google.com}} when it helpfully suggests possible search topics based on what we have already typed. This concept of ``autocomplete" is something which we take for granted in this situation, but which could be extended to many different uses. A rather different application is that of a therapeutic system. Individuals with motor disorders, such as Multiple Sclerosis or Stroke, may find some sort of text prediction service useful when using phones or computers (much like ``autocorrect"). A much more advanced use of sequence prediction is a form of question answering system, such as Watson (discussed below).

\section{Watson}

This project was originally based on the work done by IBM on Watson\footnote{\url{http://www.ibm.com/smarterplanet/us/en/ibmwatson/tech.html}}, a system capable of answering questions. It is best known for its performance on the gameshow Jeopardy!, where it won against human opponents. Watson was trained using a variety of datasources, but it was not connected to the internet during the game. Although it did occasionally struggle to understand the concept of the questions and was unable to buzz in early, Watson's reaction times were much faster than those of the humans (about 8ms to activate the buzzer after receiving an electronic signal compared to the tenths of a second taken for the humans to perceive a light signal) and so it beat them on most of the questions \cite{wiki_watson}.

\section{The Sequence Memoizer}

The Sequence Memoizer (developed in 2011) is a Bayesian nonparametric model for discrete sequence data. Its design is discussed in more detail in the literature review (Chapter \ref{chap:literatureReview}) and in Chapter \ref{chap:seqMem}. It is described by Bartlett and Wood \cite{bartlett2011deplump} as ``an incremental method for estimating the conditional distributions in an $n$-gram model in the limit of $n\to\infty$" (an $n$-gram being a contiguous sequence of $n$ symbols or words). One of its key advantages over previous sequence prediction systems is its compactness. Its space complexity is linear, a function of the number of nodes in its prefix tree model (explained in detail in Section \ref{sec:prefixTrees}). In the paper \textit{A Stochastic Memoizer for Sequence Data} \cite{wood2009stochastic}, the Sequence Memoizer is described as the ``$\infty$-gram" and it is claimed that it has the ``best known test perplexity (a measure of how well a model performs - this is defined explicitly in Section \ref{sec:perplexity}) on a well studied corpus". 




\section{The Purpose of the Project}



The original purpose of this project was to design and build an implementation of the Sequence Memoizer that can be deployed for integration into Watson or a therapeutic system. The resulting code must be:

\begin{itemize}
\item Stochastic
\item Memoizing
\item Functional
\item Recursive
\item Scalable
\end{itemize}

The project has evolved into a combination of research into and implementation of various smoothing $n$-gram models. Implementation of a simple $n$-gram model with different smoothing techniques was a good starting point. From there, implementation of a hierarchical model was required. This finally led into the implementation of the Sequence Memoizer.

The objective was then to compare and contrast all of these models based on their \textit{perplexity} (see Section \ref{sec:perplexity}) and their response to various training corpora. 



%\section{Report Structure}
%
%Chapter \ref{chap:literatureReview} is a literature review of previous work on this topic and the current state of the art systems. I will then move on to discuss the different technologies that were used in this project (Chapter \ref{chap:codingTheModels}). Chapters \ref{chap:n-gram}-\ref{chap:seqMem} cover the evolution of the model and its implementation in each of these stages. Finally Chapter \ref{chap:results} looks at some of the results found and compares the different implementations.