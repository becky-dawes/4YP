\chapter{Introduction}

%\section{Background}

Sequence prediction has possible applications in many different areas. We come across it every day whilst using the search engine Google when it helpfully suggests possible search topics based on what we have already typed. This concept of ``autocomplete" is something which we take for granted in this situation, but which could be extended to many different uses. A rather different application is that of a therapeutic system. Individuals with motor disorders may find some sort of text prediction service useful when using phones or computers (much like ``autocorrect"). 

\section{Watson}

This project was originally based on the work done by IBM on Watson, a system capable of answering questions. It is best known for its performance on the gameshow Jeopardy!, where it won against human opponents. Watson was trained using a variety of datasources, but it was not connected to the internet during the game. Although it did occasionally struggle to understand the concept of the questions and was unable to buzz in early, Watson's reaction times were much faster than those of the humans (about 8ms to activate the buzzer after receiving an electronic signal compared to the tenths of a second taken for the humans to perceive a light signal) and so it beat them on most of the questions.

\section{The Sequence Memoizer}

The Sequence Memoizer (developed in 2011) is a Bayesian nonparametric model for discrete sequence data. Its design is discussed in more detail in the literature review (Chapter \ref{chap:literatureReview}) and in Chapter \ref{chap:seqMem}. It is described by Bartlett and Wood \cite{bartlett2011deplump} as ``an incremental method for estimating the conditional distributions in an $n$-gram model in the limit of $n\to\infty$". One of its key advantages over previous sequence prediction systems is its compactness. Its space complexity is linear, a function of the number of nodes in its suffix tree model (explained in detail in Section \ref{sec:suffixTrees}). In the paper \textit{A Stochastic Memoizer for Sequence Data} \cite{wood2009stochastic}, the Sequence Memoizer is described as the ``$\infty$-gram" and it is claimed that it has the ``best known test perplexity (see Section \ref{sec:perplexity}) on a well studied corpus". 

\todo[inline]{More work on Background}



\section{The Purpose of the Project}

\todo[inline]{Purpose of project - specification and goals}

The purpose of this project was to design and build an implementation of the Sequence Memoizer that can be deployed for integration into Watson or a therapeutic system. The resulting code must be:

\begin{itemize}
\item Stochastic
\item Memoizing
\item Functional
\item Recursive
\item Scalable
\end{itemize}


What is the problem?

Why study it?

Main objectives

\section{Report Structure}

Chapter \ref{chap:literatureReview} is a literature review of previous work on this topic and the current state of the art systems. I will then move on to discuss the different technologies that were used in this project (Chapter \ref{chap:codingTheModels}). Chapters \ref{chap:n-gram}-\ref{chap:seqMem} cover the evolution of the model and its implementation in each of these stages. Finally Chapter \ref{chap:results} looks at some of the results found and compares the different implementations.