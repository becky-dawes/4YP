\chapter{Hierarchical Dirichlet Model} \label{chap:HierarchicalDirichletModel}

Typical bi-gram language models as described in Chapter \ref{chap:n-gram} often perform poorly. This is because the conditional count $F_{w_{t}|w_{t-1}}$ has large variance as there are so many possible couplets $w_{t-1}w_{t}$ that only a small fraction of them exist in the training data. Instead, a commonly used solution is that of Equation \ref{eq:MacKayPetoKludge} \cite{mackay1995hierarchical}. 

Deleted interpolation \cite{jelinek1980interpolated} is used to set $\lambda$. Description...

\todo[inline]{Explain deleted interpolation}

\begin{equation}
\hat{P}(w_{t}|w_{t-1})=\lambda f_{w_{t}}+(1-\lambda)f_{w_{t}|w_{t-1}}
\label{eq:MacKayPetoKludge}
\end{equation}

This model performs much better as...

\todo[inline]{Explain benefits of kludge}

MacKay and Peto \cite{mackay1995hierarchical} used a hierarchical Dirichlet model to explain Equation \ref{eq:MacKayPetoKludge}, arguing that it is possible to determine the parameters a posteriori from the data.

\section{The Theory}

\subsection{The Model}

This model is based on the idea that there is a $W\times W$ transition matrix, $Q$, which represents some unknown parameters. This means that $P(w_{t}=i|w_{t-1}=j)\equiv q_{i|j}$, where $q_{i|j}$ is an element in the matrix $Q$. A single row of $Q$ may be denoted $\boldsymbol{q}_{|j}$ and represents the probability transitions from state $j$.

Denoting the model $\mathscr{H}$, we can then begin finding values of interest. Using Bayes theorem, we can infer the parameters, as in Equation \ref{eq:HD-Q|DH}, where $D$ is the data and $k$ is dimensionality of $Q$. Here $P(D|Q,\mathscr{H})$ is the likelihood and $P(Q|\mathscr{H})$ is the prior distribution. We can then use this to find the probability of the next word in a given context, as in Equation \ref{eq:HD-wt|wt-1DH}.

\begin{align}
P(Q|D,\mathscr{H})&=\frac{P(D|Q,\mathscr{H})P(Q|\mathscr{H})}{P(D|\mathscr{H})} \nonumber
\\
&=\frac{P(D|Q,\mathscr{H})P(Q|\mathscr{H})}{\int P(D|Q,\mathscr{H})P(Q|\mathscr{H})d^{k}Q}
\label{eq:HD-Q|DH}
\end{align}

\begin{align}
P(w_{t}|W_{t-1},D,\mathscr{H})&=\int P(w_{t}|w_{t-1},Q,D,\mathscr{H})P(Q|D,\mathscr{H})d^{k}Q \nonumber
\\
&=\int q_{w_{t}|w_{t-1}}P(Q|D,\mathscr{H})d^{k}Q
\label{eq:HD-wt|wt-1DH}
\end{align}

We can now use our knowledge of the language model to define the likelihood function more clearly. If $F_{i|j}$ is the $n$-gram count of word $i$ appearing given word $j$, then it simplifies to Equation \ref{eq:HD-D|QHsimple}.

\begin{align}
P(D|Q,\mathscr{H})&=\prod_{t}q_{w_{t}|w_{t-1}} \nonumber
\\
&=\prod_{j}\prod_{i}q_{i|j}^{F_{i|j}}
\label{eq:HD-D|QHsimple}
\end{align}

\subsection{Using Dirichlet Distributions as Priors}

The Dirichlet distribution is a good choice of prior for this situation, as it is a distribution over distributions, i.e. we acknowledge that we don't know the distribution of the words in the text and so we must infer this. The Dirichlet distribution (Equation \ref{eq:HD-Dir-p|alpha-m}) is parameterised by a measure $\boldsymbol{u}=\alpha\boldsymbol{m}$, where $\boldsymbol{m}$ is a normalised measure of the $I$ components ($\sum_{i}m_{i}=1$), and $\alpha$ is a positive scalar. The normalising constant $Z(\alpha\boldsymbol{m})$ is given by Equation \ref{eq:HD-Z}, where $\Gamma(x)$ is the Gamma function (Equation \ref{eq:HD-Gamma}). $\delta(x)$ is the Dirac delta function. This normalises the distribution such that $\sum_{i}p_{i}=1$. $\alpha$ measures the sharpness of the distribution. Large $\alpha$ gives a distribution over $\boldsymbol{p}$ (a probability vector) which is sharply peaked around $\boldsymbol{m}$, the mean of the probability distribution (Equation \ref{eq:HD-DirMean}).

\begin{equation}
P(\boldsymbol{p}|\alpha\boldsymbol{m})=\frac{1}{Z(\alpha\boldsymbol{m})}\prod_{i=1}^{I}p^{\alpha m_{i}-1}\delta(\sum_{i}p_{i}-1)\equiv \text{Dirichlet}^{(I)}(\boldsymbol{p}|\alpha\boldsymbol{m})
\label{eq:HD-Dir-p|alpha-m}
\end{equation}

\begin{equation}
Z(\alpha\boldsymbol{m})=\prod_{i}\frac{\Gamma(\alpha m_{i})}{\Gamma(\alpha)}
\label{eq:HD-Z}
\end{equation}

\begin{equation}
\Gamma(x)\equiv\int_{0}^{\infty}du\ u^{x-1}e^{-u} \ \ \ \ \ \text{for }x>0
\label{eq:HD-Gamma}
\end{equation}

\begin{equation}
\int\text{Dirichlet}^{(I)}(\boldsymbol{p}|\alpha\boldsymbol{m})\boldsymbol{p}d^{I}\boldsymbol{p}=\boldsymbol{m}
\label{eq:HD-DirMean}
\end{equation}

Taking $\boldsymbol{F}=(F_{1},F_{2},...,F_{I})$ as the counts obtained, we can find the posterior of $\boldsymbol{p}$ (Equation \ref{eq:HD-p|F-alpham}). This gives a predictive distribution as in Equation \ref{eq:HD-i|F-alpham}.

\begin{align}
P(\boldsymbol{p}|\boldsymbol{F},\alpha\boldsymbol{m})&=\frac{P(\boldsymbol{F}|\boldsymbol{p})P(\boldsymbol{p}|\alpha\boldsymbol{m})}{P(\boldsymbol{F}|\alpha\boldsymbol{m})} \nonumber
\\
&=\frac{\prod_{i}p_{i}^{F_{i}}\prod_{i}p_{i}^{\alpha m_{i}-1}\delta(\sum_{i}p_{i}-1)}{Z(\alpha\boldsymbol{m})P(\boldsymbol{F}|\alpha\boldsymbol{m})} \nonumber
\\
&=\frac{\prod_{i}p_{i}^{F_{i}+\alpha m_{i}-1}\delta(\sum_{i}p_{i}-1)}{P(\boldsymbol{F}|\alpha\boldsymbol{m})Z(\alpha\boldsymbol{m})} \nonumber
\\
&=\text{Dirichlet}^{(I)}(\boldsymbol{p}|\boldsymbol{F}+\alpha\boldsymbol{m})
\label{eq:HD-p|F-alpham}
\end{align}

\begin{align}
P(i|\boldsymbol{F},\alpha\boldsymbol{m})&=\int\text{Dirichlet}^{(I)}(\boldsymbol{p}|\boldsymbol{F}+\alpha\boldsymbol{m})\boldsymbol{p}d^{I}\boldsymbol{p} \nonumber
\\
&=\frac{F_{i}+\alpha m_{i}}{\sum_{i'}F_{i'}+\alpha m_{i'}}
\label{eq:HD-i|F-alpham}
\end{align}

\noindent Rearranging this, we can find the evidence to $\alpha\boldsymbol{m}$ (Equation \ref{eq:HD-F|alpham}).

\begin{align}
P(\boldsymbol{F}|\alpha\boldsymbol{m})&=\frac{\prod_{i}p_{i}^{F_{i}+\alpha m_{i}-1}\delta(\sum_{i}p_{i}-1)}{P(\boldsymbol{p}|\boldsymbol{F},\alpha\boldsymbol{m})Z(\alpha\boldsymbol{m})} \nonumber
\\
&=\frac{\prod_{i}p_{i}^{F_{i}+\alpha m_{i}-1}\delta(\sum_{i}p_{i}-1)Z(\boldsymbol{F}+\alpha\boldsymbol{m})}{\prod_{i}p_{i}^{F_{i}+\alpha m_{i}-1}\delta(\sum_{i}p_{i}-1)Z(\alpha\boldsymbol{m})} \nonumber
\\
&=\frac{Z(\boldsymbol{F}+\alpha\boldsymbol{m})}{Z(\alpha\boldsymbol{m})} \nonumber
\\
&=\frac{\prod_{i}\Gamma(F_{i}+\alpha m_{i})}{\Gamma(\sum_{i}F_{i}+\alpha)}\frac{\Gamma(\alpha)}{\prod_{i}\Gamma(\alpha m_{i})}
\label{eq:HD-F|alpham}
\end{align}

\subsection{The Hierarchical Model $\mathscr{H}_{D}$}

We denote a hierarchical Dirichlet model $\mathscr{H}_{D}$. Hyperparameters define a probability distribution over the parameters $Q$ (hence the name "hierarchical"). Our prior on the vectors $\boldsymbol{q}_{|j}$ (which make up $Q$) uses an unknown measure $\boldsymbol{u}=\alpha\boldsymbol{m}$, as seen in the previous section. We define this prior as in Equation \ref{eq:HD-Q|alpham-H}. We can then marginalise out $\alpha\boldsymbol{m}$, as in Equation \ref{eq:HD-Q|H}.

\begin{equation}
P(Q|\alpha\boldsymbol{m},\mathscr{H}_{D})=\prod_{j}\text{Dirichlet}^{(I)}(\boldsymbol{q}_{|j}|\alpha\boldsymbol{m})
\label{eq:HD-Q|alpham-H}
\end{equation}

\begin{equation}
P(Q|\mathscr{H}_{D})=\int\prod_{j}\text{Dirichlet}^{(I)}(\boldsymbol{q}_{|j}|\alpha\boldsymbol{m})P(\alpha\boldsymbol{m})d^{I}\alpha\boldsymbol{m}
\label{eq:HD-Q|H}
\end{equation}

\subsection{Inferring the Parameters and Hyperparameters}

Level 1 inference involves inferring a distribution for $Q$. For this, we assume that we know $\boldsymbol{m}$ and $\alpha$. Equation \ref{eq:HD-Q|D-alpham-H} then gives the posterior for $Q$ using Bayes Rule. This is separable into a product over contexts $j$.

\begin{align}
P(Q|D,\alpha\boldsymbol{m},\mathscr{H}_{D})&=\frac{P(D|Q,\mathscr{H}_{D})P(Q,\mathscr{H}_{D})}{P(D,\alpha \boldsymbol{m},\mathscr{H}_{D})}\nonumber
\\
&=\frac{P(D|Q,\mathscr{H}_{D})P(Q|\alpha\boldsymbol{m},\mathscr{H}_{D})P(\alpha\boldsymbol{m},\mathscr{H}_{D})}{P(D|\alpha\boldsymbol{m},\mathscr{H}_{D})P(\alpha\boldsymbol{m},\mathscr{H}_{D})}\nonumber
\\
&=\frac{P(D|Q,\mathscr{H}_{D})P(Q|\alpha\boldsymbol{m},\mathscr{H}_{D})}{P(D|\alpha\boldsymbol{m},\mathscr{H}_{D})} \nonumber
\\
&=\prod_{j}P(\boldsymbol{q}_{|j}|D,\alpha\boldsymbol{m},\mathscr{H}_{D})
\label{eq:HD-Q|D-alpham-H}
\end{align}

\noindent $P(\boldsymbol{q}_{|j}|D,\alpha\boldsymbol{m},\mathscr{H}_{D})$ is then given by Equation \ref{eq:HD-q|D-alpham-H}. This can be used for prediction, as in Equation \ref{eq:HD-i|j-D-alpham-H}, where $f_{i|j}=\frac{F_{i|j}}{F_{j}}$ and $\lambda_{j}=\frac{\alpha}{F_{j}+\alpha}$.

\todo[inline]{Explain}

\begin{align}
P(\boldsymbol{q}_{|j}|D,\alpha\boldsymbol{m},\mathscr{H}_{D})&\propto\prod_{i}q_{i|j}^{F_{i|j}+\alpha m_{i}-1}\delta(\sum_{i}q_{i|j}-1) \nonumber
\\
&=\text{Dirichlet}^{(I)}(\boldsymbol{q}_{|j}|\boldsymbol{F}+\alpha\boldsymbol{m})
\label{eq:HD-q|D-alpham-H}
\end{align}

\begin{align}
P(i|j,D,\alpha\boldsymbol{m},\mathscr{H}_{D})&=\frac{F_{i|j}+\alpha m_{i}}{\sum_{i'}F_{i'|j}+\alpha m_{i'}} \nonumber
\\
&=\lambda_{j}m_{i}+(1-\lambda_{j})f_{i|j}
\label{eq:HD-i|j-D-alpham-H}
\end{align}

Level 2 inference involves inferring the hyperparameters. We find that the posterior distribution for $\alpha\boldsymbol{m}$ is as in Equation \ref{eq:HD-alpham|D-H}. We are interested in finding the maximum $[\alpha\boldsymbol{m}]^{MP}$ of this posterior distribution. This leads us to Equation \ref{eq:HD-i|j-D-H}. We expect the posterior to be sharply peaked in $\alpha\boldsymbol{m}$, therefore it is effectively a delta function (hence the approximation in the second line). This means that we optimise for $\alpha\boldsymbol{m}$ instead of marginalising it out. 

\begin{equation}
P(\alpha\boldsymbol{m}|D,\mathscr{H}_{D})=\frac{P(D|\alpha\boldsymbol{m},\mathscr{H}_{D})P(\alpha\boldsymbol{m}|\mathscr{H}_{D})}{P(D|\mathscr{H}_{D})}
\label{eq:HD-alpham|D-H}
\end{equation}

\begin{align}
P(i|j,D,\mathscr{H}_{D})&=\int P(\alpha\boldsymbol{m}|D,\mathscr{H}_{D})P(i|j,D,\alpha\boldsymbol{m},\mathscr{H}_{D})d^{W}(\alpha\boldsymbol{m}) \nonumber
\\
&\simeq P(i|j,D,[\alpha\boldsymbol{m}]^{MP},\mathscr{H}_{D})
\label{eq:HD-i|j-D-H}
\end{align}

\subsection{Finding the Hyperparameters}

The evidence for $\alpha\boldsymbol{m}$ is given by Equation \ref{eq:HD-D|alpham}. This can be optimised by differentiating with respect to $u_{i}=\alpha m_{i}$ and setting the result to be equal to zero. For this we use the digamma function, $\Psi(x)\equiv\frac{\partial log\Gamma(x)}{\partial x}$, as in Equation \ref{eq:HD-diff-logPD|u}. Note that $\sum_{i'}u_{i'}=\alpha$.

\begin{align}
P(D|\alpha\boldsymbol{m})&=\prod_{j}P(\boldsymbol{F}_{|j}|\alpha\boldsymbol{m}) \nonumber
\\
&=\prod_{j}\left(\frac{\prod_{i}\Gamma(F_{i|j}+\alpha m_{i})}{\Gamma(F_{j}+\alpha)}\frac{\Gamma(\alpha)}{\prod_{i}\Gamma(\alpha m_{i})}\right)
\label{eq:HD-D|alpham}
\end{align}

\begin{equation}
\frac{\partial}{\partial u_{i}}\log P(D|\boldsymbol{u})=\sum_{j}[\Psi(F_{i|j}+u_{i})-\Psi(F_{j}+\sum_{i'}u_{i'})+\Psi(\sum_{i'}u_{i'})-\Psi(u_{i})] 
\label{eq:HD-diff-logPD|u}
\end{equation}

We can now make some assumptions to make finding $\boldsymbol{u}=\alpha\boldsymbol{m}$ simpler. Since we know that $\sum_{i}m_{i}=1$, it is clear that each $m_{i}$ will be approximately equal to $\frac{1}{\text{size of vocabulary}}$. Therefore we can assume that $u_{i}=\alpha m_{i}<1$. We use the digamma recurrence relationship, $\Psi(x+1)=\Psi(x)+\frac{1}{x}$, to simplify the differential, as in Equation \ref{eq:HD-digamma-F-u}. The final line is based on the assumption that $F_{i|j}\geq 1$. We can also approximate the terms $\Psi(\alpha)-\Psi(F_{j}+\alpha)$ using the equation $Psi(x)=\frac{d}{dx}\log\Gamma(x)\simeq\log(x)-\frac{1}{2x}+O(\frac{1}{x^{2}})$.

\begin{align}
\Psi(F_{i|j}+u_{i})-\Psi(u_{i})&=\frac{1}{F_{i|j}-1+u_{i}}+\Psi(F_{i|j}-1+u_{i})-\Psi(u_{i}) \nonumber
\\
&=\frac{1}{F_{i|j}-1+u_{i}}+\frac{1}{F_{i|j}-2+u_{i}}+\Psi(F_{i|j}-2+u_{i})-\Psi(u_{i}) \nonumber
\\
&=\frac{1}{F_{i|j}-1+u_{i}}+\frac{1}{F_{i|j}-2+u_{i}}+...+\frac{1}{1+u_{i}}+\Psi(1+u_{i})-\Psi(u_{i}) \nonumber
\\
&=\frac{1}{F_{i|j}-1+u_{i}}+...+\frac{1}{1+u_{i}}+\frac{1}{u_{i}} \nonumber
\\
&\simeq \frac{1}{u_{i}}+\sum_{f=2}^{F_{i|j}}\frac{1}{f-1}-u_{i}\sum_{f=2}^{F_{i|j}}\frac{1}{(f-1)^{2}}+O(u_{i}^{2})
\label{eq:HD-digamma-F-u}
\end{align}

NEED TO EXPLAIN ALL OF THIS PROPERLY

\todo[inline]{Explain finding hyperparameters}

We can then find the optimal hyperparameters by Equation \ref{eq:HD-u-i-MP}, where $K(\alpha)$ is given by Equation \ref{eq:HD-K}. For each count $F$ and word $i$, we let $N_{Fi}$ be the number of context $j$ such that $F_{i|j}\geq F$. $F_{i}^{\text{max}}$ is the largest $F$ such that $N_{Fi}>0$. This gives Equations \ref{eq:HD-Gi} and \ref{eq:HD-Hi} for the values $G_{i}$ and $H_{i}$ respectively. $V_{i}$ is the number of entries in row $i$ of $F_{i|j}$ that are non-zero.

\begin{equation}
u_{i}^{MP}=\frac{2V_{i}}{K(\alpha^{MP})-G_{i}+\sqrt{(K(\alpha^{MP})-G_{i})^{2}+4H_{i}V_{i}}}
\label{eq:HD-u-i-MP}
\end{equation}

\begin{equation}
K(\alpha)=\sum_{j}\log\left[\frac{F_{j}+\alpha}{\alpha}\right]+\frac{1}{2}\sum_{j}\left[\frac{F_{j}}{\alpha(F_{j}+\alpha)}\right]
\label{eq:HD-K}
\end{equation}

\begin{equation}
G_{i}=\sum_{f=2}^{F_{i}^{\text{max}}}\frac{N_{fi}}{f-1}
\label{eq:HD-Gi}
\end{equation}

\begin{equation}
H_{i}=\sum_{f=2}^{F_{i}^{\text{max}}}\frac{N_{fi}}{(f-1)^{2}}
\label{eq:HD-Hi}
\end{equation}

\section{Implementation}

Equations \ref{eq:HD-u-i-MP}-\ref{eq:HD-Hi} were all implemented in code to find the optimal values for the hyperparameters. $K(\alpha)$ (Equation \ref{eq:HD-K}) is shown in line 1 of the code below. This uses a function \lstinline!sum!, which sums all elements in an input vector. Line 4 defines the function \lstinline!F-w2-given-w1!, which corresponds to $\boldsymbol{F}_{i|}$ and returns all bigrams with $i$ (or \lstinline!w2!) as the second word, i.e. returns row $i$ of $F_{i|j}$. $V_{i}$ is then found by simply summing the number of non-zero entries in this matrix, as in line 6 of the code. $N_{Fi}$ (line 8) is found by summing the number of elements in $\boldsymbol{F}_{i|}$ that have a value greater than \lstinline!f! (the input argument). We use this function to find $F_{i}^{\text{max}}$ by calling it repeatedly and incrementing \lstinline!f! each time until $N_{Fi}$ is zero (\lstinline!find-F-max!, line 10). \lstinline!G-i! and \lstinline!H-i! (lines 14 and 17) simply create a vector of values by iterating the count \lstinline!f! and then use the function \lstinline!sum! to sum all elements. \lstinline!u-i-MP! (line 20) merely takes the results from all the above equations to find the optimum value of $u_{i}$ for any  given $i$. We can find all elements in $\boldsymbol{u}$ by calling \lstinline!u-i-MP! for all unique words in the text (\lstinline!find-all-u!, line 23). $\alpha^{MP}$ is then found by summing all elements in $\boldsymbol{u}$ (line 25). This process is then repeated iteratively until the values for $\alpha$ converge. We begin by giving $\alpha$ some initial value and then call the function \lstinline!find-all-u! until the previous and new values for $\alpha$ agree to (in this case) two decimal places.

\begin{lstlisting}
(defn K "Returns K (MacKay and Peto Equation 34)" [alpha] 
	(let [count-vals (vals counts-1)] 
		(+ (sum (map #(/ (+ % alpha) alpha) count-vals)) (* 0.5 (sum (map #(/ % (* alpha (+ % alpha))) count-vals))))))
(defn F-w2-given-w1  "Returns all counts of bigrams with word w2 as the second word" [w2] 
	(zipmap (map key counts-2) (map #(if (= (second (key %)) w2) (val %) 0.0) counts-2)))
(defn V-i "Returns the number of entries in row 'i' of F-w2-given-w1 that are non-zero" [F-w2-w1] 
	(sum (map #(if (= 0 %) 0 1) F-w2-w1)))
(defn N-f-i "Returns the number of contexts w1 such that F-w2-given-w1 is greater than or equal to f" [f F-w2-w1] 
	(sum (map #(if (>= f %) 0 1) F-w2-w1)))
(defn find-F-max "Returns the largest F such that N-f-i is greater than zero" [F-w2-w1 f] 
	(if (> (N-f-i-memo f F-w2-w1) 0) 
		(find-F-max F-w2-w1 (inc f)) 
		f))
(defn G-i "Returns G-i (MacKay and Peto Equation 32)" [i] 
	(let [F-w2-w1 (F-w2-given-w1-memo i) F-vals (vals F-w2-w1) F-min 2 F-max (find-F-max-memo F-vals F-min) f-vector (take (- F-max (dec F-min)) (iterate inc F-min))] 
		(sum (map #(/ (N-f-i-memo % F-vals) (- % 1)) f-vector))))
(defn H-i "Returns H-i (MacKay and Peto Equation 33)" [i] 
	(let [F-w2-w1 (F-w2-given-w1-memo i) F-vals (vals F-w2-w1) F-min 2 F-max (find-F-max-memo F-vals F-min) f-vector (take (- F-max (dec F-min)) (iterate inc F-min))] 
		(sum (map #(/ (N-f-i-memo % F-vals)(Math/pow (- % 1) 2)) f-vector))))
(defn u-i-MP "Returns the optimal value for u-i" [alpha i] 
	(let [F-w2-w1 (vals (F-w2-given-w1-memo i)) V (V-i-memo F-w2-w1) K-alpha (K-memo alpha) G 
(defn find-all-u [alpha] 
	(zipmap (keys counts-1) (map #(u-i-MP-memo alpha %) (keys counts-1))))
(defn alpha-MP "Returns alpha based on the u-i values inputted" [u] 
	(sum (vals u)))
(defn find-optimum-u 
	([] 
		(let [u (find-all-u-memo initial-alpha) alpha (alpha-MP-memo u)] 
			(if (equal initial-alpha alpha 2) 
				u 
				(find-optimum-u alpha)))) 
	([old-alpha] 
		(let [u (find-all-u-memo old-alpha) alpha (alpha-MP-memo u)] 
			(if (equal old-alpha alpha 2) 
				u 
				(find-optimum-u alpha)))))
\end{lstlisting}

Having found optimum values for $\alpha$ and $\boldsymbol{u}$ (and therefore also for $\boldsymbol{m}$), we can then very simply find $P(i|j,D,\alpha\boldsymbol{m},\mathscr{H}_{D})$ as in Equation \ref{eq:HD-i|j-D-alpham-H}: we search the map \lstinline!counts-2!, which contains the counts of all bigrams, for the bigram $i|j$ to find $F_{i|j}$; $F_{j}$ is found by searching \lstinline!counts-1!, which contains the counts of all uni-grams, for the word $j$; $m_{i}$ is simply the corresponding element of $\boldsymbol{m}$ for word $i$.

\todo[inline]{Explain MacKay and Peto implementation}

