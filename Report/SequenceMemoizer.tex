\chapter{The Sequence Memoizer} \label{chap:seqMem}


\section{The Theory}

The Sequence Memoizer, like the model in Chapter \ref{chap:HierarchicalDirichletModel}, uses a hierarchical model. However, instead of Dirichlet processes, it instead uses Pitman-Yor processes. These are described in more detail in Section \ref{sec:PitmanYor}.

If $\Sigma$ is the set of all possible symbols, the probability of the symbol $s$ following a context $\boldsymbol{u}$ is given in Equation \ref{eq:SM1}, where $N(s)$ is the number of occurrences of $s$ in a sequence $\boldsymbol{x}$ and $G$ is a discrete distribution over $\Sigma$. $G_{\boldsymbol{u}}$ is a conditional distribution because the probability of $s$ depends on the context $\boldsymbol{u}$.

\begin{equation}
G_{\boldsymbol{u}}(s)=\frac{N(\boldsymbol{u}s)}{\sum_{s' \in \Sigma}N(\boldsymbol{u}s')}
\label{eq:SM1}
\end{equation}

\subsection{Power Law Scaling and the Pitman-Yor Process}\label{sec:PitmanYor}

Natural language follows a power law scaling. This means that there are a large number of words that appear very infrequently and a small number of words that occur very often. The graph of word counts for \textit{The Wonderful Wizard of Oz} (Figure \ref{fig:ozCountsGraph}) illustrates this. The maximum likelihood estimation given in Equation \ref{eq:SM1} will perform well for the frequently occurring symbols, but not for the rare ones. This was initially discussed in Section \ref{sec:smoothing}, which implemented smoothing to counteract the problem. However, a Pitman-Yor prior follows the power law properties of natural language much more accurately.

The Pitman-Yor process (PYP), $G=\{G(s)\}_{s\in\Sigma}$, has three parameters. These are the base distribution, $G_{0}=\{G_{0}(s)\}_{s\in\Sigma}$ (mean of PYP and prior on frequency of each symbol), the discount parameter, $\alpha$, which is between 0 and 1 (affects exponent of power law), and the concentration parameter, $c$, (affects variability around $G_{0}$). When $\alpha=0$, the PYP becomes the Dirichlet process. A PYP with discount parameter $\alpha$ and prior $G_{0}$ is written $G\sim\mathcal{PY}(\alpha,G_{0})$ (we assume in this project that $c=0$).

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{wizard-of-oz-word-counts-graph.png}
\caption{Graph of Word Counts for \textit{The Wonderful Wizard of Oz} \cite{baum2008wonderful}}
\label{fig:ozCountsGraph}
\end{figure}


%\subsection{Pitman-Yor Processes}\label{sec:PitmanYor}

\subsection{Inference}

As before, we want to know the probability that a symbol $s\in\Sigma$ occurs next. This is given by Equation \ref{eq:SM2}, where $\mathbb{E}$ is expectation with respect to the posterior $P(G|\boldsymbol{x})$. This expectation can be computed as in Equation \ref{eq:SM3}, where $N(s)$ is the count of symbol $s$ and $M(s)$ is another set of random counts satisfying $1\leq M(s')\leq N(s')$ if $N(s')>0$ and $M(s')=0$ otherwise. This implements a form of smoothing, as each count $N(s)$ is reduced by $\alpha M(s)$, with the total amount subtracted distributed across all symbols in $\Sigma$ proportionally according to their probability under $G_{0}$.

\begin{equation}
P(\boldsymbol{x}_{T+1}=s|\boldsymbol{x})=\int P(\boldsymbol{x}_{T+1}=s|G)P(G|\boldsymbol{x})dG=\mathbb{E}[G(s)]
\label{eq:SM2}
\end{equation}

\begin{equation}
\mathbb{E}[G(s)]=\mathbb{E}\left[\frac{N(s)-\alpha M(s)+\sum_{s'\in\Sigma}\alpha M(s')G_{0}(s)}{\sum_{s'\in\Sigma}N(s')}\right]
\label{eq:SM3}
\end{equation}

\todo[inline]{Am I using this?}

\subsection{Hierarchical Pitman-Yor Processes}
Similarly to Chapter \ref{chap:HierarchicalDirichletModel}, we employ the use of hierarchical processes. In this case, these are hierarchical PYPs (HPYP). We denote the empty context as $\epsilon$ and the parent of a context $\boldsymbol{u}$ (i.e. $\boldsymbol{u}$ with the first symbol from the left removed) as $\sigma(\boldsymbol{u})$. These give Equations \ref{eq:SM4a}-\ref{eq:SM4c}.

\begin{subequations}
\label{eq:SM4}
\begin{align}
G_{\epsilon}&~\sim\mathcal{PY}(\alpha_{0},G_{0}) \label{eq:SM4a}
\\
G_{\boldsymbol{u}}|G_{\sigma(\boldsymbol{u})}&\sim\mathcal{PY}(\alpha_{|\boldsymbol{u}|},G_{\sigma(\boldsymbol{u})})\ \ \ \ \text{for all }\boldsymbol{u}\in\frac{\Sigma_{n}^{*}}{\epsilon} \label{eq:SM4b}
\\
x_{i}|\boldsymbol{x}_{i-n:i-1}=\boldsymbol{u},G_{\boldsymbol{u}}&\sim G_{\boldsymbol{u}}\ \ \ \ \text{for }i=1,...,T \label{eq:SM4c}
\end{align}
\end{subequations}

The empty context has prior $G_{0}$ and discount $\alpha_{0}$ (Equation \ref{eq:SM4a}) and any context $\boldsymbol{u}$, given its parent $\sigma(\boldsymbol{u})$, has prior $G_{\sigma(\boldsymbol{u})}$ and discount $\alpha_{|\boldsymbol{u}|}$ (Equation \ref{eq:SM4b}). The distribution over each symbol $x_{i}$ in $\boldsymbol{x}$, given that its context $\boldsymbol{u}$ consists of the previous $n$ symbols $\boldsymbol{x}_{i-n:i-1}$, is $G_{\boldsymbol{u}}$ (Equation \ref{eq:SM4c}).

\todo[inline]{Re-word}

%\subsection{Prediction}
%
%\todo[inline]{SM prediction}
%
%\subsubsection{Stick Breaking Process} \cite{teh2010dirichlet}
%
%\todo[inline]{Explain stick breaking process}
%
%\begin{subequations}
%\begin{align}
%\beta_{k}&\sim\text{Beta}(1,\alpha)
%\\
%\theta_{k}^{*}&\sim H
%\\
%\pi_{k}&=\beta_{k}\prod_{l=1}^{k-1}(1-\beta_{k})
%\\
%G&=\sum_{k=1}^{\infty}\pi_{k}\delta_{\theta_{k}^{*}}
%\end{align}
%\end{subequations}
%
%$G\sim\text{Dir}(\alpha,H)$. Starting with a stick of length 1, we break it at $\beta_{1}$, assigning $\pi_{1}$ to be the length of stick we just broke off. Now recursively break the other portion to obtain $\pi_{2},\pi_{3}$ etc. The stick breaking distribution over $\pi$ is sometimes written $\pi\sim\text{GEM}(\alpha)$, where the letters stand for Griffiths, Engen and McCloskey.
%

%\subsubsection{Teh - HBLMBPYP}
%
%\begin{equation}
%G_{\boldsymbol{u}}\sim\mathcal{PY}(d_{|\boldsymbol{u}|},\theta_{|\boldsymbol{u}|},G_{\pi(\boldsymbol{u})})
%\label{eq:HBLMBPYP3}
%\end{equation}
%
%Equation \ref{eq:HBLMBPYP3}: $\pi(\boldsymbol{u})$ is the suffix of $\boldsymbol{u}$ consisting of all but the earliest word. Don't know $G_{\pi(\boldsymbol{u})}$ so recursively place a prior over it using Equation \ref{eq:HBLMBPYP3} but with parameters $\theta_{|\pi(\boldsymbol{u})|}, d_{|\pi(\boldsymbol{u})|}$ and mean vector $G_{\pi(\pi(\boldsymbol{u}))}$. Repeat until get to $G_{\emptyset}$, the vector of probabilities over the current word given the empty context. Place a prior on $G_{\emptyset}$: $G_{\emptyset}\sim\mathcal{PY}(d_{0},\theta_{0},G_{0})$, where $G_{0}$ is the global mean vector, given a uniform value of $G_{0}(w)=1/V$ for all $w\in W$. Place a uniform prior on discount parameter and a $\text{Gamma}(1,1)$ prior on strength parameters. Total number of parameters in model is $2n$.
%
%Structure of prior: a suffix tree of depth $n$, where each node corresponds to a context consisting of up to $n-1$ words and each child corresponds to adding a different word to the beginning of the context. - expresses belief that words appearing earlier in a context have (a priori) the least importance in modelling the probability of the current word.
%
%\begin{itemize}
%\item For each context $\boldsymbol{u}$ we have a sequence of words $x_{\boldsymbol{u}1}$, $x_{\boldsymbol{u}2}$,... drawn iid from $G_{\boldsymbol{u}}$ and another sequence of words $y_{\boldsymbol{u}1}$, $y_{\boldsymbol{u}2}$,... drawn iid from the parent distribution $G_{\pi(\boldsymbol{u})}$. 
%\item Use $l$ to index draws from $G_{\boldsymbol{u}}$ and $k$ to index draws from $G_{\pi(\boldsymbol{u})}$. 
%\item $t_{\boldsymbol{u}wk}=1$ if $y_{\boldsymbol{u}k}$ takes on value $w$, and $t_{\boldsymbol{u}wk}=0$ otherwise
%\item Each word $x_{\boldsymbol{u}l}$ is assigned to one of the draws $y_{\boldsymbol{u}k}$ from $G_{\pi(\boldsymbol{u})}$
%\item If $y_{\boldsymbol{u}k}$ takes on value $w$ define $c_{\boldsymbol{u}wk}$ as the number of words $x_{\boldsymbol{u}l}$ drawn from $G_{\boldsymbol{u}}$ assigned to $y_{\boldsymbol{u}k}$, otherwise, let $c_{\boldsymbol{u}wk}=0$
%\item Denote marginal counts by dots: 
%\begin{itemize}
%\item $c_{\boldsymbol{u}.k}$ is number of $x_{\boldsymbol{u}l}$s assigned value of $y_{\boldsymbol{u}k}$
%\item $c_{\boldsymbol{u}w.}$ is the number of $x_{\boldsymbol{u}l}$s with value $w$
%\item $t_{\boldsymbol{u}..}$ is the current number of draws $y_{\boldsymbol{u}k}$ from $G_{\pi(\boldsymbol{u})}$
%\end{itemize}
%\item $t_{\boldsymbol{u}w.}=0$ if $c_{\boldsymbol{u}w.}=0$
%\item $1\leq t_{\boldsymbol{u}w.}\leq c_{\boldsymbol{u}w.}$ if $c_{\boldsymbol{u}w.}>0$
%\item $c_{\boldsymbol{u}w.}=\sum_{\boldsymbol{u}':\pi(\boldsymbol{u}')=\boldsymbol{u}}t_{\boldsymbol{u}'w.}$
%\end{itemize}
%
%\framebox{\vbox{ Function DrawWord($\boldsymbol{u}$):
%
%\textit{Returns a new word drawn from $G_{\boldsymbol{u}}$}
%\\
%If $\boldsymbol{u}=0$, return $w\in W$ with probability $G_{0}(w)$
%\\
%Else, with probabilities proportional to:
%\\
%\indent	$c_{\boldsymbol{u}wk}-d_{|\boldsymbol{u}|}t_{\boldsymbol{u}wk}$:
%	\\
%	\indent\indent	assign the new word to $y_{\boldsymbol{u}k}$
%		\\
%\indent\indent		Increment $c_{\boldsymbol{u}wk}$ 
%		\\
%		\indent\indent return $w$
%		\\
%\indent	$\theta_{|\boldsymbol{u}|}+d_{|\boldsymbol{u}|}t_{\boldsymbol{u}}$:
%	\\
%	\indent\indent	assign new word to a new draw $y_{\boldsymbol{u}k^{\text{new}}}$ from $G_{\pi(\boldsymbol{u})}$
%		\\
%		\indent\indent let $w\leftarrow \text{DrawWord(}\pi(\boldsymbol{u})$
%		\\
%		\indent\indent set $t_{\boldsymbol{u}wk^{\text{new}}}=c_{\boldsymbol{u}wk^{\text{new}}}=1$
%		\\
%		\indent\indent return $w$}}
%		
%\framebox{\vbox{Function WordProb($\boldsymbol{u},w$):
%\\
%\textit{Returns the probability that the next word after context $\boldsymbol{u}$ will be $w$}
%\\
%If $\boldsymbol{u}=0$, return $G_{0}(w)$
%\\
%else, return $\frac{c_{\boldsymbol{u}w.}-d_{|\boldsymbol{u}|}t_{\boldsymbol{u}w.}}{\theta_{|\boldsymbol{u}|}+c_{\boldsymbol{u}..}}+\frac{\theta_{|\boldsymbol{u}|}+d_{|\boldsymbol{u}|}t_{\boldsymbol{u}..}}{\theta_{|\boldsymbol{u}|}+c_{\boldsymbol{u}..}}\text{WordProb}(\pi(\boldsymbol{u}),w)$}}
%
%The more a word $w$ has been drawn in context $\boldsymbol{u}$, the more likely we will draw $w$ again in context $\boldsymbol{u}$. Word $w$ will be reinforced for other contexts that share a common suffix with $\boldsymbol{u}$, with the probability of drawing $w$ increasing as the length of the common suffix increases - $w$ will be more likely under the context of the common suffix as well. 
%
%Our training data $\mathcal{D}$ consists of the number of occurrences $c_{\boldsymbol{u}w.}$ of each word $w$ after each context $\boldsymbol{u}$ of length exactly $n-1$. This corresponds to observing word $w$ drawn $c_{\boldsymbol{u}w.}$ times from $G_{\boldsymbol{u}}$. Given the training data $\mathcal{D}$, we are interested in the posterior distribution over the latent vectors $\mathcal{G}=\{G_{\boldsymbol{v}}:\text{ all contexts }\boldsymbol{v}\}$ and parameters $\boldsymbol{\Theta}=\{\theta_{m},d_{m}:0\leq m\leq n-1\}$ such that $p(\mathcal{G},\boldsymbol{\Theta}|\mathcal{D})=p(\mathcal{G},\boldsymbol{\Theta},\mathcal{D})/p(\mathcal{D})$. HCRP marginalises out each $G_{\boldsymbol{u}}$, replacing with the seating arrangement in the corresponding restaurant, $S_{\boldsymbol{u}}$: $\mathcal{S}=\{S_{\boldsymbol{v}}:\text{ all contexts }\boldsymbol{v}\}$. Therefore interested in equivalent posterior over seating arrangements instead: $p(\mathcal{S},\boldsymbol{\Theta}|\mathcal{D})=p(\mathcal{S},\boldsymbol{\Theta},\mathcal{D})/p(\mathcal{D})$. 
%
%Probability of a test word $w$ after a context $\boldsymbol{u}$ is given by Equation \ref{eq:HBLMBPYP9}, where the first probability (right) is the predictive probability under a particular setting of seating arrangements $\mathcal{S}$ and parameters $\boldsymbol{\Theta}$, and the overall predictive probability is obtained by averaging this wrt posterior over $\mathcal{S}$ and $\boldsymbol{\Theta}$. Approximate integral with samples $\{\mathcal{S}^{(i)},\boldsymbol{\Theta}^{(i)}\}_{i=1}^{I}$ drawn from $p(\mathcal{S},\boldsymbol{\Theta}|\mathcal{D})$ (Equation \ref{eq:HBLMBPYP10}). $p(w|\boldsymbol{u},\mathcal{S},\boldsymbol{\Theta})$ is given by the function WordProb($\boldsymbol{u},w$) (Equation \ref{eq:HBLMBPYP11}), where the counts are obtained from the seating arrangement $\mathcal{S}_{\boldsymbol{u}}$ in the CRP corresponding to $G_{\boldsymbol{u}}$.
%
%\begin{equation}
%p(w|\boldsymbol{u},\mathcal{D})=\int p(w|\boldsymbol{u},\mathcal{S},\boldsymbol{\Theta})p(\mathcal{S},\boldsymbol{\Theta}|\mathcal{D})d(\mathcal{S},\boldsymbol{\Theta})
%\label{eq:HBLMBPYP9}
%\end{equation}
%
%\begin{equation}
%p(w|\boldsymbol{u},\mathcal{D})\simeq\sum_{i=1}^{I}p(w|\boldsymbol{u},\mathcal{S}^{(i)},\boldsymbol{\Theta}^{(i)})
%\label{eq:HBLMBPYP10}
%\end{equation}
%
%\begin{subequations}
%\begin{align}
%p(w|0,\mathcal{S},\boldsymbol{\Theta})&=1/V
%\\
%p(w|\boldsymbol{u},\mathcal{S},\boldsymbol{\Theta})&=\frac{c_{\boldsymbol{u}w.}-d_{|\boldsymbol{u}|}t_{\boldsymbol{u}w.}}{\theta_{|\boldsymbol{u}|}+c_{\boldsymbol{u}..}}+\frac{\theta_{|\boldsymbol{u}|}+d_{|\boldsymbol{u}|}t_{\boldsymbol{u}..}}{\theta_{|\boldsymbol{u}|}+c_{\boldsymbol{u}..}}p(w|\pi(\boldsymbol{u}),\mathcal{S},\boldsymbol{\Theta})
%\end{align}
%\label{eq:HBLMBPYP11}
%\end{subequations}
%
%Use Gibbs sampling to obtain posterior samples $\{\mathcal{S},\boldsymbol{\Theta}\}$ - keeps track of current state of each variable of interest in the model and iteratively resamples the state of each variable given the current states of all other variables. States of variables will converge to required samples from posterior distribution after a sufficient number of iterations. For HPYP variables are, for each $\boldsymbol{u}$ and each word $x_{\boldsymbol{u}l}$ drawn from $G_{\boldsymbol{u}}$, the index $k_{\boldsymbol{u}l}$ of the draw from $G_{\pi(\boldsymbol{u})}$ assigned $x_{\boldsymbol{u}l}$. In CRP, this is the index of the table which the $l$\textsuperscript{th} customer sat at in the restaurant corresponding to $G_{\boldsymbol{u}}$. If $x_{\boldsymbol{u}l}$ has value $w$, it can only be assigned to draws from $G_{\pi(\boldsymbol{u})}$ that have value $w$ as well. This can either be a pre-existing draw with value $w$ or can be a new draw taking on value $w$. Relevant probabilities are given in functions DrawWord($\boldsymbol{u}$) and WordProb($\boldsymbol{u},w$), where we treat $x_{\boldsymbol{u}l}$ as the last word drawn from $G_{\boldsymbol{u}}$. This gives Equations \ref{eq:HBLMBPYP13} and \ref{eq:HBLMBPYP14}, where the superscript $-\boldsymbol{u}l$ means the corresponding set of variables or counts with $\boldsymbol{u}l$ excluded. 
%
%\begin{equation}
%p(k_{\boldsymbol{u}l}=k|\mathcal{S}^{-\boldsymbol{u}l},\boldsymbol{\Theta})\propto\frac{\max(0,c_{\boldsymbol{u}x_{\boldsymbol{u}l}k}^{-\boldsymbol{u}l}-d)}{\theta+c_{\boldsymbol{u}..}^{-\boldsymbol{u}l}}
%\label{eq:HBLMBPYP13}
%\end{equation}
%
%\begin{equation}
%p(k_{\boldsymbol{u}l}=k^{\text{new}}\text{ with }y_{\boldsymbol{u}k^{\text{new}}}=x_{\boldsymbol{u}l}|\mathcal{S}^{-\boldsymbol{u}l},\boldsymbol{\Theta})\propto\frac{\theta+dt_{\boldsymbol{u}..}^{-\boldsymbol{u}l}}{\theta+c_{\boldsymbol{u}..}^{-\boldsymbol{u}l}}p(x_{\boldsymbol{u}l}|\pi(\boldsymbol{u}),\mathcal{S}^{-\boldsymbol{u}l},\boldsymbol{\Theta})
%\label{eq:HBLMBPYP14}
%\end{equation}
%
%Straightforward correspondence to interpolated Kneser-Ney. If we restrict $t_{\boldsymbol{u}w.}$ to be at most $1$, we will get same discount value so long as $c_{\boldsymbol{u}w.}>0$ (i.e. absolute discounting) - see Equations \ref{eq:HBLMBPYP15} and \ref{eq:HBLMBPYP16}. If strength parameters are all $\theta_{|\boldsymbol{u}|}=0$, the predictive probabilities are now directly reduced to the probabilities given by interpolated Kneser-Ney. Therefore can interpret interpolated Kneser-Ney as the approximate inference scheme in the HPYP language model.
%
%\begin{equation}
%t_{\boldsymbol{u}w.}=\min(1,c_{\boldsymbol{u}w.})
%\label{eq:HBLMBPYP15}
%\end{equation}
%
%\begin{equation}
%c_{\boldsymbol{u}w.}=\sum_{\boldsymbol{u}':\pi(\boldsymbol{u}')=\boldsymbol{u}}t_{\boldsymbol{u}'w.}
%\label{eq:HBLMBPYP16}
%\end{equation}
%
\section{Prefix Trees} \label{sec:prefixTrees}

The strings occurring in the training corpus can be stored most efficiently in a prefix tree. These are best explained by means of an example. A simple prefix tree is built up by considering each prefix of a string individually (a prefix being any substring where the first symbol is the beginning of the original string - i.e. for the word ``the", the possible prefixes are ``t", ``th" and ``the"). A tree is then built up starting with the shortest prefix, then the next shortest etc. Any common substrings encountered cause the tree to branch. Construction of the prefix tree for ``mississippi" is detailed below. %Note that the symbol ``\$" denotes the end of the word.

\qtreecenterfalse

1.\ \ \ \ \ \ \ \ \ \Tree [.$\bullet$  m ]\ \ \ \ \ \ \ \ \ \ \ \ 2.\ \ \ \ \ \ \Tree [.$\bullet$ m im ]\ \ \ \ \ \ \ \ \ \ \ \ 3.\ \ \ \ \ \ \Tree [.$\bullet$ m im sim ]\ \ \ \ \ \ \ \ \ \ \ \ 4.\ \ \ \ \ \ \Tree [.$\bullet$ m im [.s im sim ] ]
\\
\\
 5.\ \ \ \ \ \ \ \ \Tree [.$\bullet$ m [.i m ssim ] [.s im sim ] ]\ \ \ \ \ \ \ \ \ \ \ 6.\ \ \ \ \ \ \ \ \Tree [.$\bullet$ m [.i m ssim ] [.s sim [.i m ssim ] ] ]
\qtreecentertrue
\\
\\
7.\ \ \ \ \Tree [.$\bullet$ m [.i m ssim ] [.s [.si ssim m ] [.i m ssim ] ] ]
\\
\\
8.\ \ \ \ \Tree [.$\bullet$ m [.i m [.ssi ssim im ] ] [.s [.si ssim m ] [.i m ssim ] ] ]
\\
\\
 9.\ \ \ \ \Tree [.$\bullet$ m [.i m [.ssi ssim im ] ] [.s [.si ssim m ] [.i m ssim ] ] pississim ]
\\
\\
10.\ \ \ \ \Tree [.$\bullet$ m [.i m [.ssi ssim im ] ] [.s [.si ssim m ] [.i m ssim ] ] [.p ississim pississim ] ]
\\
\\
11.\ \ \ \ \Tree [.$\bullet$ m [.i m ppississim [.ssi ssim im ] ] [.s [.si ssim m ] [.i m ssim ] ] [.p ississim pississim ] ]

\subsection{The Chinese Restaurant Process} \label{sec:chineseRestaurantProcess}

The Chinese Restaurant Process (CRP) is commonly used in language modelling. Imagine a Chinese restaurant with an infinite number of tables. At a particular time, a new customer may choose to sit either at an existing table, or at a new unoccupied table. In our case, tables are labelled with symbols. There can be more than one table with a given symbol. Each symbol in a corpus can either be assigned to a corresponding draw from the base distribution (equivalent to seating a customer at an existing table) or to a new draw from the base distribution (equivalent to seating a customer at a new table).


If $n$ customers are already seated giving $N$ occupied tables, the probabilities of the next customer sitting at an occupied table or choosing a new table are given by Equations \ref{eq:CRPExisting} and \ref{eq:CRPNew} respectively \cite{wood2009hierarchical}, where $z_{n+1}$ is the table at which the customer will sit, $n_{k}$ is the number of customers sitting at the $k$\textsuperscript{th} table, $d$ is the discount parameter, $\alpha$ is the concentration parameter and $K$ denotes a new table. 

\begin{equation}
P(z_{n+1}=k)=\frac{n_{k}-d}{n+\alpha}
\label{eq:CRPExisting}
\end{equation}

\begin{equation}
P(z_{n+1}=K)=\frac{\alpha+Nd}{n+\alpha}
\label{eq:CRPNew}
\end{equation}



The CRP is implemented in our prefix tree representation of the training corpus as a Multi-Floor CRP (MFCRP). This is a hierarchical distribution. Seating a customer at a new table means drawing from the parent restaurant (i.e. creating a new table based on the CRP probabilities in the parent node).

% This says that customers in any given restaurant either must be associated with direct observations of draws from the underlying $G_{\boldsymbol{u}}$, or must have come from a table in a child restaurant. Each table $k$ in the MFCRP is given a label $\psi_{v}^{k}$ which is an i.i.d. draw from the base distribution. We can achieve this by picking component $w$ with probability $\lambda_{w\rightarrow v}$ and drawing from the chosen parent distribution $G_{w}$. Let $s_{v}^{k}$ be the chosen component. This corresponds to table $k$ being located on floor $s_{v}^{k}$ of a multi-floor- restaurant.


\subsection{Updating Our Representation of a Prefix Tree}

The suffix tree drawn in Section \ref{sec:prefixTrees} can be extended to include the CRP representation discussed above. The updated tree is constructed similarly by starting with the shortest prefix and working through prefixes with increasing length. For each prefix considered, the branch is labelled with all but the last letter of the prefix, which is instead represented as a table with a customer in the child node's restaurant. Figure \ref{fig:theCRPTree} shows the new representation for the word ``the".



\def\restaurantOne#1#2#3{
\begin{scope}[shift={#3}]
\draw (0,0) -- (2.8,0) -- (2.8,1.25) -- (0,1.25) -- cycle ;
\draw (0.35 ,0.75) circle (0.25);
\node [draw=none, font=\normalsize] at (0.35,0.75) {#1};
\node [draw=none, font=\normalsize] at (0.35,0.25) {#2};
\end{scope}
}

\def\restaurantTwo#1#2#3#4#5{
\begin{scope}[shift={#5}]
\draw (0,0) -- (2.8,0) -- (2.8,1.25) -- (0,1.25) -- cycle;
\draw (0.35 ,0.75) circle (0.25);
\node [draw=none, font=\normalsize] at (0.35,0.75) {#1};
\node [draw=none, font=\normalsize] at (0.35,0.25) {#2};
\draw (1.05,0.75) circle (0.25);
\node [draw=none,font=\normalsize] at (1.05,0.75) {#3};
\node [draw=none,font=\normalsize] at (1.05,0.25) {#4};
\end{scope}
}

\def\restaurantThree#1#2#3#4#5#6#7{
\begin{scope}[shift={#7}]
\draw (0,0) -- (2.8,0) -- (2.8,1.25) -- (0,1.25) -- cycle;
\draw (0.35 ,0.75) circle (0.25);
\node [draw=none, font=\normalsize] at (0.35,0.75) {#1};
\node [draw=none, font=\normalsize] at (0.35,0.25) {#2};
\draw (1.05,0.75) circle (0.25);
\node [draw=none,font=\normalsize] at (1.05,0.75) {#3};
\node [draw=none,font=\normalsize] at (1.05,0.25) {#4};
\draw (1.75,0.75) circle (0.25);
\node [draw=none,font=\normalsize] at (1.75,0.75) {#5};
\node [draw=none,font=\normalsize] at (1.75,0.25) {#6};
\end{scope}
}

\def\restaurantFour#1#2#3#4#5#6#7#8#9{
\begin{scope}[shift={#9}]
\draw (0,0) -- (2.8,0) -- (2.8,1.25) -- (0,1.25) -- cycle ;
\draw (0.35 ,0.75) circle (0.25);
\node [draw=none, font=\normalsize] at (0.35,0.75) {#1};
\node [draw=none, font=\normalsize] at (0.35,0.25) {#2};
\draw (1.05,0.75) circle (0.25);
\node [draw=none,font=\normalsize] at (1.05,0.75) {#3};
\node [draw=none,font=\normalsize] at (1.05,0.25) {#4};
\draw (1.75,0.75) circle (0.25);
\node [draw=none,font=\normalsize] at (1.75,0.75) {#5};
\node [draw=none,font=\normalsize] at (1.75,0.25) {#6};
\draw (2.45,0.75) circle (0.25);
\node [draw=none,font=\normalsize] at (2.45,0.75) {#7};
\node [draw=none,font=\normalsize] at (2.45,0.25) {#8};
\end{scope}
}

\begin{figure}[h!]
\centering
\begin{tikzpicture}
\restaurantThree{h}{1}{e}{1}{t}{1}{(-1.4,0)};
\restaurantOne{e}{1}{(-4.3,-2)};
\restaurantOne{h}{1}{(-1.4,-2)};
\restaurantOne{t}{1}{(1.5,-2)};

\draw (-0.7,0)--(-3.9,-0.75) node [midway,fill=white] {ht};
\draw (0,0)--(0,-0.75) node [midway,fill=white]{t};
\draw (0.7,0)--(2.9,-0.75);
\end{tikzpicture}
\caption{Prefix Tree with CRP Representation for ``the"}
\label{fig:theCRPTree}
\end{figure}






This becomes more complex for words which require branching of nodes (such as ``mississippi"). The total number of customers for any letter in a given restaurant must be equal to the total number of tables in all of that restaurant's child restaurants. For simplicity, all customers have been seated at existing tables (where possible) for their letter in Figure \ref{fig:mississippiCRPTree}, but a more comprehensive tree would have branches for all possible arrangements of customers and tables.

\begin{figure}[h]
\centering
\begin{tikzpicture}[thick,scale=0.55, every node/.style={transform shape},font=\large]
 \restaurantFour{p}{2}{s}{2}{i}{3}{m}{1}{(-1.4,0)}  ;
\restaurantOne{i}{1}{(-7.4,-2)}  ;
\restaurantTwo{s}{2}{p}{1}{(-13.4,-2)};
\restaurantTwo{s}{1}{i}{1}{(-1.4,-2)};
\restaurantOne{m}{1}{(4.6,-2)};
\restaurantTwo{p}{1}{i}{1}{(10.6,-2)};
 
 \draw (-0.95,0)--(-6,-0.75) node [midway,fill=white] {m};
 \draw (-0.5,0)--(-12,-0.75) node [midway,fill=white] {i};
 \draw (-0.05,0)--(0,-0.75) node [midway,fill=white] {s};
 \draw (0.4,0)--(6,-0.75);
 \draw (0.85,0)--(12,-0.75) node [midway,fill=white] {p};
 
 \restaurantOne{s}{1}{(-16.4,-4)};
 \restaurantTwo{s}{1}{p}{1}{(-10.4,-4)};
 
 \draw (-12.47,-2)--(-15,-2.75) node [midway,fill=white] {m};
 \draw (-11.54,-2)--(-9,-2.75) node [midway,fill=white] {ssi};
 
 \restaurantOne{p}{1}{(-8.9,-6)};
 \restaurantOne{s}{1}{(-11.9,-6)};
 
 \draw (-9.47,-4)--(-10.5,-4.75) node [midway,fill=white] {m};
 \draw (-8.54,-4)--(-7.5,-4.75) node [midway,fill=white] {ssim};
 
 \restaurantOne{i}{2}{(-4.4,-4)};
 \restaurantOne{s}{2}{(1.6,-4)};
 
 \draw (-0.47,-2)--(-3,-2.75) node [midway,fill=white] {si};
 \draw (0.46,-2)--(3,-2.75) node [midway,fill=white] {i};
 
 \restaurantOne{i}{1}{(-5.9,-6)};
 \restaurantOne{i}{1}{(-2.9,-6)};
 
 \draw (-3.47,-4)--(-4.5,-4.75) node [midway,fill=white] {ssim};
 \draw (-2.54,-4)--(-1.5,-4.75) node [midway,fill=white] {m};
 
 
 \restaurantOne{s}{1}{(0.1,-6)};
 \restaurantOne{s}{1}{(3.1,-6)};
 
 \draw (2.53,-4)--(1.5,-4.75) node [midway,fill=white] {ssim};
 \draw (3.46,-4)--(4.5,-4.75) node [midway,fill=white] {m};
 
 \restaurantOne{p}{1}{(7.4,-4)};
 \restaurantOne{i}{1}{(13.4,-4)};
 
 \draw (11.53,-2)--(8.8,-2.75) node [midway,fill=white] {ississim};
 \draw (12.46,-2)--(14.8,-2.75) node [midway,fill=white] {pississim};
 

\end{tikzpicture}
 \caption{Prefix Tree with CRP Representation for ``mississippi"}
 \label{fig:mississippiCRPTree}
\end{figure}




\section{Building the Prefix Tree}



A new class of object was created to store the information required for each node object: \lstinline!(defrecord restaurant_node [range children restaurant depth])!, where the fields are defined as follows:

\begin{itemize}
\item \lstinline!range!: a vector containing the indices (with respect to the original string) of the first and last letters of the branch's substring
\item \lstinline!children!: a hash map of the node's children (with keys as the first letter of each child branch's substring and values as new \lstinline!restaurant_node! objects)
\item \lstinline!restaurant!: a hash map of restaurant tables (with keys as the unique symbols and values as a vector with each element denoting the number of fusers at a particular table)
\item \lstinline!depth!: a vector containing the start and end depth of the branch's context (based on an uncoagulated prefix trie - see Section \ref{sec:predictingText})
\end{itemize}

\noindent The final tree for ``the" is as in Figure \ref{fig:theCodePrefixTree}.

\begin{figure}[h!]
\centering
\singlespacing
\begin{lstlisting}
{:range [],
 :children
 {"h" {:range [0 2], :children {}, :restaurant {"e" [1]}, :depth [1 2]},
  "t" {:range [0 1], :children {}, :restaurant {"h" [1]}, :depth [1 1]},
  "" {:range [0 0], :children {}, :restaurant {"t" [1]}, :depth [0 0]}},
 :restaurant {"e" [1], "h" [1], "t" [1]},
 :depth [0 0]}
\end{lstlisting}
\doublespacing
\caption{Prefix Tree for ``the"}
\label{fig:theCodePrefixTree}
\end{figure}

%\noindent The root node has an empty range vector as there is no context. It has three children starting with the letters "h", "e" and the empty context. The node corresponding to "h" has a range [1 3], which represents the whole word. Its restaurant contains one table for "t" with one customer. The node corresponding to "e" has a range [2 3], which represents the substring "he". Its restaurant contains one table for "h" with one customer. The node corresponding to the empty context has a range [0 0], which represents an empty string. Its restaurant has one table for "e" with one customer. None of these child nodes have children of their own. The root node has a restaurant with one table each for "t", "h"  and "e", each with one customer. It is easy to see how this text-based representation relates to the diagram in Figure \ref{fig:theSuffixTree}.
%
%The \lstinline!range! vectors are created using the method \lstinline!create-indices! and dereferenced using the method \lstinline!dereference-indices! (below).
%
%\begin{lstlisting}
%(defn create-indices "Returns a vector containing the indices of the first and last characters of the sub-word in the word" [word sub-word] 
%	(let [first-char-index (.indexOf word sub-word)] 
%		[first-char-index (+ first-char-index (count sub-word))]))
%(defn dereference-indices "Returns the sub-word from word given the first and last character indicies" [word indices] 
%	(str (subs word (first indices) (second indices))))
%  \end{lstlisting}
 
 \subsection{Tree Building Procedure}\label{sec:treeBuildingProcedure}

 Figure \ref{fig:buildTreeFlowchart} details the tree building procedure. The cycle continues until the longest prefix has been constructed. A main \lstinline!build_tree! function was written with several auxiliary functions supporting it. 

    \begin{figure}[h!]
    \centering
\begin{tikzpicture}[node distance = 4cm, auto,>=triangle 45]

    \tikzstyle{every node}=[font=\scriptsize]
    % Place nodes
    \node [block] (start) {Start with shortest prefix};
    \node [decision, below of=start] (child_check) {Do any of root node's children match last symbol of context?};
 \node [decision, below of=child_check] (match_range_check) {Does child's range match context?};
 \node [decision, below of=match_range_check] (table_check) {Is there already a table for the last symbol of the prefix in the node's restaurant?};
 \node [block, left of=table_check, node distance=7.25cm] (inc_count) {Seat new customer};
 \node [block, below of=table_check] (new_table) {Create table with one customer};
 \node [block, right of=match_range_check, node distance=5.25cm] (new_root_node) {Set matching node as new root node};
 \node [block, left of=match_range_check,node distance=5.4cm] (branch) {Branch node into two sections and seat customer for last symbol of prefix};
 \node [block, left of=new_table, node distance=9cm] (create_node) {Create child node for context with a table with one customer for last symbol of prefix};
 \node [block, below of=new_table] (consistency_check) {Check that number of customers in root node's restaurant is equal to total number of tables for that symbol in its children's restaurants};
 \node [block, right of=consistency_check, node distance =7cm] (repeat) {Consider next longest prefix};

    % Draw edges
    \path [line] (start) -- (child_check);
    \path [line] (child_check) -- node {yes} (match_range_check);
    \path [line] (match_range_check) -- node {exact match} (table_check);
    \path [line] (table_check) -- node {yes} (inc_count);
    \path [line] (table_check) -- node {no} (new_table);
    \path [line] (match_range_check) -- node[above]{part of context} node[below]{matches all of range} (new_root_node);
    \path [line] (new_root_node) |- (child_check);
    \path [line] (match_range_check) -- node[above]{part of range} node[below]{matches all of context} (branch);
    \path [line] (child_check) -| node [near start] {no} (create_node);
    \path [line] (inc_count) |- (consistency_check);
    \path [line] (new_table) -- (consistency_check);
    \path [line] (branch) |- (consistency_check);
    \path [line] (create_node) |- (consistency_check);
    \path [line] (consistency_check) -- (repeat);
    \path [line] (repeat) |- (child_check);

\end{tikzpicture}

\caption{Tree Construction Flowchart}
\label{fig:buildTreeFlowchart}
\end{figure}



Checking whether a node's range matches a given context was done using regular expressions. It is necessary to discern between full and partial matches with both the range and the context, so a function  \lstinline!check_range! was written which returns a vector of the form \lstinline![matching_string! \newline \lstinline!is_match_length_equal_to_range_length is_context_length_greater_than_match_length]!.

The function \lstinline!branch! is used to split a node when a context matches part of the branch substring. It dissociates the current node from the tree and then associates a new node with the desired new range, as well as associating a child node that is a copy of the old node, but with a context that is only the second part of the original range.
 
% The method for building a tree is as follows:
% \begin{enumerate}
% \item Start with shortest suffix
% \item Create child node with key "", range [0 0], no children and restaurant containing a table with one customer for the suffix symbol
% \item Now consider the next shortest suffix
% \item Create a child node with key as the first symbol of the \textit{context} (all symbols in the suffix except the first), range as the indices of the context, no children and restaurant containing a table with one customer for the first symbol of the suffix
% \item Now consider the third shortest suffix
% \item Check if any of the children of the root node match the first symbol of the context \label{step:repeatStep}
% \begin{enumerate}
% \item If there is a match, check whether its range matches the context
% \begin{enumerate}
% \item If the range exactly matches the context, check if there is already a table for the first symbol of the suffix in the node's restaurant \label{step:exactMatch}
% \begin{enumerate}
% \item If there is a table, increment the customer count
% \item If there is not a table, create one with one customer
% \end{enumerate}
% \item If the range matches part of the context (i.e. the context is longer than the matching range),repeat from step \ref{step:repeatStep} with the matching node as the root node \label{step:contextLongerThanRange}
% \item If part of the range matches the context (i.e. the context is shorter than the matching range), branch the node into two sections and instantiate a table for the first symbol of the suffix \label{step:branchNode}
% \end{enumerate}
% \item If there is no match, create a child node for the context with a table with one customer for the first symbol of the suffix in its restaurant
% \end{enumerate}
% \item Check that the number of customers for each symbol in the root node's restaurant is equal to the total number of tables for that symbol in its children's restaurants
% \item Repeat from step \ref{step:repeatStep} for suffixes of increasing length until the whole string has been considered
% \end{enumerate}
 
% \subsection{Auxiliary Functions}
% 
% \subsubsection{Checking the Consistency Between Child Table Count and Parent Customer Count}\label{sec:customerTableConsistency}
% 
% The function \lstinline!check_table_customer_consistency! checks that the total number of tables for a given symbol in a node's children equal the number of customers for that symbol in the root node's restaurant.
% 
% \begin{lstlisting}
%(defn check_table_customer_consistency "Checks that number of tables for the given letter in the node's children is equal to the number of customers for the letter in the node's restaurant" [node letter params]
%	^(^let [table_count (find_child_table_count_memo node letter params)
%	params (into params [:restaurant letter])
%	parent_restaurant £(£if (nil? (get-in node params)) 
%		[0 0] 
%		(get-in node params)£)£]
%			!(!if (and (not (zero? table_count)) (not (= table_count (second parent_restaurant)) ))
%				|(|if (<= (first parent_restaurant) table_count)
%					^(^if (zero? (first parent_restaurant))
%						£(£assoc-in node params [1 table_count]£)£
%						£(£assoc-in node params [(first parent_restaurant) table_count]£)£^)^
%					^(^assoc-in node params [table_count table_count]^)^|)|
%				node!)!^)^)
%\end{lstlisting}
%
%\noindent If the number of customers in the root node's restaurant is not equal to the number of tables in its children's restaurants, the number number of customer's in the root node's restaurant is updated to match.
%
%The function \lstinline!check_consistency_all_tables! (below) calls \lstinline!check_table_customer_consistency! for all table letters passed to it.
%
%\begin{lstlisting}
%(defn check_consistency_all_tables "Checks that the number of tables in all children is equal to the number of customers in the node's restaurant for all letters"
%	[node tables params]
%		^(^if (<= 1 (count tables))
%			£(£check_table_customer_consistency_memo node (first tables) params£)£
%			£(£check_consistency_all_tables (check_table_customer_consistency_memo node (first tables) params) (rest tables) params£)£^)^)
%\end{lstlisting}
%
%\subsubsection{Checking Whether a Node's Range Matches the Context}
%
%The function \lstinline!check_range! returns a vector of the form \lstinline![matching_string! \newline \lstinline!is_match_length_equal_to_context_length is_context_length_greater_than_match_length]!. It uses regular expressions to determine whether the context string matches that of the dereferenced range indices. If the two strings do not match, the function is called again with the final symbol removed from the dereferenced range letters. 
%
%\begin{lstlisting}
%(defn check_range "Compares the suffix to the range on the child node and returns a vector with the values [(matching string) (is the match length equal to the range length) (is the suffix longer than the match)]"
%	^(^[range suffix root_word]
%		£(£check_range range suffix root_word (count (dereference-indices-memo root_word range)))£)£^)^
%	^(^[range suffix root_word length]
%		£(£let [range_letters (dereference-indices-memo root_word range) match (re-find (re-pattern (str "^" range_letters)) suffix)]
%			!(!if (nil? match)
%				|(|check_range (create-indices-memo root_word (subs range_letters 0 (dec (count range_letters)))) suffix root_word length|)|
%				[match (= length (count match)) (> (count suffix) (count match))]!)!£)£^)^)
%\end{lstlisting}
%
%\subsubsection{Branching a Node}
%
%The function \lstinline!branch! is used to branch a node when a context matches part of the branch substring. It dissociates the current node from the tree (line 9) and then associates a new node with the desired new range, as well as associating a child node that is a copy of the old node, but with a context that is only the second part of the original range. Finally, the customer-table consistency is checked with \lstinline!check_consistency_all_tables!.
%
%\begin{lstlisting}
%(defn branch "Replaces the node with another with range new_range and children a copy of the old node but with the remaining range"
%	[node letter new_range new_letter params]
%		^(^let [old_range (get-in node (into params [:children letter :range]))
%		restaurant (get-in node (into params [:children letter :restaurant]))
%		children (get-in node (into params [:children letter :children]))]
%			£(£check_consistency_all_tables_memo
%				!(!assoc-in
%					|(|assoc-in
%						^(^dissoc-in node (into params [:children letter])^)^
%					(into params [:children letter]) (build_restaurant_node new_range)|)|
%				(into params [:children letter :children new_letter]) (build_restaurant_node [(+ (first old_range) (- (last new_range) (first new_range))) (last old_range)] children restaurant)!)!
%			(keys restaurant)
%		(into params [:children letter])£)£^)^)
%\end{lstlisting}
 
% \subsection{The \lstinline!build_tree! Method}
% 
% The method \lstinline!build_tree! (below) follows the procedure detailed in Section \ref{sec:treeBuildingProcedure}:
% \begin{lstlisting}
% (defn build_tree "Builds a suffix tree for the given word"
% 	^(^[word] (build_tree word (build_restaurant_node) word []) ^)
%	(^[word root_node root_word params] 
%		!(!let [root_node 
%			£(£if (empty? params) 
%				|(|cond 
%					(> 2 (count word)) root_node
%					(= 2 (count word)) (build_tree (str (reduce str (rest word))) root_node root_word params)
%					:else (build_tree (reduce str (rest word)) root_node root_word params)|)|
%			root_node£)£
%		suffix 
%			£(£cond 
%				(< 2 (count word)) (reduce str (rest word))
%				(= 2 (count word)) (str (reduce str (rest word)))
%				:else ""£)£
%		suffix_start (str (first suffix))
%		letter (str (first word))
%		children (get-in root_node (into params [:children]))]
%			£(£if (empty? children)
%				|(|check_table_customer_consistency_memo
%					^(^assoc-in root_node (into params [:children suffix_start])
%						!(!build_restaurant_node (create-indices-memo root_word suffix) letter!)!^)^
%					letter params|)|
%				|(|if (check_for_children_memo suffix_start (keys children))
%					^(^let [match_results (check_range_memo (get-in root_node (into params [:children suffix_start :range])) suffix root_word)]
%						!(!if (second match_results)
%							£(£if (nth match_results 2)
%								|(|if (empty? (get-in root_node [:children suffix_start :children]))
%									^(^check_table_customer_consistency_memo 
%										!(!check_table_customer_consistency_memo
%											£(£assoc-in
%												|(|build_tree (str letter (subs suffix (count (first match_results)) (count suffix))) root_node root_word (into params [:children suffix_start])|)|
%											[:children suffix_start :children ""] (build_restaurant_node [0 0] (get-in root_node [:children suffix_start :children]) (get-in root_node [:children suffix_start :restaurant]))£)£
%										letter (into params [:children suffix_start])!)!
%									letter params^)^
%									^(^check_table_customer_consistency_memo
%										!(!check_table_customer_consistency_memo
%											£(£build_tree (str letter (subs suffix (count (first match_results)) (count suffix))) root_node root_word (into params [:children suffix_start])£)£
%										letter (into params [:children suffix_start])!)!
%									letter params^)^|)|
%								|(|check_table_customer_consistency_memo root_node letter params|)|£)£
%							£(£let [new-range (create-indices-memo root_word (first match_results)) matching_branch_range (get-in root_node (into params [:children suffix_start :range])) new_branch (dereference-indices-memo root_word new-range)]
%								|(|check_table_customer_consistency_memo
%									^(^check_table_customer_consistency_memo
%										!(!assoc-in
%											£(£branch_memo root_node suffix_start new-range (subs (dereference-indices-memo root_word matching_branch_range) (count (first match_results)) (inc (count (first match_results)))) params£)£
%										(into params [:children suffix_start :children (subs suffix (count (first match_results)) (inc (count (first match_results))))]) (build_restaurant_node (create-indices-memo root_word (subs suffix (count (first match_results)) (count suffix))) letter)!)!
%									letter (into params [:children suffix_start])^)^
%								letter params|)|£)£!)!^)^
%					^(^check_table_customer_consistency_memo
%						!(!assoc-in root_node (into params [:children suffix_start]) (build_restaurant_node (create-indices-memo root_word suffix) letter)!)!
%					letter params^)^|)|£)£!)!^)^)
%\end{lstlisting}
%
%

%\lstinline!build_tree! can be called with one argument or four arguments. When called with one argument, it calls itself with four arguments, generating values for the other arguments. The four arguments are:

The \lstinline!build_tree! function has four input arguments:

\begin{itemize}
\item \lstinline!word!: the current prefix being considered
\item \lstinline!root_node!: the root node of the tree
\item \lstinline!root_word!: the original string (from which indices are created and dereferenced)
\item \lstinline!params!: the parameters describing the node being built on (for example, the object described by \lstinline![:children "a"]! is the child of the root node whose range begins with the letter ``a"; the object described by \lstinline![:children "a" :range]! is the range of that node) 
\end{itemize}

\noindent It then recursively calls itself, removing one character from the beginning of \lstinline!word! until the tree for the final character is constructed. This then becomes the new \lstinline!root_node!. This  continues for each prefix, ensuring that the tree is built up from the shortest prefix first. 

During construction, customers were assigned to tables based on the CRP probabilities. For example, when constructing the prefix tree for the string ``miss", there are two child nodes with one table  each for the symbol ``s". Therefore, there are two possible seating arrangements in the root node: one table for ``s" with two customers, or two tables for ``s" with one customer each. When seating the second customer, the table is assigned by calculating the CRP probabilities normalised over the ``s" tables only. This means that the total number of tables is 1 and the total number of customers is 1. The code assigns tables by generating the probabilities of all possible options and then assigning each to a probability range (.i.e. if there is a 50\% chance of seating the customer at an existing or new table, then we could say that any number up to and including 0.5 corresponds to an existing table, and any number between 0.5 and 1 corresponds to a new table). A random number between 0 and 1 is then generated and the customer is seated according to which probability range this lies in. A possible full tree for the word ``mississippi" is shown in Figure \ref{fig:mississippiCodePrefixTree}.

%\noindent Lines 4-10 re-define the root node: if the suffix is longer than one symbol, the function calls itself recursively, removing the first symbol from the beginning of \lstinline!word!; otherwise the root node is unchanged (the different methods in lines 8 and 9 are due to Clojure's treatment of strings vs. characters, but produce the same results). This ensures that the tree is built up from the shortest suffix first. The data structure created by the shortest suffix is then passed back to its calling function so that the node(s) for the next shortest suffix can be built onto it. 
%
%Line 19 checks if the root node has any children. If it does not, lines 20-23 are executed. These associate a new node into the root node's children with the relevant range and table and then check table-customer consistency. 
%
%If the root node does have children, the function \lstinline!check_for_children! returns true if any of their keys are the same as the first symbol of the context. If there are no matches, a new node is associated into the root node's children (lines 50-51) in the same way as mentioned in the previous paragraph. If a match is found, the function \lstinline!check_range! is called to return the variable \lstinline!match_results!. If \lstinline!match_results! is of the form \lstinline![match true true]!, the first part of the context being considered exactly matches the substring of the branch (as in step \ref{step:contextLongerThanRange} in Section \ref{sec:treeBuildingProcedure}). In this case, \lstinline!build_tree! is called again with the matching node as the root node and with the matching part of the context removed (lines 28-40). If instead, \lstinline!match_results! has the form \lstinline![match true false]!, the context exactly matches the substring of the branch (as in step \ref{step:exactMatch} in Section \ref{sec:treeBuildingProcedure}). In this case, \todo{?} Finally, if \lstinline!match_results! has the form \lstinline![match false true]!, the first part of the context matches the first part of the branch substring (as in step \ref{step:branchNode} in Section \ref{sec:treeBuildingProcedure}). In this case, the node must be branched (lines 42-49).
%





 
 \begin{figure}[h!]
\centering
\singlespacing
\begin{lstlisting}
{:range [], :children
 {"p" {:range [8 9], :children
   {"p" {:range [0 9], :children {}, :restaurant {"i" [1]}, :depth [2 10]},
    "i" {:range [0 8], :children {}, :restaurant {"p" [1]}, :depth [2 9]}},
   :restaurant {"i" [1], "p" [1]}, :depth [1 1]},
  "i" {:range [1 2], :children
   {"s" {:range [1 4], :children
     {"s" {:range [0 4], :children {}, :restaurant {"p" [1]}, :depth [5 8]},
      "m" {:range [0 1], :children {}, :restaurant {"s" [1]}, :depth [5 5]}},
     :restaurant {"p" [1], "s" [1]}, :depth [2 4]},
    "m" {:range [0 1], :children {}, :restaurant {"s" [1]}, :depth [2 2]}},
   :restaurant {"p" [1], "s" [1 1]}, :depth [1 1]},
  "s" {:range [2 3], :children
   {"s" {:range [1 3], :children
     {"s" {:range [0 4], :children {}, :restaurant {"i" [1]}, :depth [4 7]},
      "m" {:range [0 1], :children {}, :restaurant {"i" [1]}, :depth [4 4]}},
     :restaurant {"i" [2]}, :depth [2 3]},
    "i" {:range [1 2], :children
     {"s" {:range [0 4], :children {}, :restaurant {"s" [1]}, :depth [3 6]},
      "m" {:range [0 1], :children {}, :restaurant {"s" [1]}, :depth [3 3]}},
     :restaurant {"s" [1 1]}, :depth [2 2]}},
   :restaurant {"i" [1], "s" [1 1]}, :depth [1 1]},
  "m" {:range [0 1], :children {}, :restaurant {"i" [1]}, :depth [1 1]},
  "" {:range [0 0], :children {}, :restaurant {"m" [1]}, :depth [0 0]}},
 :restaurant {"p" [1 1], "s" [1 1 1 1], "i" [1 1 1], "m" [1]}, :depth [0 0]} 
 \end{lstlisting}
\doublespacing
\caption{Prefix Tree for ``mississippi"}
\label{fig:mississippiCodePrefixTree}
\end{figure}

 The \lstinline!range! field reduces the memory required to store the tree as all branches are now defined by a two-element vector instead of string (which could have unlimited length). Storing the node children as a hash map allows us to identify each branch by the first letter of its substring, thereby reducing search times, as the indices vector does not need to be dereferenced in order to check if a particular branch is relevant.

\section{Predicting Text}\label{sec:predictingText}

Once the tree has been built, prediction is rather straightforward. Given a particular context, one must search through the tree for a match. If a match cannot be found, then a node may be instantiated, either by extending a branch, or by fragmenting an existing branch. As mentioned earlier, the probability of choosing an existing table is $\frac{n_{k}-d}{n+\alpha}$, where $n_{k}$ is the number of customers seated at that table, $d$ and $\alpha$ are the discount parameter and concentration parameter (taken to be zero for our purposes) respectively and $n$ is the total number of customers in the restaurant. The probability of choosing a new table (i.e. drawing a symbol from the parent distribution) is $\frac{\alpha+Nd}{n+\alpha}$, where $N$ is the number of tables in the restaurant. Drawing from the parent distribution means repeating this process again at the next highest node. As with tree construction, customers are drawn by assigning probability ranges for all possible options and then generating a random number between 0 and 1 to determine which to choose.

Discount parameters vary depending on a node's \textit{depth} in the tree. In this project, the values given in \textit{A Stochastic Memoizer for Sequence Data} \cite{wood2009stochastic} were used. These are $d_{[0,1,2,...]}=(0.62, 0.69, 0.74, 0.80, 0.95, 0.95, ...)$. In this situation, depth actually refers to the depth in a prefix \textit{trie}, that is a tree in which no \textit{coagulation} (the process of combining symbols to create longer contexts for nodes with only one child) has taken place (all nodes have a context consisting of just one symbol). In the representation discussed earlier, coagulation inherently took place during construction. This is essentially marginalisation. Figure \ref{fig:prefixTrie} shows the prefix trie for ``mississippi". When coagulation takes place, the discount parameters must be multiplied together in accordance with the following theorem: 
\\

\noindent\framebox{\vbox{\noindent\textbf{Coagulation Theorem:}
\\
If $G_{2}|G_{1}\sim\mathcal{PY}(d_{1},0,G_{1})$ and $G_{3}|G_{2}\sim\mathcal{PY}(d_{2},0,G_{2})$ then $G_{3}|G_{1}\sim\mathcal{PY}(d_{1}d_{2},0,G_{1})$ with $G_{2}$ marginalised out.}}\cite{wood2009stochastic}

\begin{figure}[h!]
\Tree [.$\bullet$ m [.p [.i [.s [.s [.i [.s [.s [.i m ] ] ] ] ] ] ] [.p [.i [.s [.s [.i [.s [.s [.i m ] ] ] ] ] ] ] ] ] [.i [.s [.s [.i m [.s [.s [.i m ] ] ] ] ] ] m [.p [.p [.i [.s [.s [.i [.s [.s [.i m ] ] ] ] ] ] ] ] ] ] [.s [.i m ] [.s [.i m ] [.i [.s [.s [.i m ] ] ] ] [.i [.s [.s [.i m ] ] ] ] ] ] ]
\caption{Prefix Trie for ``mississippi"}
\label{fig:prefixTrie}
\end{figure}


%\subsubsection{Coagulation}
%
%\todo[inline]{Re-word coagulation section}
%
%Consider a tree constructed so that all nodes have a context consisting of just one symbol. An infinite tree built in this way for all possible combinations of symbols is the most accurate way of expressing language. However, it is not the most compact. \textit{Coagulation} is the process of combining symbols to create a longer context if a node has only one child. This is essentially marginalisation.
%
%Consider a  graphical model $G_{1}\rightarrow G_{2}\rightarrow G_{3}$, with $G_{2}$ having no children other than $G_{3}$. $G_{2}$ can be marginalised out, leaving $G_{1}\rightarrow G_{3}$. If $G_{2}|G_{1}\sim\mathcal{PY}(d_{1},0,G_{1})$ and $G_{3}|G_{2}\sim\mathcal{PY}(d_{2},0,G_{2})$ then $G_{3}|G_{1}\sim\mathcal{PY}(d_{1}d_{2},0,G_{1})$ with $G_{2}$ marginalised out \cite{wood2009stochastic}.
%
%The stick breaking construction of $G_{2}|G_{1}$ and $G_{3}|G_{2}$: weights distributed according to GEM distributions with concentration parameters=0 (Equation \ref{eq:SMSD-marginalisation}), where $\delta_{\theta}$ is a point mass located at $\theta$. We can coagulate the sticks associated with each subset of the point masses of $G_{3}$ that correspond to a point mass of $G_{2}$ such that $G_{3}=\sum_{i=1}^{\infty}\tau_{i}\delta_{\phi_{i}}$ where $\tau_{i}=\sum_{j=1}^{\infty}\kappa_{j}I(z_{j}=i)$.
%
%\begin{subequations}
%\begin{align}
%G_{2}&=\sum_{i=1}^{\infty}\pi_{i}\delta_{\phi_{i}}
%\\
%\phi_{i}&\sim^{iid}G_{1}
%\\
%\boldsymbol{\pi}&\sim\text{GEM}(d_{1},0)
%\\
%G_{3}&=\sum_{j=1}^{\infty}\kappa_{j}\delta_{\psi_{j}}
%\\
%\psi_{j}&\sim^{iid}G_{2}
%\\
%\boldsymbol{\kappa}&\sim\text{GEM}(d_{2},0)
%\end{align}
%\label{eq:SMSD-marginalisation}
%\end{subequations}

\subsection{Handling Contexts Which Are Not Straightforward}

It is important to consider what will happen if we are asked to predict the symbols following a context that does not appear in the training data. Using again the prefix tree for ``mississippi" (Figure \ref{fig:mississippiCRPTree}), what will happen if we predict based on the context ``pi"? In this case, we follow the branch for ``i" and then find that there is no child starting with ``p". Therefore, we must instantiate a branch for ``p". Since this new restaurant has no tables, there is a 100\% probability that we must create a new table. This means drawing a symbol from the parent restaurant (that of node ``i").

Another possibility is that we want to predict the symbol following a context that has been marginalised out. For example, to predict based on the context ``is", we follow the branch for ``i" and then find that the child branch for ``s" has been coagulated to form the context ``ssi". In this case, \textit{fragmentation} must take place. This is the opposite of coagulation. The branch for ``ssi" is split into two parts, one for ``s" and one for ``si". A restaurant is then instantiated at this new node. The tables are populated by using the condition stipulated earlier that the number of customers for each symbol in a restaurant must be equal to the total number of tables for that symbol in its child restaurants. Prediction then takes place by drawing a symbol from this new restaurant.

%\todo[inline]{Explain and re-word fragmentation}
%
%\textit{Fragmentation} is the opposite of coagulation. Each stick $\tau_{i}$ is broken into a infinite number of sticks and the resulting sticks are re-ordered by \textit{size-biased permutation}. The size-biased permutation of a set of positive numbers is obtained by iteratively picking (without replacement) entries with probabilities proportional to their sizes.
%
%Define a fragmentation $\boldsymbol{\tau}$ by $\text{GEM}(d,c)$: for each stick $\tau_{i}$, draw $\boldsymbol{\rho}_{i}\sim\text{GEM}(d,c)$, define $\tilde{\kappa}_{ik}=\tau_{i}\rho_{ik}$ for all $k$ and let $\boldsymbol{\kappa}=(\kappa_{j})_{j=1}^{\infty}$ be the size-biased permutation of $(\tilde{k}_{ik})_{ik=1}^{\infty}$. Require fragmentation operation to return $\boldsymbol{\pi}=(\pi_{i})_{i=1}^{\infty}$. Sticks are directly extracted from the reversal of the size-biased permutation, which maps each $\kappa_{j}$ to some $\tau_{i}$. Set $z_{j}=i$ and define $\pi_{i}$ as the asymptotic proportion of $z_{j}$s that take the value $i$ (Equation \ref{eq:SMSD6}). $(\boldsymbol{\kappa},\boldsymbol{\pi},\boldsymbol{z})\sim\text{FRAG}_{\text{GEM}(d,c)}(\boldsymbol{\tau})$ if $\boldsymbol{\kappa}$, $\boldsymbol{\pi}$ and $\boldsymbol{z}$ constructed as above.
%
%\begin{equation}
%\pi_{i}=\lim_{j \to \infty}\frac{1}{j}\sum_{l=1}^{j}I(z_{l}=i)
%\label{eq:SMSD6}
%\end{equation}
%
%\subsubsection{Sec 5}
%
%\todo[inline]{Rename section}
%
%\todo[inline]{Difference between trie and tree}
%
%Want to compute predictive probability of a symbol $v$ given a context $\boldsymbol{s}$ that is not in training set. Predictive probability is $\mathbb{E}[G_{[\boldsymbol{s}]}(v)]=\mathbb{E}[G_{[\boldsymbol{s}']}(v)]$, where $\boldsymbol{s}'$ is the longest suffix of $\boldsymbol{s}$ that occurs in the prefix trie and the expectations are taken over the posterior. $\mathbb{E}[G_{[\boldsymbol{s}']}(v)]$ can be estimated by averaging over the seating arrangements of the restaurant corresponding to $\boldsymbol{s}'$. If $\boldsymbol{s}'$ does not appear in prefix tree, need to reinstantiate the corresponding restaurant.
%
%Consider $\boldsymbol{s}=[\text{oca}]$. Longest suffix in prefix trie is $\boldsymbol{s}'=[\text{ca}]$. This does not appear in prefix tree as $G_{[\text{ca}]}$ has been marginalised out. Reinstantiate by fragmenting $G_{[\text{oaca}]}|G_{[\text{a}]}$ into $G_{[\text{ca}]}|G_{[\text{a}]}$ and $G_{[\text{oaca}]}|G_{[\text{ca}]}$. Suppose there are $K$ tables in the $G_{[\text{oaca}]}|G_{[\text{a}]}$ restaurant, table $k$ having $n_{k}$ customers. Independently for each table $k$, sample a partition of $n_{k}$ customers in a restaurant corresponding to a PYP with discount parameter $d_{3}d_{4}$ and concentration parameter $-d_{2}d_{3}d_{4}$. This results in $J_{k}$ tables with number of customers $n_{kj}$, $\sum_{j=1}^{J_{k}}n_{kj}=n_{k}$. The $n_{k}$ customers at the original table are now seated at $J_{k}$ tables in the $G_{[\text{oaca}]}|G_{[\text{ca}]}$ restaurant with table $j$ having $n_{kj}$ customers. Each of these tables sends a customer to the $G_{[\text{ca}]}|G_{[\text{a}]}$ restaurant. These customers are all seated at the same table. There was one customer in the $G_{[\text{a}]}|G_{[]}$ restaurant corresponding to the original table in $G_{[\text{oaca}]}|G_{[\text{a}]}$ with $n_{k}$ customers. There is still one customer in the $G_{[\text{a}]}|G_{[]}$ corresponding to the new table in $G_{[\text{ca}]}|G_{[\text{a}]}$, therefore this restaurant's seating arrangement need not be altered.
%
%\todo[inline]{Re-explain using mississippi}

%    \begin{figure}
%    \centering
%\begin{tikzpicture}[node distance = 4cm, auto,>=triangle 45]
%
%    \tikzstyle{every node}=[font=\scriptsize]
%    % Place nodes
%    \node [block] (start) {Start with shortest suffix};
%    \node [decision, below of=start] (child_check) {Do any of root node's children match first symbol of context?};
% \node [decision, below of=child_check] (match_range_check) {Does child's range match context?};
% \node [decision, below of=match_range_check] (table_check) {Is there already a table for the first symbol of the suffix in the node's restaurant?};
% \node [block, left of=table_check, node distance=7.25cm] (inc_count) {Incremement customer count};
% \node [block, below of=table_check] (new_table) {Create table with one customer};
% \node [block, right of=match_range_check, node distance=5.25cm] (new_root_node) {Set matching node as new root node};
% \node [block, left of=match_range_check,node distance=5.4cm] (branch) {Branch node into two sections and instantiate table for first symbol of suffix. Draw letter from parent distribution};
% \node [block, left of=new_table, node distance=9cm] (create_node) {Create child node for context with a table with one customer for first symbol of suffix};
% \node [block, below of=new_table] (consistency_check) {Check that number of customers in root node's restaurant is equal to total number of tables for that symbol in its children's restaurants};
% \node [block, right of=consistency_check, node distance =7cm] (repeat) {Consider next longest suffix};
%
%    % Draw edges
%    \path [line] (start) -- (child_check);
%    \path [line] (child_check) -- node {yes} (match_range_check);
%    \path [line] (match_range_check) -- node {exact match} (table_check);
%    \path [line] (table_check) -- node {yes} (inc_count);
%    \path [line] (table_check) -- node {no} (new_table);
%    \path [line] (match_range_check) -- node[above]{part of context} node[below]{matches all of range} (new_root_node);
%    \path [line] (new_root_node) |- (child_check);
%    \path [line] (match_range_check) -- node[above]{part of range} node[below]{matches all of context} (branch);
%    \path [line] (child_check) -| node [near start] {no} (create_node);
%    \path [line] (inc_count) |- (consistency_check);
%    \path [line] (new_table) -- (consistency_check);
%    \path [line] (branch) |- (consistency_check);
%    \path [line] (create_node) |- (consistency_check);
%    \path [line] (consistency_check) -- (repeat);
%    \path [line] (repeat) |- (child_check);
%
%\end{tikzpicture}
%
%\caption{Tree Construction Flowchart}
%\label{fig:treeSearchFlowchart}
%\end{figure}
