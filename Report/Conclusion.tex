\chapter{Conclusion}

\section{Summary of Project}

This project has followed the evolution of $n$-gram models from comparison of simple smoothing methods to hierarchical Pitman-Yor processes. The theory behind each method has been explored and the model has then been implemented in Clojure. This has allowed testing to take place to compare the different methods.

It was found that increasing values of $n$ made for much better $n$-gram models (based on their perplexity). The Sequence Memoizer (or $\infty$-gram) had the best value of perplexity. Larger training sets and longer given contexts were found to give much more realistic prediction.

\section{Further Work}

The Sequence Memoizer model would perform much better if it were a significantly larger training corpus. Therefore a computer with more RAM is necessary. It is possible that the code could also be rewritten to reduce the amount of memory required. 

Testing based on a large training corpus, such as a text dump of Wikipedia \footnote{\url{www.wikipedia.com}} would be very useful. The code could be adapted to answer questions based on information appearing in the training data (much like Watson).