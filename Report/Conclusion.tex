\chapter{Conclusion}

\section{Summary of Project}

This project has followed the evolution of $n$-gram models from comparison of simple smoothing methods to hierarchical Pitman-Yor processes. The theory behind each method has been explored and the model has then been implemented in Clojure. This has allowed testing to take place to compare the different methods.

It was found that increasing values of $n$ made for much better $n$-gram models (based on their perplexity). The Sequence Memoizer (or $\infty$-gram) had the best value of perplexity. Larger training sets and longer given contexts were found to give much more realistic prediction.

\section{Further Work}

The Sequence Memoizer model would perform much better if it had a significantly larger training corpus. Therefore a computer with more RAM is necessary. The code used for tree construction required a large number of recursive function calls and so a large memory allocation is needed.

More comprehensive testing, such as quantifying whether a predicted sequence makes logical and grammatical sense, would allow for a better comparison of the models.

Testing based on a large training corpus, such as a text dump of Wikipedia \footnote{\url{www.wikipedia.com}} would be very useful. The code could be adapted to answer questions based on information appearing in the training data (much like Watson).

In the future, $n$-gram models could be used for intelligent systems to learn speech. Based on a large training set (for example, the whole of the internet), a system could generate reasonable responses to questions and possibly be able to converse with humans.