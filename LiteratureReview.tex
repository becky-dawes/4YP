
\subsection{Literature Review}

Currently just a load of notes from reading the papers...

\subsubsection{A Hierarchical Bayesian Language Model Based on Pitman Yor Processes - \textit{Teh} 2006}

\textit{n}-gram models approximate the distribution over sentences using the conditional distribution of each work given a context consisting of only the previous \textit{n} - 1 words:

\begin{equation}
P(sentence)\approx\prod_{i=1}^{T}P(word_{i}|word_{i-n+1}^{i-1})
\label{eq:nGramModel}
\end{equation}

Even for small \textit{n}, the number of parameters is huge due to the large vocabulary size, therefore direct maximum likelihood parameter fitting severely over fits to the training data.

This paper proposes a novel language model based on a hierarchical Bayesian model where each hidden variable is distributed according to a Pitman Yor process (a nonparametric generalisation of the Dirichlet distribution).

\textbf{Pitman Yor Process:} Let $W$ be a fixed and finite vocabulary of $V$ words. For each word $w \in W$ let $G(w)$ be the (to be estimated) probability of $w$, and let $G = [G(w)]_{w\in W}$ be the vector of word probabilities. We place a Pitman Yor process prior on $G$: $G \sim PY(d, \theta, G_{0})$ where discount parameter $0\leq d <1$, strength parameter $\theta >-d$ and a mean vector $G_{0} = [G_{0}(w)]_{w\in W}$. $G_{0}(w)$ is the a priori probability of word $w$ (usually $G_{0}(w) = 1/V$). $\theta$ and $d$ control the amount of variability around $G_{0}$. When $d=0$, the Pitman Yor process reduces to a Dirichlet distribution with parameters $\theta G_{0}$.

Let $x_{1}, x_{2}, ...$ be a sequence of words drawn independently and identically (i.i.d.) from $G$. The first word $x_{1}$ is assigned a value of the first draw $y_{1}$ from $G_{0}$. Let $t$ be the current number of draws from $G_{0}$ (currently $t=1$), $c_{k}$ be the number of words assigned the value of draw $y_{k}$ (currently $c_{1}=1$), and $c.=\sum_{k=1}^{t}c_{k}$ be the current number of draws from $G$. For each subsequent word $x_{c.+1}$, we either assign it the value of a previous draw $y_{k}$ with probability $\frac{c_{k}-d}{\theta +c.}$ (increment $c_{k}$; set $x_{c.+1}\leftarrow y_{k}$), or we assign it the value of a new draw from $G_{0}$ with probability $\frac{\theta +dt}{\theta +c.}$ (increment $t$; set $c_{t}=1$; draw $y_{t}\sim G_{0}$; set $x_{c.+1}<y_{t}$).

The more words that have been assigned to a draw from $G_{0}$, the more likely subsequent words will be assigned to the draw. Also, the more we draw from $G_{0}$, the more likely a new word will be assigned to a new draw from $G_{0}$. These effects together produce a power-law distribution where many unique words are observed, most of them rarely. $\theta$ controls the overall numbers of unique words, while $d$ controls the asymptotic growth of the number of unique words. This procedure for generating words drawn from $G$ is the Chinese Restaurant Process. (Consider a sequence of customers (corresponding to the words drawn from $G$) visiting a Chinese restaurant with an unbounded number of tables (corresponding to the draws from $G_{0}$), each of which can accommodate an unbounded number of customers. The first customer sits at the first table, and each subsequent customer either joins an already occupied table (assign the word to the corresponding draw from $G_{0}$), or sits at a new table (assign the word to a new draw from $G_{0}$).)

\textbf{Hierarchical Pitman Yor Language Models:} Given a context $\boldsymbol{u}$, let $G_{\boldsymbol{u}}(w)$ be the probability of the current word taking on value $w$. We use a Pitman Yor process as the prior for $G_{\boldsymbol{u}}[G_{\boldsymbol{u}}(w)]_{w \in W}$, in particular,

\begin{equation}
G_{\boldsymbol{u}}\sim PY(d_{|\boldsymbol{u}|}, \theta_{|\boldsymbol{u}|}, G_{\pi(\boldsymbol{u})})
\label{eq:pitmanYorPrior}
\end{equation}

\noindent where $\pi(\boldsymbol{u})$ is the suffix of $\boldsymbol{u}$ consisting of all but the earliest word. As we don't know $G_{\pi(\boldsymbol{u})}$, we recursively place a prior over $G_{\pi(\boldsymbol{u})}$ using Equation \ref{eq:pitmanYorPrior}, but now with parameters $\theta_{|\pi(\boldsymbol{u})|}, d_{|\pi(\boldsymbol{u})|}$ and mean vector $G_{\pi(\pi(\boldsymbol{u}))}$ instead. This is repeated until we get to $G_{\emptyset}$, the vector of probabilities over the current word given the empty context $\emptyset$. Finally we place a prior on $G_{\emptyset}$:

\begin{equation}
G_{\emptyset} \sim PY(d_{0}, \theta_{0}, G_{0})
\label{eq:emptyContextPrior}
\end{equation}

\noindent where $G_{0}$ is the global mean vector, given a uniform value of $G_{0}(w)=1/V$ for all $w \in W$. Finally, we place a uniform prior on the discount parameters and a Gamma(1,1) prior on the strength parameters. The total number of parameters in the model is $2n$.

The prior is structured as a suffix tree of depth $n$, where each node corresponds to a context consisting of up to $n-1$ words, and each child corresponds to adding a different word to the beginning of the context (words appearing earlier in a context have (a priori) the least importance in modelling the probability of the current word).

\textbf{Hierarchical Chinese Restaurant Process:} We may treat each $G_{\boldsymbol{u}}$ as a distribution over the current word. Since $G_{\boldsymbol{u}}$ is Pitman Yor process distributed, we can draw words from it using the Chinese Restaurant process. The same applies for $G_{\pi(\boldsymbol{u})}$. This is recursively applied until we need draws from the global mean distribution $G_{0}$, which is easy since it is just uniformally distributed.

For each context $\boldsymbol{u}$, there is a sequence of words $x_{\boldsymbol{u}1}, x_{\boldsymbol{u}2}, ...$ drawn i.i.d. from $G_{\boldsymbol{u}}$ and another sequence of words $y_{\boldsymbol{u}1}, y_{\boldsymbol{u}2}, ...$ drawn i.i.d. from the parent distribution $G_{\pi(\boldsymbol{u})}$. We use $l$ to index draws from $G_{\boldsymbol{u}}$ and $k$ to index draws from $G_{\pi(\boldsymbol{u})}$. $t_{\boldsymbol{u}wk}=1$ if $y_{\boldsymbol{u}k}$ takes on value $w$, and $t_{\boldsymbol{u}wk}=0$ otherwise. Each word $x_{\boldsymbol{u}l}$ is assigned to one of the draws $y_{\boldsymbol{u}k}$ from $G_{\pi(\boldsymbol{u})}$. If $y_{\boldsymbol{u}k}$ takes on value $w$ define $c_{\boldsymbol{u}wk}$ as the number of words $x_{\boldsymbol{u}l}$ drawn from $G_{\boldsymbol{u}}$ assigned to $y_{\boldsymbol{u}k}$, otherwise let $c_{\boldsymbol{u}wk}=0$. Finally, we denote marginal counts by dots, e.g. $c_{\boldsymbol{u}.k}$ is the number of $x_{\boldsymbol{u}l}$s assigned with the value of $y_{\boldsymbol{u}k}$, $c_{\boldsymbol{u}w.}$ is the number of $x_{\boldsymbol{u}l}$s with value $w$, and $t_{\boldsymbol{u}..}$ is the current number of draws $y_{\boldsymbol{u}k}$ from $G_{\pi(\boldsymbol{u})}$.

\begin{equation}
\left \{ \begin{array} {c c} t_{\boldsymbol{u}w.}=0 & if\ c_{\boldsymbol{u}w.}=0 \\
1\leq t_{\boldsymbol{u}w.} \leq c_{\boldsymbol{u}w.} & if\ c_{\boldsymbol{u}w.}>0
\end{array}
\right.
\label{eq:hierarchicalChineseRestaurantT}
\end{equation}

\begin{equation}
c_{\boldsymbol{u}w.}=\sum _{\boldsymbol{u}':\pi (\boldsymbol{u}')=\boldsymbol{u}}t_{\boldsymbol{u}'w.}
\label{eq:hierarchicalChineseRestaurantC}
\end{equation}

The more a word $w$ has been drawn in context $\boldsymbol{u}$, the more likely it will be drawn again in context $\boldsymbol{u}$. Word $w$ will be reinforced for other contexts that share a common suffix with $\boldsymbol{u}$, with the probability of drawing $w$ increasing as the length of the common suffix increases ($w$ will be more likely under the context of the common suffix as well).

\textbf{Markov Chain Monte Carlo sampling based inference scheme for the hierarchical Pitman Yor language model:} Training data $D$ consists of the number of occurrences $c_{\boldsymbol{u}w.}$ of each word $w$ after each context $\boldsymbol{u}$ of length exactly $n-1$. This corresponds to observing word $w$ drawn $c_{\boldsymbol{u}w.}$ times from $G_{\boldsymbol{u}}$. Given $D$, we are interested in the posterior distribution over the latent vectors $\boldsymbol{G}=\{ G_{\boldsymbol{v}}\colon all\ contexts\ \boldsymbol{v} \} $ and parameters $\boldsymbol\Theta =\{ \theta_{m}, d_{m} \colon 0 \leq m \leq n-1 \}$:

\begin{equation}
p(\boldsymbol{G}, \boldsymbol\Theta | D)=p(\boldsymbol{G}, \boldsymbol\Theta, D)/p(D)
\label{eq:mcmcPosteriorG}
\end{equation}

The hierarchical Chinese Restaurant process marginalises out each $G_{\boldsymbol{u}}$, replacing it with the seating arrangement in the corresponding restaurant, $S_{\boldsymbol{u}}$. Let $\boldsymbol{S}=\{S_{\boldsymbol{v}}:all\ context\ \boldsymbol{v}\}$. Therefore we are instead interested in the posterior over seating arrangements:

\begin{equation}
p(\boldsymbol{S}, \boldsymbol\Theta |D)=p(\boldsymbol{S}, \boldsymbol\Theta, D)/p(D)
\label{eq:mcmcPosteriorS}
\end{equation}

The probability of a test word $w$ after a context $\boldsymbol{u}$ is given by:

\begin{equation}
p(w | \boldsymbol{u}, D)=\int p(w | \boldsymbol{u}, \boldsymbol{S}, \boldsymbol\Theta)p(\boldsymbol{S}, \boldsymbol\Theta | D)d(\boldsymbol{S}, \boldsymbol\Theta)
\label{eq:mcmcWordProb}
\end{equation}

\noindent where the first probability on the right is the predictive probability under a particular setting of seating arrangements $\boldsymbol{S}$ and parameters $\boldsymbol\Theta$, and the overall predictive probability is obtained by averaging this with respect to the posterior over $\boldsymbol{S}$ and $\boldsymbol\Theta$ (second probability on right). Approximate the integral with samples $\{\boldsymbol S^{(i)}, \boldsymbol\Theta^{(i)}\}_{i=1}^{I}$ drawn from $p(\boldsymbol S, \boldsymbol\Theta |D)$:

\begin{equation}
p(w|\boldsymbol u, D)\approx \sum_{i=1}^{I}p(w|\boldsymbol u, \boldsymbol S^{(i)}, \boldsymbol \Theta ^{(i)})
\label{eq:mcmcIntegralApprox}
\end{equation}

\noindent while $p(w|\boldsymbol u, \boldsymbol S, \boldsymbol \Theta)$ is given by the function $WordProb(\boldsymbol u, w)$:

\begin{equation}
p(w|0, \boldsymbol S, \boldsymbol \Theta)=1/V
\label{eq:mcmcWordProb1}
\end{equation}

\begin{equation}
p(w|\boldsymbol u, \boldsymbol S, \boldsymbol \Theta)=\frac{c_{\boldsymbol u w.}d_{|\boldsymbol u |}t_{\boldsymbol u w.}}{\theta_{|\boldsymbol u |}+c_{\boldsymbol u ..}} + \frac{\theta_{|\boldsymbol u |}+d_{|\boldsymbol u |}t_{\boldsymbol u ..}}{\theta_{|\boldsymbol u |}+c_{\boldsymbol u ..}}p(w|\pi(\boldsymbol u), \boldsymbol S, \boldsymbol \Theta)
\label{eq:mcmcWordProb2}
\end{equation}

\noindent where the counts are obtained from the seating arrangement $S_{\boldsymbol u}$ in the Chinese Restaurant process corresponding to $G_{\boldsymbol u}$.

\subsubsection{A Hierarchical, Hierarchical Pitman Yor Process Language Model - \textit{Wood, Teh}}

\textbf{Normal Pitman Yor process language modelling:} Distribution over words following a particular context:

\begin{equation}
w_{t}|w_{t-1}, w_{t-2} \sim G_{\{w_{t-2},w_{t-1}\}}^{0}
\label{eq:PYWordDistribution}
\end{equation}

\noindent (context length 2) is a random distribution:

\begin{equation}
G_{\{w_{t-2}, w_{t-1}\}}^{0} \sim PY(d_{2}, \alpha_{2}, H)
\label{eq:PYDistributionPY}
\end{equation}

\noindent where $PY(d, \alpha, H)$ is a Pitman Yor process with a distributed count $d$, concentration $\alpha$ and base distribution $H$. When the base distribution is the distribution over words following the same context with one fewer antecedents, $H=G_{\{w_{t-1}\}}^{0}$, and $G_{\{w_{t-1}\}}^{0}$ is a random distribution which is distributed according to a Pitman Yor process with another more general base distribution, this is a \textit{Hierarchical Pitman Yor Process (HPYP)}. This "recursion" continues until the set of antecedent words is empty - "root" PY process is typically given a base distribution which is uniform over the corpus vocabulary.

\textbf{Graphical Pitman Yor Process:} Assume we have corpora from domains $D_{1}, D_{2}$ and we take the approach common to Bayesain domain adaptation, specifying a hierarchical model that allows statistical sharing between the models of each sorpus. The model has the same form as the HPYP, except that the base distribution of every PY process in the hierarchy is different:

\begin{equation}
G_{\{w_{t-2}, w_{t-1}\}}^{D_{i}} \sim PY(d_{j}, \theta_{j}, \pi G_{\{w_{t-1}\}}^{D_{i}} + (1-\pi)G_{\{w_{t-2}, w_{t-1}\}}^{0})
\label{eq:graphicalPY}
\end{equation}

\noindent The distribution over words in a particular context in a particular domain could reasonably back off to a distribution over words given a shorted context in the same domain or a distribution over words given the whole context in a general domain. Here $\pi$ is the parameter that controls how closely the base distribution is tied to the domain specific model or the general model.

\subsubsection{Dirichlet Process - \textit{Teh} 2010}

The Dirichlet Process is a stochastic process used in Bayesian nonparametric models of data. It is a distribution over distributions and has Dirichlet distributed finite dimensional marginal distributions. Distributions drawn from a Dirichlet process are discrete, but cannot be described using a finite number of parameters (nonparametric).

DP is a stochastic process whose sample paths are probability measures with probability 1. Stochastic processes are distributions over function spaces, with sample paths being random functions drawn from the distribution. DP is a distribution over probability measures, which are functions with certain special properties which allow them to be interpreted as distributions over some probability space $\Theta$. Therefore draws from a DP can be interpreted as random distributions. DP is an infinite dimensional generalisation of Dirichlet distributions.

\subsubsection{The Sequence Memoizer - \textit{Wood, Gasthaus, Archambeau, James, Teh} 2011}

"The Sequence Memoizer is a new hierarchical Bayesian model for discrete sequence data that captures long range dependencies and power-law characteristics, while remaining computationally attractive."

Let $\Sigma$ be the set of symbols that can occur in some sequence. Suppose that we are given a sequence $\boldsymbol{x}=x_{1}, x_{2}, ..., x_{T}$ of symbols from $\Sigma$ and want to estimate the probability that the next symbol takes a particular value $s \in \Sigma$.

\textbf{Using Relative Frequency:} i.e. if $s$ occurs frequently in $\boldsymbol{x}$ we expect its probability of appearing next to be high as well. $N(s)$ is number os occurrences of $s$ in $\boldsymbol{x}$. Probability of $s$ being next symbol is $G(s)=N(s)/T=N(s)/\sum_{s' \in \Sigma}N(s')$. $G$ is a discrete distribution over the elements of $\Sigma$: it assigns a non-negative number $G(s)$ to each symbol $s$ signifying the probability of observing $s$ with the numbers summing to 1 over $\Sigma$. This approach is reasonable only if the process generating $\boldsymbol{x}$ has no history dependence. 
\textbf{Taking into account context:} If the last symbol in $\boldsymbol{x}$ is $\boldsymbol{u}$, then we can estimate the probability of the next symbol being $s$ by counting the number of times $s$ occurs after $\boldsymbol{u}$ in $\boldsymbol{x}$:

\begin{equation}
G_{\boldsymbol{u}}(s)=\frac{N(\boldsymbol{u}s)}{\sum_{s' \in \Sigma}N(\boldsymbol{u}s')}
\label{eq:seqMemRelativeFreqContext}
\end{equation}

\noindent is the estimated probability go $s$ occurring after $\boldsymbol{u}$, where $N(\boldsymbol{u}s)$ is the number os occurrences of the subsequence $\boldsymbol{u}s$ in $\boldsymbol{x}$. $G_{\boldsymbol{u}}$ is a discrete distribution over the symbols in $\Sigma$, but it is a conditional distribution as the probability assigned to each symbol $s$ depends on the context $\boldsymbol{u}$.

\textbf{Maximum Likelihood:} An optimistic procedure - assumes $\boldsymbol{x}$ is an accurate reflection of the true underlying process that generated it, to the ML parameters will be an accurate estimate of the true parameters - leads to overfitting. if $\boldsymbol{u}$ is long, the chance that it nevers occurs in $\boldsymbol{x}$ is high, therefore the denominator in Equation \ref{eq:seqMemRelativeFreqContext} is 0. If $\boldsymbol{u}$ did occur in $\boldsymbol{x}$, a high probability will be assigned to the symbols that followed $\boldsymbol{u}$ and zero probability to all other symbols. The amount of data in $\boldsymbol{x}$ is insufficient to characterise the conditional distribution $G_{\boldsymbol{u}}$. Avoid this by making a fixed-order Markov assumption and restricting to estimating collections of distributions conditioned on short contexts.

\textbf{Bayesian Modelling:} conservative compared to ML approach. Uncertainty in estimation is taken into account by treating the parameters $\Theta$ as random, with a prior distribution $p(\Theta)$ reflecting the prior knowledge we have about the true data generating process. $p(\Theta | \boldsymbol{x})=p(\Theta)p(\boldsymbol{x}|\Theta)/p(\boldsymbol{x})$. Computations such as prediction are then done taking into account the a posteriori uncertainty about the underlying parameters.

Natural sequence data often exhibits power-law properties. Conditional distributions of similar contexts tend to be similar themselves, particularly in the sense that recency matters.

\textbf{Power-law scaling:} There are a small number of words that occur disproportionately frequently and a very large number of rare words that, although each occurs rarely, when taken together make up a large proportion of the language. ML estimates the probabilities of the frequently occurring symbols well, since they are based on many observations of the symbols. The estimates of the rare symbols will be bad.If a symbol did not occur in our sequence, our estimate of its probability is 0, while the estimate of a rare symbol that occurred by chance will be too high.

Using Pitman Yor:

\begin{equation}
p(\boldsymbol{x}_{T+1}=s|\boldsymbol{x})=\int(\boldsymbol{x}_{T+1}=s | G)p(G|\boldsymbol{x})dG={\mathbb E} [G(s)]
\label{eq:seqMemPowerLaw}
\end{equation}

\noindent where $\mathbb E$ stands for expectation wrt posterior distribution $p(G|\boldsymbol{x})$. This equation doesn't always have an analytic solution therefore use numerical integration approaches, including sampling and Monte Carlo integration. 

In addition to the counts $\{N(s')\}_{s' \in \Sigma}$ assume that there is another set of random "counts" $\{M(s')\}_{s' \in \Sigma}$ satisfying $1/leq M(s') /leq N(s')$ if $N(s') >)$ and $M(s')>0$ otherwise. Probability of a symbol $s$ occurring next:

\begin{equation}
\mathbb E [G(s)]=\mathbb E \left [\frac {N(s)-\alpha M(s)+\sum_{s' \in \Sigma}\alpha M(s')G_{0}(s)}{\sum_{s' \in \Sigma}N(s')}\right ]
\label{eq:seqMemPYP}
\end{equation}

\noindent Each $M(s)$ reduces the count $N)s)$ by $\alpha M(s)$. The total amount subtracted is redistributed across all symbols in $\Sigma$ proportionally according to the symbols' probability under the base distribution $G_{0}$. Therefore non-zero counts are usually reduced, with larger counts typically reduced by a larger amount. This mitigates the overestimation of probabilities of rare symbols that happen to appear by chance. For symbols that did not appear at all, the estimates of their probabilities are pulled upward from zero, mitigating underestimation.

\textbf{Context trees:} If two contexts are similar, then the corresponding conditional distributions over the symbols that follow those context will tend to be similar as well. Similarity is defined by overlapping contextual suffixes.

Using a hierarchical Bayesian model and considering only fixed, finite length contexts, we are making an \textit{n}\textsuperscript{th} order Markov assumption - each symbol depends only on the last \textit{n} observed symbols. This assumption dictates that distributions are not only similar, but equal among context whose suffixes overlap in their last \textit{n} symbols.

To construct a context tree: Arrange the context $\boldsymbol{u}$ (and the associated distributions $G_{\boldsymbol{u}}$) in a tree where the parent of a node $\boldsymbol{u}$, denoted by $\sigma(\boldsymbol{u})$, is given by its longest proper suffix (i.e. $\boldsymbol{u}$ with its first symbol from the left removed). Since we are making an \textit{n}\textsuperscript{th} order Markov assumption, it is sufficient to consider only the contexts $\boldsymbol{u} \in \Sigma_{\boldsymbol{u}}^{*} = \{u' \in \Sigma^{*} :|\boldsymbol{U}'|\leq n\}$ of length at most \textit{n}. The resulting context tree has height \textit{n} and the total number of nodes in the tree grows exponentially in \textit{n}. The memory complexity of models built on such context trees usually grows too large and too quickly for reasonable values of \textit{n} and $|\Sigma|$.

\textbf{HPYP:}

\begin{equation}
G_{\varepsilon}\sim PY(\alpha_{0}, G_{0})
\label{eq:seqMemG}
\end{equation}

\begin{equation}
G_{\boldsymbol{u}}|G_{\sigma(\boldsymbol{u})}\sim PY(\alpha_{|\boldsymbol{u}|}, G_{\sigma(\boldsymbol{u})}) \ \ \ \  for\ all\ \boldsymbol{u} \in \Sigma_{n}^{*}/\varepsilon
\label{eq:seqMemGU}
\end{equation}

\begin{equation}
x_{i}|\boldsymbol{x}_{i-n:i-1}=\boldsymbol{u}, G_{\boldsymbol{u}}\sim G_{\boldsymbol{u}} \ \ \ \  for\ i=1, ..., T
\label{eq:seqMemXi}
\end{equation}

\noindent Equation \ref{eq:seqMemGU} says that a priori the conditional distribution $G_{\boldsymbol{u}}$ should be similar to $G_{\sigma(\boldsymbol{u})}$, its parent in the context tree. The variation of $G_{\boldsymbol{u}}$ around its mean $G_{\sigma(\boldsymbol{u})}$ is described by a PYP with a context length-dependent discount parameter $\alpha_{|\boldsymbol{u}|}$. At the top of the tree, the distribution $G_{\varepsilon}$ for the empty context $\varepsilon$ is similar to an overall base distribution $G_{0}$ which specifies our prior belief that each symbol $s$ will appear with the probability $G_{0}(s)$. Equation \ref{eq:seqMemXi} describes the \textit{n}\textsuperscript{th} order Markov model for $\boldsymbol{x}$ - the distribution over each symbol $x_{i}$ in $\boldsymbol{x}$, given that its context consisting of the previous \textit{n} symbols $x_{i-n:i-1}$ is $\boldsymbol{u}$, is simply $G_{\boldsymbol{u}}$. This is the hierarchical PY process.

\textbf{The Sequence Memoizer Model:} Instead of limiting context lengths to \textit{n}, the model is extended to include the set of distributions in all contexts of any (finite) length - the distribution over each symbol is now conditioned on all previous symbols, not just the previous \textit{n}. The model is the hierarchical PY model defined in Equations \ref{eq:seqMemG}, \ref{eq:seqMemGU} and \ref{eq:seqMemXi}, but with 2 changes: the contexts range over all finite nonempty strings, $\boldsymbol{u}\in \Sigma^{*}/\varepsilon$; Equation \ref{eq:seqMemXi} becomes $x_{i}|\boldsymbol{x}_{1:i-1}=\boldsymbol{u}, G_{\boldsymbol{u}}\sim G_{\boldsymbol{u}}$ (conditioning on all previous symbols). The Model can be interpreted as the limit of an HPYP model as the Markov order \textit{n} tends to infinity.

\textbf{Compacting the context tree:} Given a finite length sequence of symbols $\boldsymbol{x}$ we need access to only a finite number of conditional distributions. We need only $G_{x_{1:i}}$ where $i=0, ..., T$ and all the ancestors of each $G_{x_{1:i}}$ in the context tree. The ancestors are needed because each $G_{\boldsymbol{u}}$ has a prior that depends on its parents $G_{\sigma(\boldsymbol{u})}$. The resulting set of conditional distributions that the sequence $\boldsymbol{x}$ actually depends on consists of $G_{\boldsymbol{u}}$ where $\boldsymbol{u}$ ranges over all continuous substrings of $\boldsymbol{x}$, a finite set of $O(T^2)$ contexts. This subtree is denoted $T(\boldsymbol{x})$.

Many of the contexts that appear in $T(\boldsymbol{x})$ appear only in non-branching chains, i.e. each node on the chain has only one child in $T(\boldsymbol{x})$. If we can directly express the prior of the longer context in terms of the shortest non-branching suffix, then we can effectively ignore those in between and marginalise them out from the model (coagulation: $G_{11}|G_{1}\sim PY(\alpha_{2}, G_{1})$ and $G_{011}|G_{11}\sim PY(\alpha_{3}, G_{11})$ gives $G_{011}|G_{1}\sim PY(\alpha_{2}\alpha_{3}, G_{1})$ where $G_{11}$ has been marginalised out - i.e. the prior for $G_{011}$ is another PYP whose discount parameter is simply the product of the discount parameters along the chaing leading into it on the tree $T(\boldsymbol{x})$, while the base distribution is simply the head of the $G_{1}$.

We can apply this marginalization procedure to all non-branching chains of $T(\boldsymbol{x})$ to give a compact context tree $\hat{T}(\boldsymbol{x})$ where all internal nodes have at least two children. The number of nodes in $\hat{T}(\boldsymbol{x})$ is at most twice the length of the sequence $\boldsymbol{x}$ (independent of $|\Sigma|$). The structure of the compact context tree is given by the suffix structure for the reverse sequence $x_{T}, x_{T-1}, ..., x_{1}$.

Inference in the full SM model with an infinite number of parameters is equivalent to inference in the compact context tree with a linear number of parameters. The prior over the conditional distributions on $\hat{T}(\boldsymbol{x})$ still retains the form of an HPYP - each node has a PYP prior with its parent as the base distribution.

\begin{equation}
\mathbb E [G_{\boldsymbol{u}}(s)]=\mathbb E \left [ \frac{N(\boldsymbol{u}s)-\alpha_{\boldsymbol{u}}M(\boldsymbol{u}s)+\sum_{s'\in \Sigma}\alpha_{\boldsymbol{u}}M(\boldsymbol{u}s')G_{\sigma(\boldsymbol{u})}(s)}{\sum_{s'\in \Sigma}N(\boldsymbol{u}s')}\right ]
\label{eq:seqMemCompact}
\end{equation}

The first term in the numerator is a count of the number of times $s$ occurs in the context $\boldsymbol u$. The second term is the reduction applied to the count. The third term spreads the total reduction across $\Sigma$ according to the base distribution $G_{\sigma(\boldsymbol u)}(s)$. Each context $\boldsymbol u$ now has its own discount parameter $\alpha_{\boldsymbol u}$, which is the product of discounts on the non-branching chain leading to $\boldsymbol u$ on $T(\boldsymbol s)$, while the parent $\sigma(\boldsymbol u)$ is the head of the chain. Equation \ref{eq:seqMemCompact} is defined recursively, with the predictive distribution $G_{\boldsymbol u}$ in context $\boldsymbol u$ being a function of the same in the parent $\sigma(\boldsymbol u)$ and so on up the tree.

If the context $\boldsymbol u$ does not appear in the compact context tree $\hat{T}(\boldsymbol x)$, the predictive distribution is simply the one given by the longest suffix of $\boldsymbol u$ that is in $T(\boldsymbol x)$. If that is still not in $\hat{T}(\boldsymbol u)$, then a converse of the coagulation property (\textit{fragmentation}) allows us to reintroduce the node back into the tree.

Use stochastic (Monte Carlo) approximations to evaluate Equation \ref{eq:seqMemCompact} where the expectation is approximated using samples from the posterior distribution. Samples are obtained using Gibbs sample, which repeatedly makes local changes to the counts, and using sequential Monte Carlo which iterates through the sequence $x_{1}, x_{2}, ..., x_{T}$, keeping track of a set of samples at each step and updating the samples as each symbol $x_{i}$ is incorporated into the model.

\textbf{Language modelling:} Language model performance is report in terms of a standard measure called \textit{perplexity}, defined as $2^{\ell(\boldsymbol x)}$ where $\ell(\boldsymbol x)=-\frac{1}{|\boldsymbol x|}\sum_{i=1}^{|\boldsymbol x|}log_{2}p(x_{i}|\boldsymbol x_{1:i-1})$ is the average \textit{log-loss} on a sequence $\boldsymbol x$ and the average number of bits per word required to encode the sequence using an optimal code. Another interpretation of perplexity is that it is the average number of guesses the model would have to make before it guessed each word correctly (if it makes these guesses by drawing samples from its estimate of the conditional distribution). For SM, $p(x_{i}|\boldsymbol x_{1:i-1})$ is computed using Equation \ref{eq:seqMemCompact}.

As more data is used to estimate a language model, typically its performance improves.Prediction in the SM has real-world time complexity that is essentially the same as that of a smoothed finite-order Markov model, while its memory complexity is linear in the amount of data. The computational complexity of Markov models theoretically does not depend on the amount of data, but is exponential in the Markov order, rendering straightforward extensions to higher orders impractical. The SM model fixes this problem while remaining computationally tractable.

\textbf{Compression}



\subsubsection{A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation - \textit{Wood, Teh} 2009}

\subsubsection{A Stochastic Memoizer for Sequence Data - \textit{Wood, Archambeau, Gasthaus, James, Teh} 2009}

\subsubsection{A Nonparametric Bayesian Alternative to Spike Sorting - \textit{Wood, Black} 2006}

\subsubsection{Deplump for Streaming Data - \textit{Bartlett, Wood} 2011}

Deplump is a general purpose, lossless, batch compressor based on a probabilistic model of discrete sequences called the sequence memoizer.

The Sequence Memoizer is an incremental method for estimating the conditional distributions in an \textit{n}-gram model in the limit of $n\to\infty$. The space complexity is a function of the number of instantiated nodes in the corresponding suffix-tree-shaped graphical model and the storage required at each node.

\textbf{Review:} A distribution over $P$ can be factorized as $P(S=[s_{0}, s_{1}, ..., s_{m}])=P(s_{0})P_{[s_{0}]}(s_{1})P_{[s_{0}, s_{1}]}(s_{2})...P_{[s_{0}, s_{1}, ..., s_{m-1}]}(s_{m})$, where $P_{U}(s)=P(s|U)$. The Sequence Memoizer jointly models these conditional distributions using a hierarchical Bayesian framework in which non-negative, integer parameters $\{c_{\sigma}^{U}, t_{\sigma}^{U}\}_{\sigma \in \Sigma, U \in \Sigma^{+}}$ are used to characterise each $P_{I}$. If we define $c^{U}=\sum_{\sigma\in\Sigma}c_{\sigma}^{U}$ and $t^{U}=\sum_{\sigma\in\Sigma}t_{\sigma}^{U}$ then the model is:

\begin{equation}
P_{U}(s)=\frac{c_{s}^{U}-t_{s}^{U}\delta^{U}}{c^{U}}+\frac{t^{U}\delta^{U}P_{\sigma(U)}(s)}{c^{U}}
\label{eq:deplumpReview}
\end{equation}

\noindent where $\sigma([s_{1}, s_{2}, s_{3}, ...])=[s_{2}, s_{3}, ...]$ and $P_{\sigma([])}$ is the uniform distribution over $\Sigma$.

The latent parameters of the SM are the counts in the set of sets $G=\{\{c_{\sigma}^{U}, t_{\sigma}^{U}\}_{\sigma\in\Sigma}|U\in\Sigma^{+}\}$ and $\delta_{n}$ for $n\geq 0$. While $|G|=\infty$ for sequences of unbounded length, inference requires only a computation on $H\subset G$ for any finite training sequence $\boldsymbol S$ such that $|H|\leq 2|\boldsymbol S |$. The linear growth of $|H|$ makes using the model intractable for streaming sequences. Deplump yields an inference algorithm with asymptotically constant storage complexity for $H$.

\textbf{Approximation:} Batch deplump uses a suffix tree which has storage complexity linear in input sequence length. Streaming deplump cannot have suffix tree representation of entire input sequence, so instead a fixed-length "reference sequence" is maintained, along with a dynamically updated suffix tree referencing only the suffixes found therein. Deleting nodes to constrain the size of this context tree forces node-removal inference approximations - operations are justifiable from a statistical perspective.

During incremental estimation, for each symbol $s$ in the input sequence, $c_{s}^{U}$ is incremented in at least one node on the tree. $max_{P\in H, \sigma\in\Sigma}\{c_{\sigma}^{U}\}$ then grows monotonically as a function of the length of the input sequence. Incremental construction of the model includes an operation on node elements known as fragmentation that requires computation proportional to $max_{\sigma\in\Sigma}\{c_{\sigma}^{U}\}$ for node $U$. To ensure computational complexity that is independent of the sequence length, $c_{\sigma}^{U}$ must be bounded for all $U\in H$.

\textbf{Algorithm:} Given a sequence of symbols $\boldsymbol S=[s_{0}, s_{1}, s_{2}, ...]$ where each symbol $s_{n}$ comes from an ordered set of symbols $\{\sigma_{1}, \sigma_{2}, ...\}=\Sigma$, streaming deplump worlds by repeatedly producing a predictive distribution for the continuation of the sequence given the full preceding context and encoding the next symbol by passing the predictive distribution to an entropy encoder. We assume that if the predictive distribution function is $F$ and the next symbol in the stream is $s$ then an entropy encoder exists that takes $F(s-1)$ and $F(s)$ as arguments and returns a bit stream. 

\subsubsection{Other Notes}

\textit{n}-gram model is a contiguous sequence of \textit{n} items from a given sequence of text.

\null
\noindent Entropy encoding is a lossless data compression scheme that is independent of the specific characteristics of the medium.

